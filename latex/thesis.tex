%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                            %%
%% thesistemplate.tex version 4.00 (2023/02/09)                               %%
%% The LaTeX template file to be used with the aaltothesis.sty (version 4.00) %%
%% style file.                                                                %%
%% This package requires pdfx.sty v. 1.5.84 (2017/05/18) or newer.            %%
%%                                                                            %%
%% This is licensed under the terms of the MIT license below.                 %%
%%                                                                            %%
%% Written by Luis R.J. Costa.                                                %%
%% Currently developed at Teacher services, Learning Services of Aalto        %%
%% University by Luis R.J. Costa since May 2019.                              %%
%%                                                                            %%
%% Copyright 2017-2021 aaltothesis.cls by Luis R.J. Costa,                    %%
%% luis.costa@aalto.fi.                                                       %%
%% Copyright 2017-2018 Swedish translations in aaltothesis.cls by Elisabeth   %%
%% Nyberg and Henrik Wallén henrik.wallen@aalto.fi.                           %%
%% Finnish documentation in the template opinnatepohja.tex translated from    %%
%% the English template documentation.                                        %%
%% Copyright 2021 English template thesistemplate.tex by Luis R.J. Costa,     %%
%% Maurice Forget, Henrik Wallén.                                             %%
%% Copyright 2018-2022 Swedish template kandidatarbetsbotten.tex by Henrik    %%
%% Wallen.                                                                    %%
%%                                                                            %%
%% Permission is hereby granted, free of charge, to any person obtaining a    %%
%% copy of this software and associated documentation files (the "Software"), %%
%% to deal in the Software without restriction, including without limitation  %%
%% the rights to use, copy, modify, merge, publish, distribute, sublicense,   %%
%% and/or sell copies of the Software, and to permit persons to whom the      %%
%% Software is furnished to do so, subject to the following conditions:       %%
%% The above copyright notice and this permission notice shall be included in %%
%% all copies or substantial portions of the Software.                        %%
%% THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR %%
%% IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,   %%
%% FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL    %%
%% THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER %%
%% LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING    %%
%% FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER        %%
%% DEALINGS IN THE SOFTWARE.                                                  %%
%%                                                                            %%
%%                                                                            %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                            %%
%%                                                                            %%
%% An example for writting your thesis using LaTeX                            %%
%% Original version and development work by Luis Costa, changes to the text   %% 
%% in the Finnish template by Perttu Puska.                                   %%
%% Support for Swedish added 15092014                                         %%
%% PDF/A-b support added on 15092017                                          %%
%% PDF/A-2 support added on 24042018                                          %%
%% Layout design and typesettin changed 15072021                              %%
%%                                                                            %%
%% This example consists of the files                                         %%
%%       thesistemplate.tex (version 4.00) (for text in English)              %%
%%       opinnaytepohja.tex (version 4.00) (for text in Finnish)              %%
%%       kandidatarbetsbotten.tex (version 1.10) (for text in Swedish)        %%
%%       aaltothesis.cls                                                      %%
%%       linediagram.pdf (graphics file)                                      %%
%%       curves.pdf      (graphics file)                                      %%
%%       ledspole.jpg    (graphics file)                                      %%
%%                                                                            %%
%%                                                                            %%
%% Typeset in Linux with                                                      %%
%% pdflatex: (recommended method)                                             %%
%%             $ pdflatex thesistemplate                                      %%
%%             $ pdflatex thesistemplate                                      %%
%%                                                                            %%
%%   The result is the file thesistemplate.pdf that is PDF/A compliant, if    %%
%%   you have chosen the proper \documenclass options (see comments below)    %%
%%   and your included graphics files have no problems.                       %%
%%                                                                            %%
%%                                                                            %%
%% Explanatory comments in this example begin with the characters %%, and     %%
%% changes that the user can make with the character %                        %%
%%                                                                            %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% WHAT is PDF/A
%%
%% PDF/A is the ISO-standardized version of the pdf. The standard's goal is to
%% ensure that he file is reproducable even after a long time. PDF/A differs
%% from pdf in that it allows only those pdf features that support long-term
%% archiving of a file. For example, PDF/A requires that all used fonts are
%% embedded in the file, whereas a normal pdf can contain only a link to the
%% fonts in the system of the reader of the file. PDF/A also requires, among
%% other things, data on colour definition and the encryption used.
%% Currently three PDF/A standards exist:
%% PDF/A-1: based on PDF 1.4, standard ISO19005-1, published in 2005.
%%          Includes all the requirements essential for long-term archiving.
%% PDF/A-2: based on PDF 1.7, standard ISO19005-2, published in 2011.
%%          In addition to the above, it supports embedding of OpenType fonts,
%%          transparency in the colour definition and digital signatures.
%% PDF/A-3: based on PDF 1.7, standard ISO19005-3, published in 2012.
%%          Differs from the above only in that it allows embedding of files in
%%          any format (e.g., xml, csv, cad, spreadsheet or wordprocessing
%%          formats) into the pdf file.
%% PDF/A-4: based on PDF 2.0, standard ISO19005-4, published in November 2020.
%%
%% PDF/A-1 files are not necessarily PDF/A-2 -compatible and PDF/A-2 are not
%% necessarily PDF/A-1 -compatible.
%% Standards PDF/A-1, PDF/A-2 and PDF/A-3 have two levels:
%% b: (basic) requires that the visual appearance of the document is reliably
%%    reproduceable.
%% a (accessible) in addition to the b-level requirements, specifies how
%%   accessible the pdf file is to assistive software, say, for the physically
%%   impaired.
%% The PDF/A-4 standard does not have additional levels like in the earlier
%% standards.
%% For more details on PDF/A, see, e.g., 
%% https://www.loc.gov/preservation/digital/formats/fdd/fdd000318.shtml or
%% https://www.pdfa.org/resource/iso-19005-pdfa/
%%
%%
%% WHICH PDF/A standard should my thesis conform to?
%%
%% Either to the PDF/A-1b or the PDF/A-2b standard. If all the figures and
%% graphs used in thesis work do not require transparency features, use either
%% PDF/A-1b or PFDF/A-2b. If you have figures with transparency
%% characteristics, use the PDF/A-2b standard. However, drawing applications
%% often use the transparency parameter, setting it to zero, to specify opacity
%% and get the basic 2-D visualisation. As a result, validation of PDF/A-1b
%% will fail. Use PDF/A-2b if PDF/A-1b validation fails.
%% Do not use the PDF/A-3b standard for your thesis.
%% The font to be used are specified in this templatenand they should not be
%% changed. In addition to not adhering to Aalto's visual guidelines, you may
%% have difficulties in producing a PDF/A-compliant PDF.
%%
%%
%% Validate your PDF/A file at https://www.pdf-online.com/osa/validate.aspx
%%
%%
%% WHAT graphics format can I use to produce my PDF/A compliant file?
%%
%% When using pdflatex to compile your work, favour the use of pdf, but you can
%% use the jpg or png format especially for photographs. You will have PDF/A 
%% compliance problems with figures in pdf if the fonts are not embedded in the
%% pdf file.
%% If you choose to use latex to compile your work, the only acceptable file
%% format for your figure is eps. DO NOT use the ps format for your figures.

%% USE one of the following three \documentclass set-ups:
%% * the first when using pdflatex to directly typeset your document in the
%%   chosen pdf/a format for online publishing (centred page layout),
%% * the second for one-sided printing your thesis with the layout (wide left 
%%   margin), or
%% * the third for two-sided printing.
%%
\documentclass[english, 12pt, a4paper, sci, utf8, a-2b, online]{aaltothesis}
%\documentclass[english, 12pt, a4paper, elec, utf8, a-2b, print]{aaltothesis}
%\documentclass[english, 12pt, a4paper, elec, utf8, a-2b, print, twoside]{aaltothesis}

%% Use the following options in the \documentclass macro above:
%% your school: arts, biz, chem, elec, eng, sci
%% the character encoding scheme used by your editor: utf8, latin1
%% thesis language: english, finnish, swedish
%% make an archiveable PDF/A-1b or PDF/A-2b compliant file: a-1b, a-2b
%%                    (with pdflatex, a normal pdf containing metadata is
%%                     produced without the a-*b option)
%% typset for online document or print on paper: online, print
%%        online: typeset in symmetric layout and blue hypertext for online
%%                publishing
%%        print: typeset in a symmetric layout and black hypertext for printing
%%               on paper
%%          two-side printing: twoside (default is one-sided printing)
%%               typeset in a wide margin on the binding side of the page and
%%               black hypertext. Use with print only.
%%

%% Use one of these if you write in Finnish (or use the Finnish template
%% opinnaytepohja.tex)
%\documentclass[finnish, 12pt, a4paper, elec, utf8, a-1b, online]{aaltothesis}
%\documentclass[finnish, 12pt, a4paper, elec, utf8, a-1b, print]{aaltothesis}
%\documentclass[finnish, 12pt, a4paper, elec, utf8, a-1b, print, twoside]{aaltothesis}

%% Use one of these if you write in Swedish (or use the Swedish template
%% kandidatarbetsbotten.tex)
%\documentclass[swedish, 12pt, a4paper, elec, utf8, a-2b, online]{aaltothesis}
%\documentclass[swedish, 12pt, a4paper, elec, utf8, a-2b]{aaltothesis}
%\documentclass[swedish, 12pt, a4paper, elec, dvips, online]{aaltothesis}

%% FOR USERS OF AMS PACKAGES:
%% * newtxmath used in this template loads amsmath, so
%%   you needn't load it. If you want to use options in amsmath, load it here, 
%%   before \setupthesisfonts below to pass the options to amsmath.
%% * If you want to use amsthm, load it here before \setupthesisfonts to avoid
%%   a clash with newtxmath.
%% * If using amsmath with options and you want to use amsthm, load amsthms
%%   after amsmath, as described in the amsthm documentation.
%% * Don't use amsbsym or amsfonts. The symbols [and macros] there are defined in
%%   newtxmath and so clash if used.
%\usepackage[options]{amsmath}
\usepackage{amsthm}

%% DO NOT MOVE OR REMOVE \setupthesisfonts
\setupthesisfonts

%%
%% Add here the packges you need
%%
\usepackage{graphicx}

\usepackage{mathtools}
\usepackage[shortlabels]{enumitem}
\usepackage{tikz}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{pgfplots}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.9}
\usepgfplotslibrary{external}
\tikzexternalize

\allowdisplaybreaks

%% For tables that span multiple pages; used to split a paraphrasing example in
%% the appendix. If you don't need it, remove it.
\usepackage{longtable}

%% A package for generating Creative Commons copyright terms. If you don't use
%% the CC copyright terms, remove it, since otherwise undesired information may
%% be added to this document's metadata.
\usepackage[type={CC}, modifier={by-nc-sa}, version={4.0}]{doclicense}
%% Find below three examples for typesetting the CC license notice.


%% Edit to conform to your degree programme
%% Capitalise the words in the name of the degree programme: it's a name
\degreeprogram{Mathematics and Operations Research}
%%

%% Your major
%%
\major{Applied Mathematics}
%%

%% Choose one of the three below
%%
%\univdegree{BSc}
\univdegree{MSc}
%\univdegree{Lic}
%%

%% Your name (self explanatory...)
%%
\thesisauthor{Joonas Laaksonen}
%%

%% Your thesis title and possible subtitle comes here and possibly, again,
%% together with the Finnish or Swedish abstract. Do not hyphenate the title
%% (and subtitle), and avoid writing too long a title. Should LaTeX typeset a
%% long title (and/or subtitle) unsatisfactorily, you might have to force a
%% linebreak using the \\ control characters. In this case...
%% * Remember, the title should not be hyphenated!
%% * A possible 'and' in the title should not be the last word in the line; it
%%   begins the next line.
%% * Specify the title (and/or subtitle) again without the linebreak characters
%%   in the optional argument in box brackets. This is done because the title
%%   is part of the metadata in the pdf/a file, and the metadata cannot contain
%%   linebreaks.
%%
\thesistitle[L2 Convergence of the p- Finite Element Method for the 2D Poisson Problem with a Dirac Delta Load]{$L^2$ Convergence of the $p$- Finite Element Method for the 2D Poisson Problem with a Dirac Delta Load}
%\thesistitle[Title of the thesis]{Title of\\ the thesis}
%%
%% Either remove or leave \thesissubtitle{} empty if you don't use it
%%
%\thesissubtitle{A possible subtitle}
%\thesissubtitle[Subtitle of the thesis]{Subtitle of\\ the thesis}
%\thesissubtitle{}

%%
\place{Espoo}
%%

%% The date for the master's thesis is the day it is presented
%%
\date{29 April 2024}
%%

%% Thesis supervisor
%% Note the "\" character in the title after the period and before the space
%% and the following character string.
%% This is because the period is not the end of a sentence after which a
%% slightly longer space follows, but what is desired is a regular interword
%% space.
%%
%\supervisor{Prof.\ Pirjo Professori}
\supervisor{D.Sc.\ (Tech.) Harri Hakula}
%%

%% Advisor(s)---two at the most---of the thesis. Check with your supervisor how
%% many official advisors you can have.
%%
%\advisor{Dr Alan Advisor}
%\advisor{Ms Elsa Expert (MSc)}
%%

%% If you do your thesis work in a company of other institute, give the name of
%% the company or instution here. Otherwise, leave the macro empty, comment it
%% out, or remove it. This will remove this field from the abstract page.
%%
%\collaborativepartner{Company or institute name}
%%

%% Aaltologo: syntax:
%% \uselogo{?|!|''}
%% The logo language is set to be the same as the thesis language.
%%
%\uselogo{?}
%\uselogo{!}
\uselogo{''}
%%

%%%%%%%%%%%%%%%%%%               COPYRIGHT TEXT               %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Copyright of a work is with the creator/author of the work regardless of
%% whether the copyright mark is explicitly in the work or not. You may, if you
%% wish---we encourage you to do so---publish your work under a Creative
%% Commons license (see creativecommons.org), in which case the license text
%% must be visible in the work. Write here the copyright text you want using the
%% macro \copyrighttext, which writes the text into the metadata of the pdf file
%% as well.
%%
%% Syntax:
%% \copyrigthtext{metadata text}{text visible on the page}
%%
%% CHOOSE ONE OF THE COPYRIGHT NOTICE STYLES BELOW.
%% IF USING THE CC TERMS, CHOOSE THE LICENSE YOU WANT TO USE.
%% The different CC licenses are listed at 
%% https://creativecommons.org/about/cclicenses/.
%% If you use the icons from the dolicense.sty package, add the package above
%% (\usepackage{dolicense}).
%% IMPORTANT NOTE!! Manually write the CC text in the \copyrighttext metadata
%% text field.
%%
%% NOTE: In the macros below, the text written in the metadata must have a
%% \noexpand macro before the \copyright special character. When not in pdf/a
%% mode (i.e. a-1b or a-2b are not specified in \documentclass), two \noexpands
%% are required in the metadata text to correctly render the copyright mark in
%% the pdf metadata. In pdf/a mode one \noexpand suffices.
%%
%% EXAMPLE OF PLAIN COPYRIGHT TEXT
%% The macros \copyright and \year below must be separated by the \ character 
%% (space chacter) from the text that follows. The macros in the argument of the
%% \copyrighttext macro automatically insert the year and the author's name.
%% (Note! \ThesisAuthor is an internal macro of the aaltothesis.cls class file).
%%
%\copyrighttext{Copyright \noexpand\textcopyright\ \number\year\ \ThesisAuthor}
%{Copyright \textcopyright{} \number\year{} \ThesisAuthor}
%%
%% Of course, the same text could have simply been written as
%% \copyrighttext{Copyright \noexpand\copyright\ 2018 Eddie Engineer}
%% {Copyright \copyright{} 2022 Eddie Engineer}
%%
%% EXAMPLES OF CC LICENSE: different ways to display the same license
%% 1. A simple Creative Commons license text with a link to the copyright notice:
%\copyrighttext{\noexpand\textcopyright\ \number\year. This work is 
%	licensed under a CC BY-NC-SA 4.0 license.}{\textcopyright{} 
%	\number\year. This work is licensed under a 
%	\href{https://creativecommons.org/licenses/by-nc-nd/4.0/}{CC BY-NC-SA 4.0} 
%	license.}
%
%% To get the URL of the license of your choice, go to 
%% https://creativecommons.org/about/cclicenses/, click on the chosen license
%% you want to use, and copy-and-paste the URL in the macro \href above.
%%
%% 2. A short Creative Commons license text containing the respective CC icons
%% (requires the package dolicense.sty to be added in the preamble as done
%% above) and a link to the corresponding Creative Commons license webpage (see
%% the dolicense package documentation for other license icons):
%\copyrighttext{\noexpand\textcopyright\ \number\year. This work is licensed
%	under a CC BY-NC-SA 4.0 license.}{
%	\parbox{95mm}{\noindent\textcopyright\ \number\year. \doclicenseText} 
%	\hspace{1em}\parbox{35mm}{\doclicenseImage}
%}
%%
%% 3. An expanded Creative Commons license text containing the respective CC
%% icons text and as generated by the dolicense.sty package (the license is set
%% via package options in \usepackage[options]{dolicense} above; see the
%% dolicense package documentation for other license texts and icons):
\copyrighttext{\noexpand\textcopyright\ \number\year. This work is 
	licensed under a Creative Commons "Attribution-NonCommercial-ShareAlike 4.0 
	International" (BY-NC-SA 4.0) license.}{\noindent\textcopyright\ \number
	\year \ \doclicenseThis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% The English abstract:
%% All the details (name, title, etc.) on the abstract page appear as specified
%% above.
%% Thesis keywords:
%% Note! The keywords are separated using the \spc macro
%%
\keywords{$p$-FEM\spc Poisson's equation\spc Dirac delta function\spc $L^2$ convergence}
%%

%% The abstract text. This text is included in the metadata of the pdf file as
%% well as the abstract page.
%%
\thesisabstract{%
In this thesis, we study the approximation properties of the p-version of
the finite element method when it is applied to the two-dimensional Poisson's equation
with a Dirac delta load term and with Dirichlet and Neumann boundary conditions.
Based on similar research on the h-version of the finite element method,
we prove that the p-version converges to the exact solution in L2
when the domain satisfies certain convexity assumptions.
Before proving this result, we provide detailed descriptions of the solvability
of the Dirac delta problem and of the p- finite element method along with the needed
approximation theorems to prove the L2 convergence result.
Finally, we consider the accuracy of the obtained L2 convergence rate through various
numerical experiments, and we conclude that the predicted rate is a fairly accurate
worst-case estimate. However, the numerical experiments also suggest that it would be
beneficial to construct the finite element mesh so that the Dirac delta load is located
at a vertex of the mesh to obtain the best rate of convergence.
The predicted rate of convergence is overly pessimistic in this case.
}

%% You can prevent LaTeX from writing into the xmpdata file (it contains all the 
%% metadata to be written into the pdf file) by setting the writexmpdata switch
%% to 'false'. This allows you to write the metadata in the correct format
%% directly into the file thesistemplate.xmpdata.
%\setboolean{writexmpdatafile}{false}

%% Definitions, theorems, lemmas etc.
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{assumption}{Assumption}[section]

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}

%% Custom commands
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\newcommand*{\innerprod}[2]{\left\langle #1, #2 \right\rangle}
\newcommand*\diff{\mathop{}\!d}
\newcommand*{\boldone}{\text{\usefont{U}{bbold}{m}{n}1}}
\DeclareMathOperator*{\esssup}{ess\,sup}
\DeclareMathOperator\supp{supp}
\DeclareMathOperator{\spn}{span}

\numberwithin{equation}{section}

%% All that is printed on paper starts here
%%
\begin{document}

%% Create the coverpage
%%
\makecoverpage

%% Typeset the copyright text.
%% If you wish, you may leave out the copyright text from the human-readable
%% page of the pdf file. This may seem like a attractive idea for the printed
%% document especially if "Copyright (c) yyyy Eddie Engineer" is the only text
%% on the page. However, the recommendation is to print this copyright text.
%%
\makecopyrightpage

\clearpage
%% Note that when writing your thesis in English, place the English abstract
%% first followed by the possible Finnish or Swedish abstract.

%% Abstract text
%% All the details (name, title, etc.) on the abstract page appear as specified
%% above.
%%
\begin{abstractpage}[english]
In this thesis, we study the convergence properties of the $p$-version of
the finite element method when it is applied to the two-dimensional Poisson problem
with a Dirac delta load term and with Dirichlet or Neumann boundary conditions.
Based on identical existing results for the $h$-version of the finite element method,
we prove that the approximations from the $p$-version converge to the exact solution in $L^2$
when the domain satisfies certain convexity assumptions,
which does not seem to be covered by the existing literature.
Before proving this result, we provide detailed descriptions of the solvability
of the Dirac delta problem and of the $p$- finite element method along with the needed
approximation theorems to prove the main convergence result.
Finally, we consider the accuracy of the obtained error bound through various
numerical experiments, and we conclude that the predicted convergence rate of the error
can be fairly close to the actual observed rate
when the Dirac delta load is located either on a side or in the interior of an element
of the mesh. When the load is located at a vertex of the mesh,
the predicted rate of convergence is overly pessimistic, however.
\end{abstractpage}

%% The text in the \thesisabstract macro is stored in the macro \abstractext, so
%% you can use the text metadata abstract directly as follows:
%%
%\begin{abstractpage}[english]
%	\abstracttext{}
%\end{abstractpage}

%% Force a new page so that the possible Finnish or Swedish abstract does not
%% begin on the same page
%%
\newpage
%%
%% Abstract in Finnish.  Delete if you don't need it. 
%%
%% Respecify those fields that differ from the earlier specification or simply
%% respecify all fields.
\thesistitle{Elementtimenetelmän $p$-version $L^2$-suppeneminen pistekuormitetun 2D Poissonin ongelman tapauksessa}
%\thesissubtitle{Opinnäytteen mahdollinen alaotsikko}
\supervisor{TkT Harri Hakula}
%\advisor{TkT Alan Advisor}
%\advisor{DI Elsa Expert}
\degreeprogram{Matematiikka ja operaatioanalyysi}
\major{Sovellettu matematiikka}
%\collaborativepartner{Yhtiön tai laitoksen nimi}
\date{29.4.2024}
%% The keywords need not be separated by \spc now.
\keywords{$p$-FEM, Poissonin yhtälö, Diracin deltafunktio, $L^2$-suppeneminen}
%% Abstract text
\begin{abstractpage}[finnish]
Tässä diplomityössä tarkastellaan elementtimenetelmän $p$-version suppenemista
kaksiulotteisen Poissonin yhtälön tapauksessa, kun yhtälön kuormana on Diracin deltafunktio
ja määrittelyjoukon reunoille on asetettu Dirichlet'n tai Neumannin reunaehdot.
Elementtimenetelmän $h$-version tiedetään suppenevan ongelman tarkkaan ratkaisuun $L^2$:ssa,
kun ongelman määrittelyjoukko toteuttaa tietyt konveksisuusehdot.
Tämä tulos laajennetaan tässä työssä $p$-versiolle, mitä ei ilmeisesti ole tarkasteltu aiemmin.
Ensin kuitenkin tarkastellaan ongelman tarkan ratkaisun olemassaoloa ja sen yksikäsitteisyyttä.
$L^2$-suppenemistulos seuraa pitkälti $p$-versiolle ominaisten approksimaatiotulosten
pohjalta, jotka käydään myös perusteellisesti läpi. Saatua virhe-estimaattia
verrataan tietokoneella numeerisesti laskettuihin virheisiin.
Numeeristen tulosten pohjalta voidaan päätellä,
että virheen ennustettu suppenemisnopeus on kohtalaisen lähellä todellista suppenemisnopeutta,
kun deltakuorma on elementin reunalla tai elementin sisällä.
Kun deltakuorma on elementin solmussa, todellinen suppenemisnopeus on kuitenkin huomattavasti
suurempi kuin ennustettu nopeus.
\end{abstractpage}

\dothesispagenumbering{}

%% Preface
%%
%% This section is optional. Remove it if you do not want a preface.
\mysection{Preface}
%\mysection{Esipuhe}
This thesis is inspired by a similar project which my supervisor and advisor
Dr.~Harri Hakula collaborated in during his visit to the University of Texas at Austin in 2013.
The results from that project helped shape the body of this thesis.

I would like to extend my sincere thanks to Dr.~Hakula for his guidance and encouragement
which have been invaluable to me.
I also wish to thank Professor Nuutti Hyvönen for helping me find a topic for my thesis.

My special thanks go to my parents, my brother and my late grandparents
for their unwavering support throughout my educational journey.

\vspace{7mm}
\hfill
\begin{tabular}{@{}l@{}}
Joonas Laaksonen\\
Otaniemi, 29 April 2024
\end{tabular}
\hspace{1cm}

%% Force a new page after the preface
%%
\newpage

\newpage

%% Table of contents. 
%%
\thesistableofcontents

%% \clearpage is similar to \newpage, but it also flushes the floats (figures
%% and tables).
%%
\cleardoublepage

\section{Introduction}
\label{sec:intro}

%% Leave page number of the first page empty
%% 
\thispagestyle{empty}
A partial differential equation (PDE) is an equation that consists of an
unknown function of two or more variables and its partial derivatives of
arbitrary order \cite{evans2010}.
Such equations describe how a function, possibly corresponding to a physical
quantity of interest, behaves over its domain of definition
which in practical applications typically corresponds to a geometric shape or
time or both. For example, many fundamental laws of physics can be elegantly
expressed as partial differential equations, e.g.\ Maxwell's equations
in electromagnetism.
The set of all possible partial differential equations is incredibly vast and
complex, which makes it unwieldy, if not impossible,
to come up with a general PDE theory that could be productively used for any kind
of problem. Instead, the study of PDEs focuses on important families
and instances of partial differential equations arising from
different fields of science.

A solution to a partial differential equation is said to be classical if it
can be differentiated at least as many times as the formulation of the equation
requires and the equation holds pointwise everywhere in the domain of the problem.
The solution may also need to satisfy some conditions on the boundary of its domain,
turning the problem into a boundary value problem.
However, it turns out that rather few partial differential equations have classical solutions
\cite{evans2010}. Thus, the modern theory of partial differential equations typically
abandons the quest of searching for a classical solution and instead reformulates
the problem in a more generalized form which relaxes the requirements
that the solution must fulfil regarding e.g.\ differentiability.
The solution space is essentially expanded to contain less smooth functions,
which may even be favorable for a modeled phenomenon for which the solution is expected to be
non-differentiable at some points.
A solution to the generalized problem is typically called a generalized solution or a weak solution,
and its existence can be guaranteed for a large set of problems.
Whether a weak solution is also a classical solution can then be
assessed separately, and such results belong to the
regularity theory of partial differential equations.

In practical applications, numerical methods are used to approximate the
solutions of partial differential equations. One such method that has been
extremely successful is the finite element method (FEM). Oden \cite{oden1991}
presents a review of its history.
Without too stringent a viewpoint, some attributes 
of the finite element method can be traced back a couple of centuries, but serious
interest in the method started to accumulate during the mid-1950s and 1960s
especially in the engineering community. During this time, the finite element method 
also gained its name. The mathematical foundations were established somewhat
later during the 1970s, and ever since, different variants of the finite element method
have been developed, with the $h$-, $p$- and $hp$-versions being among the most popular variants.

Oden also discusses some of the factors leading to the success of the finite
element method. The method is based on the generalized, weak formulation of a
partial differential equation, which will be discussed in more
detail in later sections, but for now it suffices to say that it is a crucial
factor for the reason why the finite element method merits its success. Being based on the weak
formulation, the finite element method is essentially geometry-agnostic, which
means that it can be used to solve problems over almost any
kind of shape. Combined with the rich modern theory of partial differential
equations, the finite element method has solid mathematical foundations which offer
optimal estimates for the convergence of the approximations. 
Moreover, from a computational standpoint, the implementation of the method in 
computer code lends itself extremely well to parallelization.

Decision-making based on computed information requires that the computed
information is reliable. Szabó and Babu{\v s}ka \cite{szabobabuska2011}
discuss this systematically in the context of finite element analysis, i.e.\
the process of using the finite element method to solve a problem. In finite
element analysis, the typical workflow is to first create a mathematical model,
i.e.\ a set of partial differential equations and constraints
that represent the physical system of interest,
then find an approximate solution to the model by using the finite element method
and, finally, extract the desired information from the computed approximate solution.
There are two critical factors contributing to the reliability of such computed 
information: the suitability of the mathematical model as a representation of
idealized reality and the accuracy of the approximate solution with respect
to the exact solution.
Szabó and Babu{\v s}ka term the processes of assessing these qualities validation 
and verification, respectively.
The validation process may consist of, for example, comparing the results of
real-life experiments to predictions obtained from the mathematical model.
The verification process leans on the well-understood approximation
properties of the finite element method.

Sometimes a physical phenomenon is modeled sufficiently well by a 
mathematical model for which the approximation error estimates from the standard
theory of the finite element method do not directly apply.
If possible, one option to still be able to perform the verification process would 
be to modify the model so that the error estimates readily apply.
However, there could be some tradeoffs involved in the choice between the models,
which could still make the initial model more favorable,
e.g.\ the initial model is a crude simplification but requires much less
effort to solve. For example, Babu{\v s}ka et al.\ \cite{babuskasoanesuri2017}
study the effects of replacing holes having extremely small radii with singular
points, i.e.\ holes with the radii equal to zero, which simplifies the approximation process
but essentially renders the model as incorrect.

In this thesis, we study a problem similar in vein to the problem
mentioned above studied by Babu{\v s}ka et al.\
which may simplify modeling but for which
the convergence of the finite element approximations is not obvious.
We study the Poisson problem with a Dirac delta load term in a two-dimensional domain,
and it can be expressed as finding a real-valued function $u$ defined over
a domain $\Omega \subset \mathbb{R}^2$ with some predefined boundary values such that
\begin{equation}
    \label{eq:intro_problem}
    -\Delta u = \delta_{x_0} \quad \text{in } \Omega,
\end{equation}
where $\Delta u$ is the Laplacian of $u$,
i.e.\ the sum of the second partial derivatives of $u$
with respect to each independent variable,
and $\delta_{x_0}$ is the Dirac delta for a given point $x_0 \in \Omega$
which can be loosely regarded as a function that is concentrated
at the point $x_0$ such that
\begin{equation*}
    \delta_{x_0}(x) =
    \begin{cases}
        \infty, & x = x_0 \\
        0, & x \neq x_0
    \end{cases}
\end{equation*}
for all $x \in \Omega$ and the integral of $\delta_{x_0}$ over $\Omega$ is one.
The adverb ``loosely'' shall be emphasized because the standard Lebesgue integration
theory forbids the existence of such a function, but the above characterization
still provides useful intuition. This also means that the partial differential equation
\eqref{eq:intro_problem} should be
understood in some specific sense, namely in the weak sense.
Poisson's equation can be used to
model, for example, the electrostatic potential caused by an electric charge, and
with the Dirac delta load term, the equation can be used to model a point charge
located at the point $x_0$, i.e.\ an idealized charge with no area or volume \cite{wang2020}.

More specifically, we study the problem \eqref{eq:intro_problem}
in bounded polygonal two-dimensional domains
with some additional convexity assumptions
and with Dirichlet and Neumann boundary conditions.
The first objective of this thesis is to consider the unique solvability
of these boundary value problems.
Casas \cite{casas1985} proves this for the Dirichlet problem with
homogeneous boundary values, and we extend this result to the other boundary 
value problems with non-homogeneous boundary values.

The second objective is to study the convergence of the $p$-version of the finite element method
when it is applied to the problem \eqref{eq:intro_problem}.
The convergence of the $h$-version
when applied to the same problem has already been studied
quite extensively in the existing literature.
For example, Casas \cite{casas1985} and Scott \cite{scott1973}
prove convergence in $L^2$ when applied to the homogeneous Dirichlet problem.
Moreover, Schatz and Wahlbin \cite{schatzwahlbin1977} prove estimates for pointwise convergence, and
more recent results have been obtained by Millar et al.\ \cite{millarmuga2021}
who obtain convergence by approximating the
Dirac delta and by Araya et al.\ \cite{arayabehrens2006} who deduce
a posteriori error estimates.

The $p$-version has not received as much attention for the same problem, however.
We extend the $L^2$ convergence result by Casas to the $p$-version resulting in the bound
\begin{equation*}
    \norm{u - u_S}_{L^2(\Omega)} \leq C p^{-1} \left( 1 + \sqrt{\ln(p+1)} \right),
\end{equation*}
where $u$ and $u_S$ are the exact solution and the finite element solution, respectively,
and $C > 0$ is a constant independent of $p$.
The value of the constant $C$ is not known, however.
The bound above is the main result of this thesis,
and all the details will be covered in the subsequent sections.
We also assess the convergence numerically for a Neumann problem for which
the exact solution is known.

The remainder of this thesis is structured as follows.
Section~\ref{sec:preliminaries} presents the preliminary mathematical
concepts that are essential in the weak formulation of partial differential equations
and in the theory of the finite element method.
In Section~\ref{sec:poissons_equation_in_a_polygon},
we first consider how a classically formulated boundary value problem for Poisson's equation
can be transformed into the weak form, and then we consider its solvability.
In particular, we consider the weak formulation and solvability of
the problem \eqref{eq:intro_problem}, reaching the first objective of this thesis.
Section~\ref{sec:finite_element_method} is devoted to the theory
of the finite element method with emphasis on the $p$-version.
A substantial portion of this section deals with the approximation properties
of high-order piecewise polynomials which form the basis for the
convergence properties of the $p$-version.
In Section~\ref{sec:finite_element_solutions_with_a_concentrated_load},
we prove the $L^2$ convergence of the $p$-version
for the problem \eqref{eq:intro_problem} and compare the obtained error
bound to numerical results, reaching the second objective of this thesis.
Finally, Section~\ref{sec:summary} contains a summary of the results, and
we discuss possible future lines of study.

\clearpage

\section{Preliminaries}
\label{sec:preliminaries}

In this section, we review the following preliminary content.
We begin by covering properties related to open subset of $\mathbb{R}^n$
which correspond to the geometric domains where the boundary value problems shall be defined.
We will see in a later section that the solvability of many boundary value problems
follows from abstract results in functional analysis.
In anticipation of this, we go through the definitions and basic properties
of Banach spaces, Hilbert spaces, basic types of linear operators
and dual spaces. Finally, we consider Lebesgue spaces and Sobolev spaces
that correspond to the function spaces where we shall search for the solutions of
the problems.

\subsection{Domains and Boundaries}
\label{subsec:domains}

Throughout the rest of this thesis, $\Omega$ denotes a subset of $\mathbb{R}^n$,
the $n$-dimensional Euclidean space. We start restricting ourselves to the case $n=2$
towards the end of this section, as that is the setting for the boundary value problems
we are looking to study.
The set $\Omega$ is typically assumed to be non-empty, open and connected,
and such a set is called a domain.
Being connected simply means that $\Omega$ cannot be given
as the union of two disjoint non-empty open sets.
Disconnected sets could be handled by considering the
disjoint parts separately. We typically also assume that $\Omega$ is bounded,
i.e.\ it can be enclosed within a ball of finite radius.

Convergence results in finite element analysis typically rely on regularity 
results that guarantee higher-order smoothness of the solutions of the boundary
value problems. Many of these regularity results, in turn, rely on the 
properties of the domain. One such property is convexity which will be one 
of our key assumptions later on.
\begin{definition}[Convex set]
    \label{def:convexity}
    A set $\Omega \subset \mathbb{R}^n$ is said to be convex if for every
    pair of points $x,y \in \Omega$ it holds that $tx + (1-t)y \in \Omega$
    for all $t \in (0,1)$.
\end{definition}

Another important property of a domain is the regularity of its boundary.
The boundary of a domain $\Omega$ is denoted by $\partial \Omega$.
Analogously to functions in analysis, boundaries can be defined to be continuous,
Lipschitz continuous, continuously differentiable, etc.
The definition of a domain with Lipschitz boundary is given below,
and it borrows notation from
Grisvard \cite{grisvard2011} and Schwab \cite{schwab1998}.
See also Figure~\ref{fig:lipschitzboundary} below.
\begin{definition}[Lipschitz boundary]
    \label{def:lipschitzboundary}
    Assume that $n \geq 2$, and let $\Omega \subset \mathbb{R}^n$ be a domain.
    The boundary $\partial \Omega$ is said to be Lipschitz
    if for every $x \in \partial \Omega$ there exist a Lipschitz function
    $g: \mathbb{R}^{n-1} \to \mathbb{R}$, a local coordinate
    system $\{\tilde{e}_1,\dotsc,\tilde{e}_n\} \subset \mathbb{R}^n$
    and constants $d > 0$ and $h > 0$ such that whenever
    a point $y = \sum_{i=1}^{n} \tilde{y}_i \tilde{e}_i \in \mathbb{R}^n$
    satisfies $\abs{\tilde{y}_i} < d$ for all $i=1,\dotsc,n-1$,
    then the following hold.
    \begin{enumerate}[(i)]
        \item \label{def:lipschitzboundary_cond1}
        If $g(\tilde{y}_1,\dotsc,\tilde{y}_{n-1}) = \tilde{y}_n$,
        then $y \in \partial \Omega$.
        \item \label{def:lipschitzboundary_cond2}
        If $g(\tilde{y}_1,\dotsc,\tilde{y}_{n-1}) < \tilde{y}_n
        < g(\tilde{y}_1,\dotsc,\tilde{y}_{n-1}) + h$, then $y \in \Omega$.
        \item \label{def:lipschitzboundary_cond3}
        If $g(\tilde{y}_1,\dotsc,\tilde{y}_{n-1}) > \tilde{y}_n
        > g(\tilde{y}_1,\dotsc,\tilde{y}_{n-1}) - h$, then $y \notin \Omega$.
    \end{enumerate}
\end{definition}
\begin{figure}[t]
    \centering
    \begin{tikzpicture}
        %\draw[help lines] (0,0) grid (10,7);
        \path[draw] (3,0)
            -- (4,2)
            -- (6.5,2)
            -- (8.7,3)
            -- (7.5,6)
            -- (4.5,5.7)
            -- (2,6)
            -- (1,3)
            -- cycle;

        \filldraw[black] (6.5,2) circle (1.5pt);
        \node at (6.75,1.9) {$x$};

        \node at (3.2,3.8) {$\Omega$};
            
        \begin{scope}[shift={(6.5,2)},rotate=12.2219774]
            % Coordinate axes
            \draw[->,thick] (-2.5,-1.3) -- (2.5,-1.3);
            \node[rotate=12.2219774] at (2.55,-1) {$\tilde{e}_1$};
            \draw[->,thick] (0,-1.7) -- (0,2.7);
            \node[rotate=12.2219774] at (0.35,2.75) {$\tilde{e}_2$};

            % Ticks
            \draw[thick] (1.2,-1.45) -- (1.2,-1.15);
            \node[rotate=12.2219774] at (1.2,-1.65) {$d$};
            \draw[thick] (-1.2,-1.45) -- (-1.2,-1.15);
            \node[rotate=12.2219774] at (-1.32,-1.65) {$-d$};

            % Dashed lines
            \draw[dashed] (1.2,-1.3) -- (1.2,0.25993-0.8);
            \draw[dashed] (-1.2,-1.3) -- (-1.2,0.25993-0.8);

            % Double arrows
            \draw[<->] (1.2,0.25993) -- (1.2,0.25993+0.8);
            \node[rotate=12.2219774] at (1.4,0.7) {$h$};
            \draw[<->] (1.2,0.25993) -- (1.2,0.25993-0.8);
            \node[rotate=12.2219774] at (1.4,-0.09) {$h$};
            \draw[<->] (-1.2,0.25993) -- (-1.2,0.25993+0.8);
            \draw[<->] (-1.2,0.25993) -- (-1.2,0.25993-0.8);

            % Upper and lower bounds
            \draw[rotate around={-12.2219774:(-1.2,0.25993+0.8)}]
                (-1.2,0.25993+0.8) -- (-1.2+1.22,0.25993+0.8);
            \draw[rotate around={-12.2219774:(-1.2,0.25993-0.8)}]
                (-1.2,0.25993-0.8) -- (-1.2+1.22,0.25993-0.8);
            \draw[rotate around={12.2219774:(1.2,0.25993+0.8)}]
                (1.2,0.25993+0.8) -- (1.2-1.22,0.25993+0.8);
            \draw[rotate around={12.2219774:(1.2,0.25993-0.8)}]
                (1.2,0.25993-0.8) -- (1.2-1.22,0.25993-0.8);
        \end{scope}
    \end{tikzpicture}
    \caption{A domain with Lipschitz boundary.}
    \label{fig:lipschitzboundary}
\end{figure}
In other words, the requirement \ref{def:lipschitzboundary_cond1}
means that a Lipschitz boundary is locally the image of a Lipschitz function.
The requirements \ref{def:lipschitzboundary_cond2} and 
\ref{def:lipschitzboundary_cond3} mean that the set $\Omega$ is not allowed
to be on both sides of its boundary.
Other types of boundary regularity
have analogous definitions by just replacing the word ``Lipschitz'' with e.g.\
``continuously differentiable''.

In finite element analysis, the domain is typically modeled as a union of polygons.
A polygon consists of vertices and line segments that connect the vertices.
In general, a polygonal domain can have holes, that are of course polygonal as well,
but a convex polygonal domain cannot have any holes.
Assuming that the angle between adjacent line segments is never $0$ nor $2\pi$,
a polygonal domain has Lipschitz boundary \cite{grisvard2011}.
This is an important property, as having Lipschitz boundary is needed
for many later results.
Clearly, the boundary of a polygonal domain is not differentiable.

\subsection{Functional Analysis}
\label{subsec:functionalanalysis}

We describe here some of the most fundamental concepts and results from functional 
analysis. For a more complete reference, see for example \cite{rudin1991} and \cite{rudin1986}.

\subsubsection{Vector Spaces, Normed Spaces and Inner Product Spaces}
\label{subsubsec:normedspaces}

We begin with the definition of a vector space over the real numbers.
\begin{definition}[Real vector space]
    \label{def:vectorspace}
    A vector space over the real numbers is a non-empty set $X$ equipped
    with the following two operations.
    \begin{enumerate}[(i)]
        \item Addition: $x+y \in X$ for all $x,y \in X$.
        \item Scalar multiplication: $a x \in X$ for all $a \in \mathbb{R}$
        and $x \in X$.
    \end{enumerate}
    In addition, the following axioms hold for all $x,y,z \in X$
    and $a,b \in \mathbb{R}$.
    \begin{enumerate}[(i)]
        \item Commutativity: $x+y=y+x$.
        \item Associativity: $(x+y)+z=x+(y+z)$ and $(ab)x=a(bx)$.
        \item Existence of additive identity: there exists a zero vector $0 \in X$
        such that $x+0=x$.
        \item Existence of additive inverse: for every $x \in X$, there exists
        a $y \in X$ such that $x+y=0$.
        \item Multiplicative identity: $1x=x$.
        \item Distributivity: $a(x+y)=ax+ay$ and $(a+b)x=ax+bx$.
    \end{enumerate}
\end{definition}
By a vector space, we shall always mean a real vector space.
A subset of a vector space is said to be a vector subspace,
or simply a subspace, if it is closed under the operations of addition
and scalar multiplication of the ambient vector space.
\begin{definition}[Vector subspace]
    \label{def:subspace}
    A non-empty subset $S$ of a vector space $X$
    is said to be a vector subspace of $X$
    if $ax+by \in S$ for all $a,b \in \mathbb{R}$ and $x,y \in S$.
\end{definition}

Next, we define a norm on a vector space which can be thought to be a measure
of the magnitude of a vector.
\begin{definition}[Norm]
    \label{def:norm}
    A norm on a vector space $X$ is a function $\norm{\cdot}: X \to \mathbb{R}$
    that satisfies the following properties for all $x,y \in X$
    and $a \in \mathbb{R}$.
    \begin{enumerate}[(i)]
        \item $\norm{x+y} \leq \norm{x} + \norm{y}$.
        \item $\norm{ax} = \abs{a} \norm{x}$.
        \item $\norm{x} \geq 0$, and $\norm{x}=0$ if and only if $x=0$.
    \end{enumerate}
\end{definition}
A vector space equipped with a norm is called a normed space.
For example, $\mathbb{R}^n$ is a normed space with the Euclidean norm.
We shall always assume that a normed space is equipped with the usual
norm topology. This means that given a normed space $X$, a subset $S \subset X$
is open if and only if for every $x \in S$ there exists a ball
$B(x,\varepsilon) = \{ y \in X : \norm{x-y} < \varepsilon \}$ with a positive
radius $\varepsilon > 0$ such that $B(x,\varepsilon) \subset S$.
If a function $s: X \to \mathbb{R}$ does not satisfy the condition
that $s(x) = 0$ implies $x = 0$ but otherwise satisfies Definition~\ref{def:norm},
then $s$ is said to be a seminorm.

A norm enables us to consider the convergence of a sequence of vectors.
\begin{definition}[Convergence of a sequence]
    \label{def:convergence}
    A sequence of vectors $(x_i)_{i=1}^{\infty}$ in a normed space $X$
    is said to converge to the limit $x \in X$ if for every $\varepsilon > 0$ 
    there exists a number $N > 0$
    such that $\norm{x-x_i} < \varepsilon$ whenever $i \geq N$.
\end{definition}
The condition in Definition~\ref{def:convergence} can be written succinctly
as $\lim_{i \to \infty} \norm{x-x_i} = 0$.
We can use convergence to define the concept of a closure point of a set.
\begin{definition}[Closure point]
    \label{def:closure_point}
    Let $S$ be a subset of a normed space $X$.
    A point $x \in X$ is said to be a closure point of $S$
    if there exists a sequence $(x_i)_{i=1}^{\infty}$ in $S$
    that converges to $x$.
    The set of all closure points of $S$ is the closure of $S$,
    and it is denoted by $\overline{S}$.
\end{definition}
The definition of the closure has some useful consequences.
It is a standard result that $\overline{S}$ is closed.
In fact, $S$ is closed precisely when $S = \overline{S}$.
Since obviously $S \subset \overline{S}$,
this means that to prove closedness of $S$ we only need to show that
every closure point of $S$ belongs to $S$, i.e.\ the limit belongs to $S$.
If $\overline{S} = X$, it is said that the subset $S$ is dense in the
normed space $X$. If $S$ is dense in $X$ and $S$ has a well-known structure
with useful properties, then these properties can often be extended to $X$
via so-called density arguments.

The closure and density essentially allow one to pick a converging sequence
for a given point. Compactness, on the other hand, allows one to pick
a limit point for a given (sub)sequence.
\begin{definition}[Compact set]
    \label{def:compactsubset}
    A subset $S$ of a normed space $X$ is said to be compact if every sequence
    in $S$ has a subsequence that has a limit in $S$.
    Moreover, a subset $S$ is called precompact if $\overline{S}$
    is compact.
\end{definition}
Definition~\ref{def:compactsubset} is more generally referred to as
sequential compactness. Compactness generally means that every open cover of the
set $S$ has a finite subcover, but, in normed spaces, the more general definition of
compactness and sequential compactness are equivalent.

An important class of normed spaces is Banach spaces for which we need
to introduce two key concepts: Cauchy sequences and completeness.
\begin{definition}[Cauchy sequence]
    \label{def:cauchysequence}
    A sequence $(x_i)_{i=1}^{\infty}$ in a normed space $X$ is said to be
    a Cauchy sequence if for every $\varepsilon > 0$ there exists
    a number $N > 0$ such that
    $\norm{x_i - x_j} < \varepsilon$ whenever $i,j \geq N$.
\end{definition}
\begin{definition}[Completeness]
    \label{def:completeness}
    A normed space $X$ is said to be complete if every Cauchy sequence in $X$
    converges to a limit in $X$.
\end{definition}
A Banach space is simply a normed space that is complete, i.e.\ it
satisfies Definition~\ref{def:completeness}.
It is easy to see that a closed subspace $S$ of a Banach space $X$ is also
a Banach space: a Cauchy sequence in $S$ is also a Cauchy sequence in $X$,
which means that it converges to some point $x \in X$, but this implies
that $x$ is a closure point of $S$, which then implies that $x \in S$
since $S$ is closed and, thus, $S$ is complete.

Hilbert spaces are an important special case of Banach spaces
for which we need the concept of an inner product.
An inner product can be thought to be a measure of the orthogonality
between two vectors.
\begin{definition}[Inner product]
    \label{def:innerproduct}
    An inner product on a vector space $X$ is a function
    $\innerprod{\cdot}{\cdot}: X \times X \to \mathbb{R}$
    that satisfies the following properties for all $x,y,z \in X$
    and $a \in \mathbb{R}$.
    \begin{enumerate}[(i)]
        \item $\innerprod{x+y}{z}=\innerprod{x}{z} + \innerprod{y}{z}$.
        \item $\innerprod{ax}{y} = a\innerprod{x}{y}$.
        \item $\innerprod{x}{y} = \innerprod{y}{x}$.
        \item $\innerprod{x}{x} \geq 0$, and $\innerprod{x}{x}=0$
        if and only if $x=0$.
    \end{enumerate}
\end{definition}
A vector space equipped with an inner product is called an inner product space.
An inner product also induces a norm. Namely, given an inner product space $X$,
the mapping $x \mapsto \sqrt{\innerprod{x}{x}}$ defines a norm on $X$,
which means that $X$ is also a normed space.
An inner product space that is complete with respect to the induced norm
is said to be a Hilbert space.

An important result for inner product spaces is the Cauchy-Schwarz inequality.
\begin{theorem}[Cauchy-Schwarz inequality]
    \label{thm:cauchyschwarz}
    Let $X$ be an inner product space with the inner product
    $\innerprod{\cdot}{\cdot}$ and the induced norm $\norm{\cdot}$.
    Then for all $x,y \in X$, it holds that
    \begin{equation*}
        \abs{\innerprod{x}{y}} \leq \norm{x} \norm{y}.
    \end{equation*}
\end{theorem}
\begin{proof}
    Let $x,y \in X$. Then
    \begin{align*}
        0 
        &\leq \norm{x + y}^2 \\
        &= \innerprod{x + y}{x + y} \\
        &= \innerprod{x}{x} + \innerprod{y}{y} + 2\innerprod{x}{y} \\
        &= \norm{x}^2 + \norm{y}^2 + 2\innerprod{x}{y}
    \end{align*}
    so that $-\innerprod{x}{y} \leq (\norm{x}^2 + \norm{y}^2)/2$.
    An identical argument with $x-y$ instead of $x+y$ yields
    $\innerprod{x}{y} \leq (\norm{x}^2 + \norm{y}^2)/2$.
    Combining these two inequalities yields
    \begin{equation}
        \label{eq:cauchyschwarzhelperineq}
        \abs{\innerprod{x}{y}} \leq (\norm{x}^2 + \norm{y}^2)/2.
    \end{equation}
    If $x=0$ or $y=0$, the claim is obviously true.
    Let us thus assume that $x \neq 0 \neq y$. Now
    \begin{align*}
        \abs{\innerprod{x}{y}}
        &= \norm{x} \norm{y} 
           \abs*{\innerprod{\frac{x}{\norm{x}}}{\frac{y}{\norm{y}}}} \\
        &\leq \norm{x} \norm{y} (1+1)/2 \\
        &= \norm{x} \norm{y},
    \end{align*}
    where we used the inequality \eqref{eq:cauchyschwarzhelperineq}.
\end{proof}

\subsubsection{Linear Operators}
\label{subsubsec:linearoperators}

In functional analysis, the noun operator is typically a synonym for a function. 
Operators that are linear are arguably among the most important entities
in functional analysis. We consider here some basic types of linear operators
that we will need later. Specifically, we cover
continuous, bounded and compact linear operators.
\begin{definition}[Linear operator]
    \label{def:linearfunction}
    An operator $A: X \to Y$ between two vector spaces $X$ and $Y$
    is said to be linear if it satisfies the following properties
    for all $x,y \in X$ and $a \in \mathbb{R}$.
    \begin{enumerate}[(i)]
        \item $A(x+y) = A(x) + A(y)$.
        \item $A(ax) = a A(x)$.
    \end{enumerate}
\end{definition}
An obvious corollary to the above definition is that $A(0)=0$ since
$A(0)=A(x-x)=A(x)-A(x)=0$.
When an operator is linear, we adopt the common practice of dropping the parentheses
around a singular argument. That is, $Ax$ is used to mean $A(x)$.

The definition of a continuous linear operator between two normed spaces
is identical to the definition of a continuous function typically encountered
in e.g.\ calculus.
\begin{definition}[Continuous linear operator]
    \label{def:continuousoperator}
    A linear operator $A: X \to Y$ between two normed spaces
    $X$ and $Y$ is said to be continuous
    if for every $x \in X$ and for every $\varepsilon > 0$
    there exists a $\delta > 0$ such that $\norm{Ax - Ay}_Y < \varepsilon$
    for all $y \in X$ satisfying $\norm{x-y}_X < \delta$.
\end{definition}
Next, we define a bounded linear operator which should not be confused
with the usual definition of boundedness of a function.
\begin{definition}[Bounded linear operator]
    \label{def:boundedness}
    A linear operator $A: X \to Y$ between two normed spaces
    $X$ and $Y$ is said to be bounded
    if there exists a constant $C > 0$ such that
    $\norm{Ax}_Y \leq C \norm{x}_X$ for all $x \in X$.
\end{definition}
In normed spaces, there is an important connection between
continuous linear operators and bounded linear operators.
Namely, they are precisely the same concept.
We prove this basic but fundamental result.
\begin{theorem}
    \label{thm:boundedcontinuous}
    Let $A: X \to Y$ be a linear operator between normed spaces $X$ and $Y$.
    Then $A$ is continuous if and only if it is bounded.
\end{theorem}
\begin{proof}
    Assume first that $A$ is continuous.
    $A$ is then continuous at the origin $0 \in X$, which
    means that for $\varepsilon=1$ there exists a $\delta > 0$ such that
    $\norm{Ay}_Y < 1$ for all $y \in X$ satisfying
    $\norm{y}_X < \delta$.
    Let now $x \in  X$, and assume that $x \neq 0$. Define
    \begin{equation*}
        y = \frac{\delta}{2} \frac{x}{\norm{x}_X}
    \end{equation*}
    for which clearly $\norm{y}_X = \delta/2 < \delta$.
    Now by using the linearity and continuity of $A$, we get
    \begin{equation*}
        \frac{\delta}{2 \norm{x}_X} \norm{Ax}_Y
        = \norm*{A\left( \frac{\delta}{2} \frac{x}{\norm{x}_X} \right)}_Y
        = \norm{Ay}_Y
        < 1.
    \end{equation*}
    Multiplying each side by $2 \norm{x}_X / \delta$ yields
    \begin{equation*}
        \norm{Ax}_Y \leq \frac{2}{\delta} \norm{x}_X.
    \end{equation*}
    If $x=0$, this inequality
    obviously holds as well since $\norm{Ax}_Y = 0 =\norm{x}_X$. Thus,
    $A$ is bounded with the constant $C = 2/\delta > 0$.

    Then assume that $A$ is bounded.
    Let $x \in X$, $\varepsilon > 0$, and set $\delta = \varepsilon / C$,
    where $C$ is the constant in the definition of boundedness. Now
    for all $y \in X$ satisfying $\norm{x-y}_X < \delta$, it holds that
    \begin{equation*}
        \norm{Ax - Ay}_Y
        = \norm{A(x-y)}_Y
        \leq C \norm{x-y}_X
        < C \delta
        = \varepsilon,
    \end{equation*}
    where we used the linearity and boundedness of $A$. Thus, $A$ is continuous.
\end{proof}

A compact linear operator is defined as follows.
\begin{definition}[Compact linear operator]
    \label{def:compactoperator}
    A linear operator $A: X \to Y$ between two normed spaces $X$ and $Y$
    is said to be compact if $A$ maps every bounded set in $X$ to
    a precompact set in $Y$.
\end{definition}
A precompact set was defined in Definition~\ref{def:compactsubset},
and a bounded set $S$ in $X$ is naturally understood as the existence of a radius $r > 0$
such that $S \subset B(0,r)$.

\subsubsection{Dual Spaces}
\label{subsubsec:dualspaces}

The concept of a dual space and, in particular, the so-called Riesz representation 
theorem will be essential tools
when we study the existence and uniqueness of solutions to
weakly formulated boundary value problems.
\begin{definition}[Dual space]
    \label{def:dualspace}
    The dual space of a normed space $X$, denoted by $X'$,
    is the vector space of all real-valued continuous linear operators defined on the space $X$. The vector space operations are the usual pointwise operations:
    for all $\varphi, \psi \in X'$, $a \in \mathbb{R}$ and $x \in X$,
    the addition and scalar multiplication are defined by
    \begin{enumerate}[(i)]
        \item $(\varphi + \psi)(x) = \varphi(x) + \psi(x)$,
        \item $(a \varphi)(x) = a \varphi(x)$.
    \end{enumerate}
\end{definition}
Real-valued operators defined on a vector space are more commonly referred to
as functionals. As an example, we now interpret the Dirac delta $\delta_{x_0}$ 
as an element of the dual space of $C(\overline{\Omega})$, where
$C(\overline{\Omega})$ is the normed space of bounded uniformly continuous
functions defined on a domain $\Omega \subset \mathbb{R}^n$.
The norm is given by
\begin{equation*}
    \norm{u}_{C(\overline{\Omega})} = \sup_{x \in \Omega} \abs{u(x)}.
\end{equation*}
Let $x_0 \in \Omega$. We define the Dirac delta as a functional
$\delta_{x_0}: C(\overline{\Omega}) \to \mathbb{R}$ that evaluates its argument
at the point $x_0$, that is, $\delta_{x_0}(u) = u(x_0)$ for all
$u \in C(\overline{\Omega})$.
To show that $\delta_{x_0} \in C(\overline{\Omega})'$,
we need to show that it is linear and continuous.
Linearity is easy: for all $u,v \in C(\overline{\Omega})$
and $a,b \in \mathbb{R}$, we have
\begin{equation*}
    \delta_{x_0}(au+bv)
    = (au+bv)(x_0)
    = au(x_0)+bv(x_0)
    = a \delta_{x_0}(u) + b \delta_{x_0}(v).
\end{equation*}
Boundedness is also easy: for all $u \in C(\overline{\Omega})$, we have
\begin{equation*}
    \abs{\delta_{x_0}(u)}
    = \abs{u(x_0)}
    \leq \sup_{x \in \Omega} \abs{u(x)}
    = \norm{u}_{C(\overline{\Omega})}.
\end{equation*}
By Theorem~\ref{thm:boundedcontinuous}, boundedness and continuity
mean the same thing so $\delta_{x_0}$ is continuous.
Thus, $\delta_{x_0} \in C(\overline{\Omega})'$.

Given an inner product space $X$ and a vector $y \in X$,
it immediately follows from the definition
of an inner product (Definition~\ref{def:innerproduct})
and the Cauchy-Schwarz inequality (Theorem~\ref{thm:cauchyschwarz})
that the functional $x \mapsto \innerprod{y}{x}$ defined on $X$
belongs to the dual space $X'$. When $X$ is also a Hilbert space,
it turns out that every functional in $X'$ is of this form.
This is the message of the Riesz representation theorem.
\begin{theorem}[Riesz representation theorem]
    \label{thm:rieszrepresentationtheorem}
    Let $X$ be a Hilbert space with the inner product $\innerprod{\cdot}{\cdot}$.
    Let $\varphi \in X'$. Then there exists a unique $y \in X$ such that
    $\varphi(x) = \innerprod{y}{x}$ for all $x \in X$.
\end{theorem}
For a proof, see for example \cite[Theorem~4.12 on p.~81]{rudin1986}.

\subsection{Lebesgue Spaces}
\label{subsec:lebesguespaces}

This subsection is concerned with the properties of Lebesgue integrable functions.
We skip the complete definitions of the Lebesgue measure and Lebesgue integration,
for which the reader is referred to \cite{folland1999}.
At the end of this subsection, we do, however, consider the problem of defining
an integral over the boundary of a domain with Lipschitz boundary.

\subsubsection{Lebesgue Integral}
\label{subsubsec:lebesgueintegral}

The Lebesgue measure over $\mathbb{R}^n$ is essentially a real-valued function
that returns the $n$-dimensional volume of practically any set in $\mathbb{R}^n$,
excluding some pathological sets, and that has properties one would typically
expect such a function to have, e.g.\
invariance under rigid motion, such as translation and rotation,
and that the measure of the union of two disjoint
sets is the sum of their individual measures. We denote the Lebesgue measure
of a measurable set $\Omega \subset \mathbb{R}^n$ by $\abs{\Omega}$.

The notation for the Lebesgue integral of a measurable function
$f: \Omega \to \mathbb{R}$ over a measurable set $\Omega \subset \mathbb{R}^n$
is the usual
\begin{equation}
    \label{eq:integral_of_f}
    \int_{\Omega} f \diff x,
\end{equation}
given that the integral exists. When $\Omega = [a,b] \in \mathbb{R}$,
we simply write $\int_{a}^{b} f \diff x$.
The function $f$ is said to be integrable if its integral \eqref{eq:integral_of_f}
exists and is finite.
Integrability underlays the definition of Lebesgue spaces.
\begin{definition}[Lebesgue spaces]
    \label{def:lebesguespaces}
    Let $\Omega \subset \mathbb{R}^n$ be open,
    and let $1 \leq s < \infty$ be a real number.
    The Lebesgue space $L^s(\Omega)$ is the normed space of functions
    $u: \Omega \to \mathbb{R}$ that satisfy
    \begin{equation}
        \label{eq:lpintegral}
        \int_{\Omega} \abs{u}^s \diff x < \infty,
    \end{equation}
    and the norm is given by
    \begin{equation*}
        \norm{u}_{L^s(\Omega)}
        = \left( \int_{\Omega} \abs{u}^s \diff x \right)^{1/s}.
    \end{equation*}
    Moreover, the space $L^2(\Omega)$ is defined to be an inner product space
    with the inner product
    \begin{equation*}
        \innerprod{u}{v}_{L^2(\Omega)} = \int_{\Omega} uv \diff x.
    \end{equation*}
\end{definition}
In fact, the Lebesgue spaces $L^s(\Omega)$ are Banach spaces,
and $L^2(\Omega)$ is a Hilbert space.

We defined $u \in L^s(\Omega)$ as a regular function $u: \Omega \to \mathbb{R}$.
However, this is not strictly speaking valid because such $u$ is not uniquely defined.
To see why, assume that $v$ is another function that is equal to $u$ everywhere
in $\Omega$ except in a non-empty set whose Lebesgue measure is zero,
e.g.\ a countable set of points.
Now $u-v$ is zero everywhere except in a non-empty set with Lebesgue measure zero,
but then clearly $\norm{u-v}_{L^s(\Omega)} = 0$, which implies that $u=v$
in the normed space $L^s(\Omega)$ even though they are technically different functions.
The proper way to define $u \in L^s(\Omega)$ would be to consider it as
an equivalence class of functions where two functions are considered to be
equivalent if they satisfy the integrability condition \eqref{eq:lpintegral}
and they are equal almost everywhere, i.e.\ everywhere except in a set
with Lebesgue measure zero. However, for simplicity, we stick to treating
$u \in L^s(\Omega)$ as a function and say that $u=v$ if they are equal almost 
everywhere in $\Omega$. An obvious caveat regarding this choice is that
the pointwise values of $u$ are not necessarily well-defined.

The space $L^{\infty}(\Omega)$ can be defined as the normed space of functions
(again, equivalence classes to be pedantic) that are essentially bounded
over $\Omega$. A function $u: \Omega \to \mathbb{R}$ is said to be
essentially bounded over $\Omega$
if there exists a constant $M > 0$ such that $\abs{u(x)} \leq M$
for almost every $x \in \Omega$. That is, $L^{\infty}(\Omega)$
is the normed space of functions $u: \Omega \to \mathbb{R}$ that satisfy
\begin{equation*}
    \norm{u}_{L^{\infty}(\Omega)}
    = \esssup_{x \in \Omega} \abs{u(x)}
    < \infty,
\end{equation*}
where
\begin{equation*}
    \esssup_{x \in \Omega} u(x)
    = \inf \{ M \in \mathbb{R} : \abs{\{ x \in \Omega : u(x) > M \}} = 0 \}
\end{equation*}
is the essential supremum of $u$ over $\Omega$.
The space $L^{\infty}(\Omega)$ is a Banach space as well.

Different $L^s$-norm inequalities will be the backbone of many applications
later. An important inequality is Hölder's inequality.
\begin{theorem}[Hölder's inequality]
    \label{thm:höldersineq}
    Let $1 \leq s \leq \infty$ and $1 \leq s' \leq \infty$ be
    such that $1/s + 1/s' = 1$
    (when $s=\infty$ or $s'=\infty$, set $s'=1$ and $s=1$, respectively).
    Let $u \in L^s(\Omega)$ and $v \in L^{s'}(\Omega)$. Then $uv \in L^1(\Omega)$ and
    \begin{equation*}
        \norm{uv}_{L^1(\Omega)} \leq \norm{u}_{L^s(\Omega)}
            \norm{v}_{\vphantom{L^s(\Omega)} \smash{L^{s'}(\Omega)}}.
    \end{equation*}
\end{theorem}
For a reference, see \cite[Theorem~6.2 on p.~182]{folland1999} for the case $1<s,s'<\infty$
and \cite[Theorem~6.8 on p.~184]{folland1999} for the case $s=\infty$ or $s'=\infty$.
Numbers $1 \leq s,s' \leq \infty$ that satisfy $1/s + 1/s' = 1$
are called conjugate exponents. Theorem~\ref{thm:höldersineq} actually
holds in an arbitrary measure space. In particular, it holds for the
counting measure, which implies that for any two real-valued sequences $(x_i)_{i=1}^{\infty}$
and $(y_i)_{i=1}^{\infty}$ it holds that
\begin{equation*}
    \sum_{i=1}^{\infty} \abs{x_i y_i}
    \leq \left( \sum_{i=1}^{\infty} \abs{x_i}^s \right)^{\frac{1}{s}}
        \left( \sum_{i=1}^{\infty} \abs{y_i}^{s'} \right)^{\frac{1}{s'}}.
\end{equation*}

When $\Omega$ is bounded, Hölder's inequality implies the imbedding
$L^r(\Omega) \subset L^s(\Omega)$ when $1 \leq s \leq r \leq \infty$,
which we prove next.
\begin{theorem}
    \label{thm:lpimbedding}
    Assume that $\Omega$ is bounded, that is, $\abs{\Omega} < \infty$.
    Let $1 \leq s \leq r \leq \infty$.
    Then there exists a constant $C > 0$ such that
    \begin{equation*}
        \norm{u}_{L^s(\Omega)} \leq C \norm{u}_{L^r(\Omega)}
    \end{equation*}
    for all $u \in L^r(\Omega)$. In other words, $L^r(\Omega) \subset L^s(\Omega)$.
\end{theorem}
\begin{proof}
    The case $s = r$ is trivial so assume that $s < r$.
    Let $u \in L^r(\Omega)$, and assume first that $r < \infty$.
    
    Define $v \equiv 1$ on $\Omega$. Since $\Omega$ is bounded, we have
    for all $1 \leq q < \infty$ that
    \begin{equation*}
        \norm{v}_{L^q(\Omega)}^q
        = \int_{\Omega} 1 \diff x
        = \abs{\Omega}
        < \infty
        \quad \text{and} \quad
        \norm{v}_{L^{\infty}(\Omega)} = 1 < \infty.
    \end{equation*}
    That is, $v \in L^q(\Omega)$ for all $1 \leq q \leq \infty$.
    
    Since $u \in L^r(\Omega)$, clearly $\abs{u}^s \in L^{r/s}(\Omega)$.
    Note that $r/s > 1$, and its conjugate exponent is given by
    $r/(r-s)$. Now by applying Hölder's inequality to
    $\abs{u}^s \in L^{r/s}(\Omega)$ and $v \in L^{r/(r-s)}(\Omega)$,
    we get
    \begin{align*}
        \norm{u}_{L^s(\Omega)}^s
        &= \int_{\Omega} \abs{u}^s \diff x \\
        &= \int_{\Omega} v \abs{u}^s \diff x \\
        &\leq \norm{v}_{L^{\frac{r}{r-s}}(\Omega)}
              \left( \int_{\Omega} \abs{u}^r \diff x \right)^{\frac{s}{r}} \\
        &= \abs{\Omega}^{\frac{r-s}{r}} \norm{u}_{L^r(\Omega)}^s.
    \end{align*}
    Taking the $s$th root from both sides proves the claim for $r < \infty$.

    If $u \in L^{\infty}(\Omega)$, then clearly $\abs{u}^s \in L^{\infty}(\Omega)$ as well
    with $\norm{\abs{u}^s}_{L^{\infty}(\Omega)} = \norm{u}_{L^{\infty}(\Omega)}^s$.
    Now by applying Hölder's inequality to
    $\abs{u}^s \in L^{\infty}(\Omega)$ and $v \in L^1(\Omega)$, we get
    \begin{align*}
        \norm{u}_{L^s(\Omega)}^s
        &= \int_{\Omega} v \abs{u}^s \diff x \\
        &\leq \norm{v}_{L^1(\Omega)} \norm{\abs{u}^s}_{L^{\infty}(\Omega)} \\
        &= \abs{\Omega} \norm{u}_{L^{\infty}(\Omega)}^s.
    \end{align*}
    Taking the $s$th root from both sides proves the claim for $r = \infty$.
\end{proof}

The next theorem is the dominated convergence theorem which we will use later
to verify the integrability of certain functions.
For a proof, see \cite[Theorem~2.24 on p.~54]{folland1999}.
\begin{theorem}[The dominated convergence theorem]
    \label{thm:dominated_convergence}
    Let $\Omega \subset \mathbb{R}^n$ be open.
    Let $(f_i)_{i=1}^{\infty}$ be a sequence of functions in $L^1(\Omega)$
    that converges pointwise almost everywhere in $\Omega$ to a function $f$.
    Assume that there exists a function $g \in L^1(\Omega)$ such that
    $\abs{f_i} \leq g$ almost everywhere in $\Omega$ for all $i=1,2,\dotsc$. Then
    $f \in L^1(\Omega)$ and
    \begin{equation*}
        \lim_{i \to \infty} \int_{\Omega} f_i \diff x
        = \int_{\Omega} f \diff x.
    \end{equation*}
\end{theorem}

We will also need the following result which essentially states that
the average values of a continuous function over balls or spheres
with a fixed center point
converge to the value of the function at that center point as the radius tends to zero.
\begin{theorem}
    \label{thm:lebesgue_differentiation_theorem}
    Let $\Omega \subset \mathbb{R}^n$ be open, and
    let $f: \Omega \to \mathbb{R}$ be a continuous function.
    Then for all $x \in \Omega$, it holds that
    \begin{equation*}
        \lim_{r \to 0^+}
            \frac{1}{\abs{B(x,r)}}
                \int_{B(x,r)} f(y) \diff y = f(x)
    \end{equation*}
    and
    \begin{equation*}
        \lim_{r \to 0^+}
            \frac{1}{\abs{\partial B(x,r)}}
                \int_{\partial B(x,r)} f(y) \diff S = f(x).
    \end{equation*}
\end{theorem}
\begin{proof}
    Let $x \in \Omega$ and $\varepsilon > 0$.
    The function $f$ is continuous at $x$, which means that there exists
    an $r_0 > 0$ such that $\abs{f(x)-f(y)} < \varepsilon$ whenever
    $y \in B(x,r_0) \subset \Omega$. Now whenever $r \leq r_0$, it holds that
    \begin{align*}
        \abs*{\frac{1}{\abs{B(x,r)}} \int_{B(x,r)} f(y) \diff y - f(x)}
        &= \abs*{\frac{1}{\abs{B(x,r)}} \int_{B(x,r)} f(y) - f(x) \diff y} \\
        &\leq \frac{1}{\abs{B(x,r)}} \int_{B(x,r)} \abs{f(y) - f(x)} \diff y \\
        &\leq \frac{1}{\abs{B(x,r)}} \int_{B(x,r)} \varepsilon \diff y \\
        &= \varepsilon.
    \end{align*}
    Thus, by the definition of a limit, it holds that
    \begin{equation*}
        \lim_{r \to 0^+}
            \frac{1}{\abs{B(x,r)}}
                \int_{B(x,r)} f(y) \diff y = f(x).
    \end{equation*}
    The proof with the spheres $\partial B(x,r)$ is identical.
\end{proof}

\subsubsection{Boundary Integral}
\label{subsubsec:boundaryintegral}

This short section presents how to define integration over the boundary of a bounded domain
$\Omega \subset \mathbb{R}^n$, $n \geq 2$, with Lipschitz boundary.
We give the same definition that is given by e.g.\
Schwab \cite{schwab1998} and Ne{\v c}as \cite{necas2011}.

Let $Q_d$ denote an $(n-1)$-dimensional hypercube centered at the origin
and with side length $2d > 0$. By Definition~\ref{def:lipschitzboundary},
for every $x \in \partial \Omega$, there exists a Lipschitz function
$g: \mathbb{R}^{n-1} \to \mathbb{R}$ and an open set
\begin{equation*}
    U = \left\{ y = \sum_{i=1}^{n} \tilde{y}_i \tilde{e}_i \in \mathbb{R}^n :
        \tilde{y}' = (\tilde{y}_1,\dotsc,\tilde{y}_{n-1}) \in Q_d, \,
        \abs{\tilde{y}_n - g(\tilde{y}')} < h
    \right\}
\end{equation*}
that covers a part of the boundary $\partial \Omega$.
The collection of the sets $U$ for all $x \in \partial \Omega$ is
an open cover of $\partial \Omega$. Since $\Omega$ is bounded, its boundary
is compact. This means that the open cover has a finite subcover which
we denote by $\{ U_j \}_{j=1}^{m}$. The corresponding Lipschitz functions
are denoted by $g_j$ for $1 \leq j \leq m$. Similarly, we denote
$Q_{d_j}$ and $\{ \tilde{e}_i^j \}_{i=1}^{n}$.

The functions $g_j$ are the graphs of $\partial \Omega \cap U_j$.
Thus, it makes sense to define the integral of a function
$f: \partial \Omega \to \mathbb{R}$ over $\partial \Omega \cap U_j$
as
\begin{equation}
    \label{eq:localboundaryintegral}
    \int_{\partial \Omega \cap U_j} f \diff S
    = \int_{Q_{d_j}} f\left( \sum_{i=1}^{n-1} \tilde{y}_i \tilde{e}_i^j + 
        g_j(\tilde{y}') \tilde{e}_n^j \right)
            \sqrt{1 + \abs{\nabla g_j}^2} \diff \tilde{y}',
\end{equation}
which is essentially a line integral when $n=2$ and a surface integral when $n=3$.
For simplicity, we assume in \eqref{eq:localboundaryintegral}
that the local coordinate system $\{ \tilde{e}_1,\dotsc,\tilde{e}_n \}$
is obtained from the standard basis
with rotation and translation but with no scaling.
Note that since $g_j$ is Lipschitz,
the gradient $\nabla g_j$ exists almost everywhere and is essentially bounded
by Rademacher's theorem \cite{schwab1998}.

To then define integration over the whole boundary, note that since
$\{ U_j \}_{j=1}^{m}$ is an open cover of $\partial \Omega$, there exists
a partition of unity of $\partial \Omega$ subordinate to $\{ U_j \}_{j=1}^{m}$. 
That is, there exist functions $\eta_j \in C_0^{\infty}(U_j)$,
$1 \leq j \leq m$, such that
\begin{equation*}
    0 \leq \eta_j \leq 1
    \quad \text{and} \quad
    \sum_{j=1}^{m} \eta_j(x) = 1 \text{ for all } x \in \partial \Omega.
\end{equation*}
The space $C_0^{\infty}(U_j)$ is the space of infinitely differentiable functions
whose support is a compact subset of $U_j$.
We now define the integral of $f: \partial \Omega \to \mathbb{R}$ over $\partial \Omega$ by
\begin{equation}
    \label{eq:boundaryintegral}
    \int_{\partial \Omega} f \diff S
    = \sum_{j=1}^{m} \int_{\partial \Omega \cap U_j} \eta_j f \diff S,
\end{equation}
where the right-hand side integrals are defined by \eqref{eq:localboundaryintegral}.
This definition does not depend on the cover $\{ U_j \}_{j=1}^{m}$
or the partition of unity $\{ \eta_j \}_{j=1}^{m}$ \cite{necas2011}.

The normed spaces $L^s(\partial \Omega)$ for $1 \leq s \leq \infty$ are defined
analogously to the spaces $L^s(\Omega)$. Two functions in $L^s(\partial \Omega)$
are again identified if they are equal everywhere in $\partial \Omega$ except
possibly in a non-empty set with zero $(n-1)$-dimensional measure over
$\partial \Omega$. Similarly, the definition of the space $L^{\infty}(\partial 
\Omega)$ uses an $(n-1)$-dimensional measure to define essentially bounded
functions. We skip the full construction of a measure over $\partial \Omega$
and merely characterize the measure of a measurable
set $\Gamma \subset \partial \Omega$ as the integral of its indicator
function over $\partial \Omega$ via \eqref{eq:boundaryintegral}.

Hölder's inequality can be extended to the spaces $L^s(\partial \Omega)$,
which we prove next.
\begin{theorem}
    \label{thm:boundaryhöldersineq}
    Let $1 \leq s \leq \infty$ and $1 \leq s' \leq \infty$ be
    such that $1/s + 1/s' = 1$.
    Let $u \in L^s(\partial \Omega)$ and $v \in L^{s'}(\partial \Omega)$.
    Then $uv \in L^1(\partial \Omega)$ and
    \begin{equation*}
        \norm{uv}_{L^1(\partial \Omega)} \leq \norm{u}_{L^s(\partial \Omega)}
            \norm{v}_{\vphantom{L^s(\partial \Omega)} \smash{L^{s'}(\partial \Omega)}}.
    \end{equation*}
\end{theorem}
\begin{proof}
    Assume first that $1 < s < \infty$ and $1 < s' < \infty$.
    Since $\eta_j \in C_0^{\infty}(U_j)$, we may extend it by zero to the whole
    $\mathbb{R}^n$. Then 
    \begin{align*}
        \int_{\partial \Omega} \abs{uv} \diff S
        &= \sum_{j=1}^{m} \int_{\partial \Omega \cap U_j} \eta_j \abs{uv} \diff S \\
        &= \sum_{j=1}^{m} \int_{Q_{d_j}} \eta_j \abs{uv}
            \sqrt{1 + \abs{\nabla g_j}^2} \diff \tilde{y}' \\
        &= \int_{\mathbb{R}^{n-1}} \sum_{j=1}^{n} \eta_j \abs{uv}
            \sqrt{1 + \abs{\nabla g_j}^2} \diff \tilde{y}' \\
        &= \int_{\mathbb{R}^{n-1}} \sum_{j=1}^{n}
            \left( \eta_j^{\frac{1}{s}} \abs{u}
                \left( 1 + \abs{\nabla g_j}^2 \right)^{\frac{1}{2s}} \right)
            \left( \eta_j^{\frac{1}{s'}} \abs{v}
                \left( 1 + \abs{\nabla g_j}^2 \right)^{\frac{1}{2s'}} \right)
            \diff \tilde{y}' \\
        &\leq \int_{\mathbb{R}^{n-1}}
            \left( \sum_{j=1}^{n} \eta_j \abs{u}^s \sqrt{1 + \abs{\nabla g_j}^2} 
                \right)^{\frac{1}{s}}
            \left( \sum_{j=1}^{n} \eta_j \abs{v}^{s'} \sqrt{1 + \abs{\nabla g_j}^2}
                \right)^{\frac{1}{s'}}
            \diff \tilde{y}' \\
        &\leq \left( \int_{\mathbb{R}^{n-1}} \sum_{j=1}^{n} \eta_j \abs{u}^s
                \sqrt{1 + \abs{\nabla g_j}^2}
                    \diff \tilde{y}' \right)^{\frac{1}{s}}
            \left( \int_{\mathbb{R}^{n-1}} \sum_{j=1}^{n} \eta_j \abs{v}^{s'}
                \sqrt{1 + \abs{\nabla g_j}^2}
                    \diff \tilde{y}' \right)^{\frac{1}{s'}} \\
        &= \left( \int_{\partial \Omega} \abs{u}^s \diff S
                \right)^{\frac{1}{s}}
           \left( \int_{\partial \Omega} \abs{v}^{s'} \diff S
                \right)^{\frac{1}{s'}} \\
        &= \norm{u}_{L^s(\partial \Omega)}
            \norm{v}_{\vphantom{L^s(\partial \Omega)} \smash{L^{s'}(\partial \Omega)}}.
    \end{align*}
    Both inequalities above follow from Hölder's inequality.
    For the second application of Hölder's inequality, we used the assumptions
    $u \in L^s(\partial \Omega)$ and $v \in L^{s'}(\partial \Omega)$
    and the definition of a boundary integral as a regular Lebesgue integral
    over $\mathbb{R}^{n-1}$.

    Let us then consider the case $s=\infty$ or $s'=\infty$.
    Without loss of generality, let $s=\infty$. Then clearly
    \begin{equation*}
        \int_{\partial \Omega} \abs{uv} \diff S
        \leq \int_{\partial \Omega} \norm{u}_{L^{\infty}(\partial \Omega)}
            \abs{v} \diff S
        = \norm{u}_{L^{\infty}(\partial \Omega)} \norm{v}_{L^1(\partial \Omega)}.
    \end{equation*}
\end{proof}
Since $\Omega$ is assumed to be bounded,
$\partial \Omega$ has finite $(n-1)$-dimensional measure.
Thus, with a proof more or less identical to the proof of 
Theorem~\ref{thm:lpimbedding}, the imbedding
$L^r(\partial \Omega) \subset L^s(\partial \Omega)$
holds when $1 \leq s \leq r \leq \infty$.

\subsection{Sobolev Spaces}
\label{subsec:sobolevspaces}

A Sobolev space is a Banach space consisting of functions where the functions
along with their weak partial derivatives up to a given order belong to a given Lebesgue space.
The Sobolev spaces are the vector spaces where we shall search for the solutions
of the boundary value problems. The following review of the Sobolev spaces is based on
\cite{necas2011} and \cite{adams2003}.

\subsubsection{Weak Derivatives}
\label{subsubsec:weakderivatives}

Let us first clarify the notation used for the spaces of classically
differentiable functions on an open set $\Omega \subset \mathbb{R}^n$.
The space of $k$ times continuously differentiable functions is denoted by
$C^k(\Omega)$, and $C(\Omega) = C^0(\Omega)$ is the space of continuous functions.
The intersection of $C^k(\Omega)$ for all $k \in \mathbb{N}_0$
is the space of infinitely differentiable functions $C^{\infty}(\Omega)$,
and $C_0^{\infty}(\Omega)$ denotes the space of infinitely differentiable
functions with compact support.

Let us then motivate weak differentiability with a classically
differentiable function. Let $u \in C^1(\Omega)$. For all
$\phi \in C_0^{\infty}(\Omega)$, integration by parts gives
\begin{equation}
    \label{eq:weakderivativemotivation1}
    \int_{\Omega} u \frac{\partial \phi}{\partial x_j} \diff x
    = - \int_{\Omega} \frac{\partial u}{\partial x_j} \phi \diff x
\end{equation}
for all $1 \leq j \leq n$. There is no boundary term because
$\phi$ vanishes near the boundary. The identity \eqref{eq:weakderivativemotivation1}
essentially defines the meaning of a weak partial derivative:
a function $v_j$ is a weak partial derivative of $u$ with respect to the variable
$x_j$ if it satisfies
\begin{equation}
    \label{eq:weakderivativemotivation2}
    \int_{\Omega} u \frac{\partial \phi}{\partial x_j} \diff x
    = - \int_{\Omega} v_j \phi \diff x
\end{equation}
for all $\phi \in C_0^{\infty}(\Omega)$.

Clearly, a classical derivative is also a weak derivative, but the reverse need not be true.
The integrals in the identity \eqref{eq:weakderivativemotivation2} exist
with the modest assumption that $u \in L_{\text{loc}}^1(\Omega)$
and $v_j \in L_{\text{loc}}^1(\Omega)$, where $L_{\text{loc}}^1(\Omega)$
is the space of functions that are integrable over all compact subsets of $\Omega$.
Indeed, weak derivatives exist
for a larger class of functions than just classically differentiable functions.
If a weak partial derivative exists, it is unique up to sets of measure zero \cite{adams2003}.

Extending the above to higher orders
of weak differentiability is straightforward.
For this, we use the standard multi-index notation.
Let $\alpha = (\alpha_1,\dotsc,\alpha_n) \in \mathbb{N}_0^n$
be a multi-index, and define
$\abs{\alpha} = \alpha_1 + \dotsb + \alpha_n$. The elements of $\alpha$
correspond to the numbers of times a function is differentiated
with respect to each variable. Now for a $u \in C^k(\Omega)$, $k \geq 1$,
and for a multi-index $\alpha$ satisfying
$\abs{\alpha} \leq k$, we denote the $\alpha$th partial derivative of $u$ by
\begin{equation*}
    D^{\alpha} u
    = \frac{\partial^{\abs{\alpha}}}
        {\partial x_1^{\alpha_1} \dotsb \partial x_n^{\alpha_n}} u
    = \frac{\partial^{\alpha_1}}{\partial x_1^{\alpha_1}} \dotsb
        \frac{\partial^{\alpha_n}}{\partial x_n^{\alpha_n}} u.
\end{equation*}
Naturally, $D^{(0,\dotsc,0)}u=u$.
Successive application of integration by parts gives
\begin{equation*}
    \int_{\Omega} u D^{\alpha} \phi \diff x
    = (-1)^{\abs{\alpha}} \int_{\Omega} D^{\alpha} u \phi \diff x
\end{equation*}
for all $\phi \in C_0^{\infty}(\Omega)$, and $v_{\alpha} = D^{\alpha} u$
is also the $\alpha$th weak partial derivative of $u$.

Let us synthesize the above discussion into a self-contained definition
of a weak partial derivative of arbitrary order.
\begin{definition}[Weak partial derivative]
    \label{def:weakderivative}
    Let $\Omega \subset \mathbb{R}^n$ be open,
    $u \in L_{\text{loc}}^1(\Omega)$, and let 
    $\alpha \in \mathbb{N}_0^n$ be a multi-index. If there exists
    a $v_{\alpha} \in L_{\text{loc}}^1(\Omega)$ such that
    \begin{equation*}
        \int_{\Omega} u D^{\alpha} \phi \diff x
        = (-1)^{\abs{\alpha}} \int_{\Omega} v_{\alpha} \phi \diff x
    \end{equation*}
    for all $\phi \in C_0^{\infty}(\Omega)$, then
    $v_{\alpha}$ is said to be the $\alpha$th weak partial derivative of $u$,
    and it is denoted by $D^{\alpha} u$.
\end{definition}
Weak derivatives behave much like classical derivatives.
For example, weak differentiation is commutative and linear:
\begin{equation*}
    D^{\alpha}(D^{\beta} u) = D^{\beta}(D^{\alpha} u)
    \quad \text{and} \quad
    D^{\alpha}(a u + b v) = a D^{\alpha}u + b D^{\alpha}v,
    \quad a,b \in \mathbb{R}.
\end{equation*}
We tend to use the classical notation $\partial u / \partial x_j$
or $D_j u$ for weak partial derivatives as well,
and this is extended in an obvious manner to other
differential operators, such as the gradient $\nabla u$ and the Laplacian $\Delta u$.

We are now ready to define the Sobolev spaces.
\begin{definition}[Sobolev spaces]
    \label{def:sobolevspaces}
    Let $\Omega \subset \mathbb{R}^n$ be open.
    Let $k$ be a positive integer and $1 \leq s \leq \infty$.
    The Sobolev space $W^{k,s}(\Omega)$ is the normed space of functions
    $u \in L^s(\Omega)$ that have all weak partial derivatives of orders
    $0 \leq \abs{\alpha} \leq k$, and the weak partial derivatives satisfy
    $D^{\alpha} u \in L^s(\Omega)$ for all $0 \leq \abs{\alpha} \leq k$.
    The norm is given by
    \begin{alignat*}{2}
        \norm{u}_{W^{k,s}(\Omega)}
        &= \left( \sum_{0 \leq \abs{\alpha} \leq k}
                \norm{D^{\alpha} u}_{L^s(\Omega)}^s \right)^{\frac{1}{s}}
                    &&\quad \text{for } 1 \leq s < \infty, \\
        \norm{u}_{W^{k,\infty}(\Omega)}
        &= \max_{0 \leq \abs{\alpha} \leq k}
                \norm{D^{\alpha} u}_{L^{\infty}(\Omega)}
                    &&\quad \text{for } s = \infty.
    \end{alignat*}
    Moreover, the space $W^{k,2}(\Omega)$, commonly denoted by $H^k(\Omega)$,
    is an inner product space with the inner product
    \begin{equation*}
        \innerprod{u}{v}_{H^k(\Omega)}
        = \sum_{0 \leq \abs{\alpha} \leq k}
                \innerprod{D^{\alpha} u}{D^{\alpha} v}_{L^2(\Omega)}.
    \end{equation*}
\end{definition}
For $1 \leq s < \infty$, it is also common to consider the
$L^s$-norms of the weak derivatives of some fixed order $\abs{\alpha} = k$:
\begin{equation*}
    \abs{u}_{W^{k,s}(\Omega)}
    = \left( \sum_{\abs{\alpha} = k}
                \norm{D^{\alpha} u}_{L^s(\Omega)}^s \right)^{\frac{1}{s}}.
\end{equation*}
The above defines a seminorm in $W^{k,s}(\Omega)$.

The Sobolev spaces inherit many of their properties from the Lebesgue spaces.
Once again, two functions in a Sobolev space are identified
if they are equal almost everywhere in their domain.
The spaces $W^{k,s}(\Omega)$ are Banach spaces for all $k$ and $s$,
and $H^k(\Omega)$ is a Hilbert space for all $k$.
When $k$ and $m$ are positive integers such that $k \leq m$,
it is obvious that the imbedding
$W^{m,s}(\Omega) \subset W^{k,s}(\Omega)$ holds for all $1 \leq s \leq \infty$.
When $\Omega$ is bounded, Theorem~\ref{thm:lpimbedding}
can also be extended to the Sobolev spaces, which we prove next.
\begin{theorem}
    \label{thm:sobolevlpimbedding}
    Assume that $\Omega$ is bounded. Let $k$ be a positive integer, and let
    $1 \leq s \leq r \leq \infty$. Then there exists a constant $C > 0$
    such that
    \begin{equation*}
        \norm{u}_{W^{k,s}(\Omega)} \leq C \norm{u}_{W^{k,r}(\Omega)}
    \end{equation*}
    for all $u \in W^{k,r}(\Omega)$. In other words,
    $W^{k,r}(\Omega) \subset W^{k,s}(\Omega)$.
\end{theorem}
\begin{proof}
    The case $s=r$ is trivial so assume that $s < r$.
    Let $u \in W^{k,r}(\Omega)$, and assume first that $r < \infty$.
    Then by Theorem~\ref{thm:lpimbedding}, we get
    \begin{align}
        \norm{u}_{W^{k,s}(\Omega)}^s
        &= \sum_{0 \leq \abs{\alpha} \leq k} \norm{D^{\alpha} u}_{L^s(\Omega)}^s
            \nonumber \\
        &\leq C_1 \sum_{0 \leq \abs{\alpha} \leq k}
            \norm{D^{\alpha} u}_{L^r(\Omega)}^s \nonumber \\
        \label{eq:thm2.8intermediate}
        &= C_1 \sum_{0 \leq \abs{\alpha} \leq k}
            \norm{D^{\alpha} u}_{L^r(\Omega)}^{r \frac{s}{r}}
    \end{align}
    for some constant $C_1 > 0$ independent of $u$.
    The function $t \mapsto t^{s/r}$ is increasing on $[0, \infty)$,
    which implies that
    \begin{equation*}
        \norm{D^{\alpha} u}_{L^r(\Omega)}^{r \frac{s}{r}}
        \leq \left( \sum_{0 \leq \abs{\alpha} \leq k}
            \norm{D^{\alpha} u}_{L^r(\Omega)}^{r}
                \right)^{\frac{s}{r}}
        = \norm{u}_{W^{k,r}(\Omega)}^s.
    \end{equation*}
    Inserting this into \eqref{eq:thm2.8intermediate} gives
    \begin{equation*}
        \norm{u}_{W^{k,s}(\Omega)}^s
        \leq C_1 \sum_{0 \leq \abs{\alpha} \leq k} \norm{u}_{W^{k,r}(\Omega)}^s
        = C_1 C_2 \norm{u}_{W^{k,r}(\Omega)}^s,
    \end{equation*}
    where
    $C_2 = \abs{\{ \alpha \in \mathbb{N}_0^n : 0 \leq \abs{\alpha} \leq k \}}$.
    Taking the $s$th root from both sides proves the claim for $r < \infty$.

    Assume then that $r = \infty$.
    By Theorem~\ref{thm:lpimbedding} and the definition of
    $\norm{u}_{W^{k,\infty}(\Omega)}$, we get
    \begin{align*}
        \norm{u}_{W^{k,s}(\Omega)}^s
        &= \sum_{0 \leq \abs{\alpha} \leq k} \norm{D^{\alpha} u}_{L^s(\Omega)}^s \\
        &\leq C_1 \sum_{0 \leq \abs{\alpha} \leq k}
            \norm{D^{\alpha} u}_{L^{\infty}(\Omega)}^s \\
        &\leq C_1 \sum_{0 \leq \abs{\alpha} \leq k}
            \norm{u}_{W^{k,\infty}(\Omega)}^s \\
        &= C_1 C_2 \norm{u}_{W^{k,\infty}(\Omega)}^s.
    \end{align*}
    Taking the $s$th root from both sides proves the claim for $r=\infty$.
\end{proof}

Functions in Sobolev spaces resemble classically differentiable functions,
although they need not be continuous or bounded in the interior of their domain.
This resemblance is reflected by the properties of
weak derivatives but also by the fact that the space
$C^{\infty}(\Omega) \cap W^{k,s}(\Omega)$ is dense in $W^{k,s}(\Omega)$
for all positive integers $k$ and real numbers $1 \leq s < \infty$.
In other words, for every $u \in W^{k,s}(\Omega)$, there exists a sequence
of infinitely differentiable functions which converges to $u$ with respect
to the norm $\norm{\cdot}_{W^{k,s}(\Omega)}$. For a proof, see
\cite[Theorem~3.17 on p.~67]{adams2003}. This is an immensely useful result that
enables one to extend many existing results for smooth functions to the
Sobolev spaces by passing to a limit. Note, however, that the density result
does not hold when $s=\infty$.

It is also common to consider the closure of the space $C_0^{\infty}(\Omega)$
with respect to the norm $\norm{\cdot}_{W^{k,s}(\Omega)}$. The resulting
subspace of $W^{k,s}(\Omega)$ is denoted by $W_{\smash{0}}^{k,s}(\Omega)$ for a general $s$ and
by $H_{\smash{0}}^k(\Omega)$ for $s=2$.
A function in this space vanishes on the boundary $\partial \Omega$
in the trace sense. Boundary traces will be discussed soon.
These types of Sobolev spaces are important in the study of partial differential equations
with Dirichlet boundary conditions.

For a function in $W_{\smash{0}}^{1,p}(\Omega)$, there exists a useful inequality
between the $L^s$-norms of the function and its gradient. This is commonly
known as Poincaré's inequality.
\begin{theorem}[Poincaré's inequality]
    \label{thm:poincare_inequality}
    Let $\Omega \subset \mathbb{R}^n$ be a bounded domain,
    and let $1 \leq s < \infty$.
    Then there exists a constant $C > 0$ such that
    \begin{equation*}
        \norm{u}_{L^s(\Omega)} \leq C \abs{u}_{W^{1,s}(\Omega)}
    \end{equation*}
    for all $u \in W_{\smash{0}}^{1,s}(\Omega)$.
\end{theorem}
For a proof, see \cite[Theorem~6.30 on p.~183]{adams2003}.
Theorem~\ref{thm:poincare_inequality} is sometimes also called
Friedrichs' inequality or Poincaré-Friedrichs inequality.
We will later prove a variant of Poincaré's inequality
for which $u$ does not necessarily belong to the space $W_{\smash{0}}^{1,p}(\Omega)$.

\subsubsection{Sobolev Imbeddings}
\label{subsubsec:sobolevimbeddingtheorem}

We previously saw two types of imbeddings between Sobolev spaces $W^{k,s}(\Omega)$
where either $k$ or $s$ was fixed:
when $k$ and $m$ are such that $k \leq m$, then
$W^{m,s}(\Omega) \subset W^{k,s}(\Omega)$ for all $1 \leq s \leq \infty$,
and when $\Omega$ is bounded, then $W^{k,r}(\Omega) \subset W^{k,s}(\Omega)$
whenever $1 \leq s \leq r \leq \infty$.
When the boundary of $\Omega$ satisfies some additional regularity
assumptions, further imbeddings exist for which both $k$ and $s$ can vary.
Moreover, in some cases for some positive integer $j$,
the space $W^{k,s}(\Omega)$ can be considered as a subset of
$j$ times continuously differentiable functions
in the sense that each 
equivalence class $u \in W^{k,s}(\Omega)$ contains a function
that is $j$ times continuously differentiable.
Such an imbedding is denoted like the other imbeddings, i.e.\
$W^{k,s}(\Omega) \subset C^j(\Omega)$. These additional imbeddings
are given by the Sobolev imbedding theorem \cite{adams2003}.

The Sobolev imbedding theorem also states that the imbeddings are continuous.
This means that when we consider an imbedding of the form $X \subset Y$
between two normed spaces $X$ and $Y$ via the identity mapping
$I: X \to Y$, $Ix = x$, then the operator $I$ is continuous.
Since the operator $I$ is linear, this is equivalent to $I$ being bounded
by Theorem~\ref{thm:boundedcontinuous}. For example, the imbedding
$W^{m,s}(\Omega) \subset W^{k,s}(\Omega)$ for $k \leq m$ is obviously
continuous since $\norm{u}_{W^{k,s}(\Omega)} \leq \norm{u}_{W^{m,s}(\Omega)}$
for all $u \in W^{m,s}(\Omega)$. By Theorem~\ref{thm:sobolevlpimbedding},
the imbedding $W^{k,r}(\Omega) \subset W^{k,s}(\Omega)$ for
$1 \leq s \leq r \leq \infty$ is also continuous.
Similarly, an imbedding is said to be compact if the operator $I$ is compact,
see Definition~\ref{def:compactoperator}. Most Sobolev imbeddings are
compact, and this result is known as the Rellich-Kondrachov theorem 
\cite{adams2003}.

We now present a simplified combined version of the Sobolev imbedding
theorem and the Rellich-Kondrachov theorem which contains the relevant
imbeddings for our purposes. In their most general forms,
both theorems consider both bounded and unbounded domains,
different types of boundary regularity, and the results
depend on the dimension $n$. We are only interested in the
case $n=2$ and bounded domains with Lipschitz boundaries.
Note that below $C^k(\overline{\Omega})$ denotes the normed space of functions
$u \in C^k(\Omega)$ for which $D^{\alpha} u$ is bounded and uniformly
continuous on $\Omega$ for all $0 \leq \abs{\alpha} \leq k$,
and its norm is given by
\begin{equation*}
    \norm{u}_{C^k(\overline{\Omega})}
    = \max_{0 \leq \abs{\alpha} \leq k}
        \sup_{x \in \Omega} \abs{D^{\alpha} u(x)}.
\end{equation*}
As usual, we use the alias $C(\overline{\Omega}) = C^0(\overline{\Omega})$.
The notation $C^k(\overline{\Omega})$ signifies
that the function $u$ along with its partial derivatives of order
$\abs{\alpha} \leq k$ can be uniquely continuously extended to the boundary $\partial \Omega$.
\begin{theorem}
    \label{thm:sobolevimbedding}
    Let $\Omega \subset \mathbb{R}^2$ be a bounded domain with Lipschitz boundary.
    Let $k \geq 1$ and $j \geq 0$ be integers, and let $1 \leq s < \infty$ be a
    real number. Then the following imbeddings are continuous and compact.
    \begin{enumerate}[(i)]
        \item If $ks < 2$, then
        \begin{equation*}
            W^{j+k,s}(\Omega) \subset W^{j,r}(\Omega)
            \quad \text{if } 1 \leq r < 2s/(2-ks) .
        \end{equation*}
        \item If $ks = 2$, then
        \begin{equation*}
            W^{j+k,s}(\Omega) \subset W^{j,r}(\Omega)
            \quad \text{if } 1 \leq r < \infty.
        \end{equation*}
        \item If $ks > 2$, then
        \begin{align*}
            W^{j+k,s}(\Omega) &\subset W^{j,r}(\Omega)
            \quad \text{if } 1 \leq r < \infty, \\
            W^{j+k,s}(\Omega) &\subset C^j(\overline{\Omega}).
        \end{align*}
    \end{enumerate}
    Note that $W^{0,s}(\Omega) = L^s(\Omega)$.
\end{theorem}
For the proofs and complete versions of the above imbeddings, see
\cite[Theorem~4.12 on p.~85]{adams2003} for the Sobolev imbedding theorem
and \cite[Theorem~6.3 on p.~168]{adams2003} for the Rellich-Kondrachov theorem.

Let us immediately note one important consequence of 
Theorem~\ref{thm:sobolevimbedding} regarding the Dirac delta functional
$\delta_{x_0}$. In Section~\ref{subsubsec:dualspaces}, we defined $\delta_{x_0}$
as an element of the dual space $C(\overline{\Omega})'$ such that
for a given $x_0 \in \Omega$ we have $\delta_{x_0}(u) = u(x_0)$
for all $u \in C(\overline{\Omega})$.
If we now try to define $\delta_{x_0}$ similarly as an element of the dual space
of an arbitrary Sobolev space $W^{k,s}(\Omega)$, we run into the subtlety
that $W^{k,s}(\Omega)$ does not really consist of functions but equivalence
classes of functions. For every possible value $c \in \mathbb{R}$,
each such equivalence class contains a function $u$ such that $u(x_0) = c$.
The problem then becomes to choose one of these functions to evaluate
$\delta_{x_0}(u)$. When $W^{k,s}(\Omega) \subset C(\overline{\Omega})$,
we can simply choose the continuous function and $\delta_{x_0}(u)$
becomes well-defined. Theorem~\ref{thm:sobolevimbedding} states when this is possible.
For example, the imbedding $W^{1,s}(\Omega) \subset C(\overline{\Omega})$ is true when $s > 2$
and, thus, $\delta_{x_0} \in W^{1,s}(\Omega)'$. Note that the continuity
of $\delta_{x_0}$ follows from the continuity of the imbedding:
\begin{equation*}
    \abs{\delta_{x_0}(u)}
    = \abs{u(x_0)}
    \leq \norm{u}_{C(\overline{\Omega})}
    \leq C \norm{u}_{W^{1,s}(\Omega)}
\end{equation*}
for all $u \in W^{1,s}(\Omega)$.

\subsubsection{Boundary Traces}
\label{subsubsec:boundarytraces}

By definition, a boundary value problem requires the ability to consider, in some sense,
the restriction of a function on the boundary of its domain.
This is obviously possible in the usual pointwise sense
for functions in $C(\overline{\Omega})$
and, thus by Theorem~\ref{thm:sobolevimbedding}, for functions
in e.g.\ the Sobolev space $W^{1,s}(\Omega)$
when $\Omega$ is a two-dimensional bounded domain with Lipschitz boundary
and $s > 2$. But what about the case $s \leq 2$ for which pointwise
evaluation does not necessarily make sense?
Fortunately, for all $1 \leq s < \infty$, the restriction of a function $u \in W^{1,s}(\Omega)$
on the boundary $\partial \Omega$ can always be considered as a function in $L^r(\partial \Omega)$
for some $r$ in a way that is consistent with the usual pointwise restriction
for functions in $C(\overline{\Omega})$.
This is the message of the following trace theorem.
\begin{theorem}[Trace theorem]
    \label{thm:tracetheorem}
    Let $\Omega \subset \mathbb{R}^2$ be a bounded domain with Lipschitz boundary,
    and let $1 \leq s < \infty$.
    Then there exists a unique bounded linear operator
    \begin{equation*}
        T: W^{1,s}(\Omega) \to L^s(\partial \Omega)   
    \end{equation*}
    such that $Tu = u|_{\partial \Omega}$ for all
    $u \in C(\overline{\Omega}) \cap W^{1,s}(\Omega)$.
\end{theorem}
For a proof,
see \cite[Theorem~4.2 on p.~79 and Theorem~4.6 on p.~81]{necas2011}.
The operator $T$ is called the trace operator.
The notation $u|_{\partial \Omega}$ naturally extends to traces as well.
Similarly, $\norm{u}_{L^s(\partial \Omega)}$ is shorthand for
$\norm{Tu}_{L^s(\partial \Omega)}$.

The trace operator is not surjective.
Thus, if we wish to pick a function $g \in L^s(\partial \Omega)$
such that there exists a $u \in W^{1,s}(\Omega)$ for which
$Tu = g$, then we need to require that $g$ belongs to the range of $T$, i.e.\
$g \in T(W^{1,s}(\Omega))$.
The range of the trace operator can be characterized as a fractional-order
Sobolev space on $\partial \Omega$, which we shall not consider
any further. For more information, see \cite{adams2003} and \cite{lions1972}.

With the trace concept, integration by parts can be extended to the Sobolev spaces.
\begin{theorem}[Integration by parts]
    \label{thm:integrationbyparts}
    Let $\Omega \subset \mathbb{R}^2$ be a bounded domain with Lipschitz
    boundary. Let $1 < s < \infty$ and $1 < s' < \infty$ be such that
    $1/s + 1/s' \leq 3/2$, and let $u \in W^{1,s}(\Omega)$
    and $v \in W^{1,s'}(\Omega)$. Then for $i=1$ and $i=2$, it holds that
    \begin{equation*}
        \int_{\Omega} u \frac{\partial v}{\partial x_i} \diff x
        = - \int_{\Omega} \frac{\partial u}{\partial x_i} v \diff x
            + \int_{\partial \Omega} uv n_i \diff S,
    \end{equation*}
    where $n = (n_1, n_2)$ is the exterior unit normal
    to the boundary $\partial \Omega$.
\end{theorem}
For a proof, see \cite[Theorem~1.1 on p.~117]{necas2011}.
Note that the exterior unit normal exists almost everywhere on $\partial \Omega$
\cite[Lemma~4.2 on p.~83]{necas2011}.
As a corollary to Theorem~\ref{thm:integrationbyparts},
let us prove the following Green's formula that will be useful later.
\begin{theorem}[Green's formula]
    \label{thm:greensformula}
    Let $\Omega \subset \mathbb{R}^2$ be a bounded domain with Lipschitz
    boundary. Let $1 < s < \infty$ and $1 < s' < \infty$ be such that
    $1/s + 1/s' \leq 3/2$, and let $u \in W^{2,s}(\Omega)$
    and $v \in W^{1,s'}(\Omega)$. Then
    \begin{equation*}
        \int_{\Omega} \nabla u \cdot \nabla v \diff x
        = -\int_{\Omega} \Delta u v \diff x
            + \int_{\partial \Omega} \frac{\partial u}{\partial n} v \diff S,
    \end{equation*}
    where $\partial u / \partial n = \nabla u \cdot n$ is the directional
    derivative of $u$ in the direction of the exterior
    unit normal on $\partial \Omega$.
\end{theorem}
\begin{proof}
    Since $u \in W^{2,s}(\Omega)$, it holds that
    $\nabla u \in W^{1,s}(\Omega) \times W^{1,s}(\Omega)$.
    Now by the integration by parts formula in
    Theorem~\ref{thm:integrationbyparts}, we get
    \begin{align*}
        \int_{\Omega} \nabla u \cdot \nabla v \diff x
        &= \int_{\Omega} \frac{\partial u}{\partial x_1}
            \frac{\partial v}{\partial x_1} \diff x
            + \int_{\Omega} \frac{\partial u}{\partial x_2}
            \frac{\partial v}{\partial x_2} \diff x \\
        &= -\int_{\Omega} \left( 
            \frac{\partial^2 u}{\partial x_1^2}
                + \frac{\partial^2 u}{\partial x_2^2} \right) v \diff x
            + \int_{\partial \Omega} \left(
                \frac{\partial u}{\partial x_1} n_1
                    + \frac{\partial u}{\partial x_2} n_2 \right) v \diff S \\
        &= -\int_{\Omega} \Delta u v \diff x
            + \int_{\partial \Omega} \frac{\partial u}{\partial n} v \diff S.
    \end{align*}
\end{proof}

\clearpage

\section{Poisson's Equation in a Polygon}
\label{sec:poissons_equation_in_a_polygon}

Assume that $\Omega \subset \mathbb{R}^2$ is a bounded polygonal domain.
Let $\Gamma_j$ denote the $j$th linear boundary segment on $\partial \Omega$
for $j = 1,2,\dotsc,J$, where $J$ is the total number of boundary segments.
That is,
\begin{equation*}
    \partial \Omega = \bigcup_{1 \leq j \leq J} \overline{\Gamma_j}.
\end{equation*}
The linear boundary segments are assumed to be analogous to one-dimensional
open intervals, which is why the union is taken over the closures of the
boundary segments.
It is also assumed that $\Omega$ does not contain any slits.
In other words, the angle between two boundary segments with a common vertex
is never $0$ nor $2\pi$.
This implies that $\partial \Omega$ is Lipschitz according to
Definition~\ref{def:lipschitzboundary} \cite{grisvard2011}.

Poisson's equation in $\Omega$ with prescribed boundary conditions
is classically formulated as finding a function
$u \in C^2(\Omega) \cap C^1(\overline{\Omega})$ such that
\begin{equation}
    \label{eq:classical_poissons_equation}
    \left\{
        \begin{aligned}
            -\Delta u &= f && \text{in } \Omega \\
            u &= g_j && \text{on } \Gamma_j, \quad j \in D \\
            \frac{\partial u}{\partial n} &= g_j && \text{on } \Gamma_j,
            \quad j \in N,
        \end{aligned}
    \right.
\end{equation}
where the functions $f$ and $g_j$ for $j=1,2,\dotsc,J$ constitute the data
of the problem. On each linear boundary segment, either a Dirichlet or a Neumann
boundary condition is imposed according to the index sets $D$ and $N$, respectively,
that are disjoint subsets of the full index set
$\{ j \in \mathbb{N} : 1 \leq j \leq J \}$.

Of course, $\Omega$ need not be polygonal for the boundary value problem
\eqref{eq:classical_poissons_equation} to make sense. One could consider
a general open set $\Omega \subset \mathbb{R}^n$, but, for the purposes of
this thesis, the assumption that $\Omega$ is a polygon has important theoretical
and practical implications. This is especially true in the light of
the finite element method.

The unknown function $u$ in the problem \eqref{eq:classical_poissons_equation}
can be used to model quantities, such as chemical concentration, temperature or
electric potential, over a physical domain $\Omega$ when the quantity is
in equilibrium, that is, the quantity does not change over time \cite{evans2010}. 
The function $f$ is typically called the load term or the source term. For example,
in the context of heat diffusion, it can correspond to a source of heat,
and its unit is energy per unit volume and time. In the context of electrostatics,
the function $f$ can correspond to the charge density over $\Omega$.
A Dirichlet boundary condition means that the quantity is held fixed
on that boundary segment,
and a Neumann boundary condition corresponds to the flux
on that boundary segment. For example, in the context of heat diffusion,
setting $g_j = 0$ for some $j \in N$ means that the boundary segment $\Gamma_j$ is perfectly 
insulated.

A solution to the classically formulated problem 
\eqref{eq:classical_poissons_equation} is accordingly said to be classical.
However, proving the existence of classical solutions is typically difficult
and may turn out to be impossible if the data are not at least continuous
and the boundary $\partial \Omega$ smooth enough.
Even when a classical solution does exist, approximating it via
e.g.\ finite differences can become unwieldy in complex domains.
This makes the classical problem unfit for many practical applications.
Instead, the problem is typically formulated in a weak form, for which
the existence and uniqueness of a solution, even for irregular data,
follows rather easily based on abstract results from functional analysis.
Then the weak solution can be approximated
with the finite element method over virtually any polygonal domain $\Omega$.

Let us formulate the boundary value problem \eqref{eq:classical_poissons_equation}
in weak form.
Let $f \in L^2(\Omega)$, $g_j \in T(H^2(\Omega))$ for $j \in D$ and
$g_j \in T(H^1(\Omega))$ for $j \in N$, where $T$ is the trace operator in
Theorem~\ref{thm:tracetheorem}. Assume that $u \in H^2(\Omega)$
is a classical solution to the problem \eqref{eq:classical_poissons_equation}.
Let $v \in H^1(\Omega)$ be such that $v|_{\Gamma_j} = 0$ for all $j \in D$.
Multiplying both sides of Poisson's equation by $v$,
integrating both sides over $\Omega$ and applying Green's formula
from Theorem~\ref{thm:greensformula} gives
\begin{equation}
    \label{eq:weak_formulation_step1}
    \int_{\Omega} \nabla u \cdot \nabla v \diff x
        - \int_{\partial \Omega} \frac{\partial u}{\partial n} v \diff S
    = \int_{\Omega} f v \diff x.
\end{equation}
Using the fact that $v|_{\Gamma_j} = 0$ for all $j \in D$
and substituting in the Neumann boundary conditions,
the boundary integral in \eqref{eq:weak_formulation_step1} becomes
\begin{align}
    \int_{\partial \Omega} \frac{\partial u}{\partial n} v \diff S
    &= \sum_{j \in D} \int_{\Gamma_j} \frac{\partial u}{\partial n} v \diff S
        + \sum_{j \in N} \int_{\Gamma_j} \frac{\partial u}{\partial n} v \diff S
            \nonumber \\
    \label{eq:weak_formulation_step2}
    &= \sum_{j \in N} \int_{\Gamma_j} g_j v \diff S.
\end{align}
Combining \eqref{eq:weak_formulation_step1} and \eqref{eq:weak_formulation_step2}
gives
\begin{equation}
    \label{eq:weak_formulation_variational}
    \int_{\Omega} \nabla u \cdot \nabla v \diff x
    = \int_{\Omega} f v \diff x + \sum_{j \in N} \int_{\Gamma_j} g_j v \diff S.
\end{equation}
Note that the expression \eqref{eq:weak_formulation_variational}
also makes sense when only
$u \in H^1(\Omega)$. This motivates the definition of a weak solution
to the problem \eqref{eq:classical_poissons_equation}.
\begin{definition}
    \label{def:weak_solution}
    A function $u \in H^1(\Omega)$ is said to be a weak solution to the
    boundary value problem \eqref{eq:classical_poissons_equation} if
    \begin{enumerate}[(i)]
        \item it satisfies the identity \eqref{eq:weak_formulation_variational}
        for all $v \in H^1(\Omega)$ with $v|_{\Gamma_j} = 0$ for all $j \in D$,
        \item $u|_{\Gamma_j} = g_j$ for all $j \in D$.
    \end{enumerate}
\end{definition}

It is difficult to deduce the existence of a weak solution to the problem
\eqref{eq:classical_poissons_equation} when it is formulated in its most
general form. Instead, it is more convenient
to separately consider the cases $D \neq \varnothing$,
i.e.\ the pure Dirichlet and mixed Dirichlet-Neumann problems, and
$D = \varnothing$, i.e.\ the pure Neumann problem.
For both cases, we consider a special instance of the problem whose
solution will enable us to deduce the existence of a weak solution
to the general problem as per Definition~\ref{def:weak_solution}.
Let us begin with the case $D \neq \varnothing$.

It will be easier to work with
a problem that has homogeneous Dirichlet boundary conditions
$g_j = 0$ for all $j \in D$.
Thus, we would like to transform the problem \eqref{eq:classical_poissons_equation}
to an equivalent problem with such boundary conditions.
To make this simple, we always assume that there exists a function
$u_D \in H^2(\Omega)$ such that $u_D|_{\Gamma_j} = g_j$ for all $j \in D$.
This implies that when $g_i$ and $g_j$ approach a common vertex,
they approach the same value because $H^2(\Omega) \subset C(\overline{\Omega})$
by the Sobolev imbedding theorem (Theorem~\ref{thm:sobolevimbedding}).

Let us then consider finding a weak solution $w \in H^1(\Omega)$ to the modified problem
\begin{equation}
    \label{eq:poissons_equation_modified}
    \left\{
        \begin{aligned}
            -\Delta w &= f + \Delta u_D && \text{in } \Omega \\
            w &= 0 && \text{on } \Gamma_j, \quad j \in D \\
            \frac{\partial w}{\partial n} &=
                g_j - \frac{\partial u_D}{\partial n} && \text{on } \Gamma_j,
                \quad j \in N.
        \end{aligned}
    \right.
\end{equation}
Note that since $u_D \in H^2(\Omega)$, it holds that
$f + \Delta u_D \in L^2(\Omega)$, and since the normal vector $n$
is constant on each linear boundary segment, it clearly holds that
$g_j - \partial u_D / \partial n \in T(H^1(\Omega))$ for all $j \in N$.

If $w \in H^1(\Omega)$ is a weak solution to the problem
\eqref{eq:poissons_equation_modified}, then $u = w + u_D \in H^1(\Omega)$
is a weak solution to the original non-homogeneous problem
\eqref{eq:classical_poissons_equation}. Namely, we easily see that
$u|_{\Gamma_j} = g_j$ for all $j \in D$, and, by the definition of
a weak solution and by Green's formula, we have
\begin{align*}
    \int_{\Omega} \nabla u \cdot \nabla v \diff x
    &= \int_{\Omega} \nabla w \cdot \nabla v \diff x
        + \int_{\Omega} \nabla u_D \cdot \nabla v \diff x \\
    &= \int_{\Omega} (f + \Delta u_D) v \diff x
        + \sum_{j \in N} \int_{\Gamma_j}
            \left( g_j - \frac{\partial u_D}{\partial n} \right) v \diff S \\
    &\hspace{7mm} - \int_{\Omega} \Delta u_D v \diff x
                    + \sum_{j \in N} \int_{\Gamma_j}
                        \frac{\partial u_D}{\partial n} v \diff S \\
    &= \int_{\Omega} fv \diff x + \sum_{j \in N} \int_{\Gamma_j} g_j v \diff S
\end{align*}
for all $v \in H^1(\Omega)$ with $v|_{\Gamma_j} = 0$ for all $j \in D$.
This means that to solve the non-homogeneous problem 
\eqref{eq:classical_poissons_equation}, it suffices to only consider the solvability
of the homogeneous problem
\begin{equation}
    \label{eq:poissons_equation_homogeneous}
    \left\{
        \begin{aligned}
            -\Delta u &= f && \text{in } \Omega \\
            u &= 0 && \text{on } \Gamma_j, \quad j \in D \\
            \frac{\partial u}{\partial n} &= g_j && \text{on } \Gamma_j,
            \quad j \in N.
        \end{aligned}
    \right.
\end{equation}

Homogeneity brings symmetricity to the weak formulation.
Namely, define a subspace $V_D$ of $H^1(\Omega)$ as
\begin{equation*}
    V_D = \{ v \in H^1(\Omega) :
        v|_{\Gamma_j} = 0 \text{ for all } j \in D \}.
\end{equation*}
Then the weak formulation of the problem \eqref{eq:poissons_equation_homogeneous}
can be succinctly written as: Find a $u \in V_D$ such that it satisfies
\eqref{eq:weak_formulation_variational} for all $v \in V_D$.
We consider the solvability of this problem shortly.

Let us then consider how to apply a similar transformation to
the pure Neumann problem, i.e.\ $D = \varnothing$.
By Definition~\ref{def:weak_solution}, the weak formulation is already quite simple
and, most importantly, symmetric:
Find a $u \in H^1(\Omega)$ such that it satisfies 
\eqref{eq:weak_formulation_variational} for all $v \in H^1(\Omega)$.
However, there are a few subtleties.
First, the data $f$ and $g_j$ must satisfy the following compatibility condition.
Define $v \equiv 1$ in $\Omega$. Since $\Omega$ is bounded, clearly
$v \in H^1(\Omega)$. Substituting $v$ into \eqref{eq:weak_formulation_variational}
gives
\begin{equation}
    \label{eq:neumann_compatibility_condition}
    \int_{\Omega} f \diff x + \sum_{j \in N} \int_{\Gamma_j} g_j \diff S = 0.
\end{equation}
Thus, $f$ and $g_j$ for all $j \in N$
must satisfy \eqref{eq:neumann_compatibility_condition}.

Second, if $u \in H^1(\Omega)$ is a weak solution to the pure Neumann problem,
then $u + C \in H^1(\Omega)$ is also a weak solution for any constant
$C \in \mathbb{R}$.
In other words, a weak solution is never unique as per Definition
\ref{def:weak_solution}. To fix a solution,
it is typically searched from the subspace of $H^1(\Omega)$ whose elements have zero mean value
over $\Omega$:
\begin{equation*}
    V_N = \left\{ v \in H^1(\Omega) : \int_{\Omega} v \diff x = 0 \right\}.
\end{equation*}
The weak formulation then becomes: Find a $u \in V_N$ such that it satisfies
\eqref{eq:weak_formulation_variational} for all $v \in H^1(\Omega)$.
The symmetry is now lost, but, fortunately, if the compatibility condition 
\eqref{eq:neumann_compatibility_condition} is
satisfied and \eqref{eq:weak_formulation_variational} holds for all
$v \in V_N$, then \eqref{eq:weak_formulation_variational} also holds for all $v \in H^1(\Omega)$.
Let us prove this. Let $v \in H^1(\Omega)$. Then $v - \overline{v} \in V_N$, where
\begin{align*}
    \overline{v} = \frac{1}{\abs{\Omega}} \int_{\Omega} v \diff x.
\end{align*}
Now
\begin{align*}
    \int_{\Omega} \nabla u \cdot \nabla v \diff x
    &= \int_{\Omega} \nabla u \cdot \nabla (v - \overline{v}) \diff x \\
    &= \int_{\Omega} f (v - \overline{v}) \diff x
        + \sum_{j \in N} \int_{\Gamma_j} g_j (v - \overline{v}) \diff S \\
    &= \int_{\Omega} f v \diff x + \sum_{j \in N} \int_{\Gamma_j} g_j v \diff S
        - \overline{v} \left( \int_{\Omega} f \diff x
            + \sum_{j \in N} \int_{\Gamma_j} g_j \diff S \right) \\
    &= \int_{\Omega} f v \diff x + \sum_{j \in N} \int_{\Gamma_j} g_j v \diff S,
\end{align*}
where the last equality follows from the compatibility condition
\eqref{eq:neumann_compatibility_condition}.
Thus, in the case of a pure Neumann problem,
we rather consider the weak formulation:
Find a $u \in V_N$ such that it satisfies \eqref{eq:weak_formulation_variational}
for all $v \in V_N$.

The above weak forms of the boundary value problem
\eqref{eq:classical_poissons_equation} are the most commonly studied formulations,
see e.g.\ \cite{evans2010}, \cite{grisvard2011}, \cite{ciarlet2002} and
\cite{braess2007}. Due to the symmetricity and the fact
that $H^1(\Omega)$ is a Hilbert space, the existence and uniqueness of
solutions can be shown relatively easily, as we will see.
Moreover, in many cases the weak solution can be shown to belong to the space
$H^2(\Omega)$. We refer to the weak formulations presented above as the
classical weak formulations.

Let us now discuss how to formulate the problem 
\eqref{eq:classical_poissons_equation}
when the load is the Dirac delta, i.e.\ $f = \delta_{x_0}$ for some $x_0 \in \Omega$.
We use the definition of $\delta_{x_0}$ provided in Section~\ref{subsubsec:sobolevimbeddingtheorem},
that is, $\delta_{x_0}(v) = v(x_0)$ for all $v \in W^{1,s'}(\Omega)$ for any $s' > 2$.
By the Sobolev imbedding theorem, $W^{1,s'}(\Omega) \subset C(\overline{\Omega})$
so the pointwise evaluation makes sense.
The definition of $\delta_{x_0}$ already suggests that
the problem \eqref{eq:classical_poissons_equation} can be understood
in some weak sense only. We can characterize it as a limit of the classical
weak problems where $f \in L^2(\Omega)$.
Let us do this in the context of electrostatics.

Assume that we would like to model the electric potential in $\Omega$
caused by a single point charge located at the point $x_0 \in \Omega$.
The load $f$ corresponds to the charge density of the point charge
which is infinite at $x_0$ and zero everywhere else in $\Omega$.
Such a function is zero almost everywhere, which means that it would vanish
in \eqref{eq:weak_formulation_variational}. This would then incorrectly correspond
to the scenario where there are no charges in $\Omega$.
Let us instead consider the point charge as the limit of small charged disks.
Let $B(x_0,\varepsilon) \subset \Omega$ be a disk centered at the point $x_0$
with a small radius $\varepsilon > 0$. For simplicity, assume that the total
charge over the disk is one so that the charge density over $\Omega$ is
given by
\begin{equation*}
    f_{\varepsilon}
    = \frac{1}{\abs{B(x_0,\varepsilon)}} \boldone_{B(x_0,\varepsilon)}.
\end{equation*}
Clearly, $f_{\varepsilon} \in L^2(\Omega)$.
Assume that $u_{\varepsilon} \in H^1(\Omega)$ is a weak solution to the 
corresponding weak problem with the load $f_{\varepsilon}$.
Choose now a $v \in W^{1,s'}(\Omega) \subset H^1(\Omega)$, $s' > 2$,
according to the type of the boundary value problem,
i.e.\ either $v \in V_D$ or $v \in V_N$, and substitute it into
\eqref{eq:weak_formulation_variational}:
\begin{align*}
    \int_{\Omega} \nabla u_{\varepsilon} \cdot \nabla v \diff x
    &= \int_{\Omega} f_{\varepsilon} v \diff x
        + \sum_{j \in N} \int_{\Gamma_j} g_j v \diff S \\
    &= \frac{1}{\abs{B(x_0,\varepsilon)}}
        \int_{B(x_0,\varepsilon)} v \diff x
        + \sum_{j \in N} \int_{\Gamma_j} g_j v \diff S \\
    &\xrightarrow[]{\varepsilon \to 0} \delta_{x_0}(v)
        + \sum_{j \in N} \int_{\Gamma_j} g_j v \diff S.
\end{align*}
The limit follows from Theorem~\ref{thm:lebesgue_differentiation_theorem}
and the definition of $\delta_{x_0}$.
Thus, the Dirac delta can be thought to model the effect of a point charge
as a limit.

Let us now define what we mean by a weak solution to the problem
\begin{equation}
    \label{eq:poissons_equation_with_dirac_delta}
    \left\{
        \begin{aligned}
            -\Delta u &= \delta_{x_0} && \text{in } \Omega \\
            u &= g_j && \text{on } \Gamma_j, \quad j \in D \\
            \frac{\partial u}{\partial n} &= g_j && \text{on } \Gamma_j,
            \quad j \in N,
        \end{aligned}
    \right.
\end{equation}
where $x_0 \in \Omega$ and the functions $g_j \in T(H^2(\Omega))$ for $j \in D$
and $g_j \in T(H^1(\Omega))$ for $j \in N$ are the same as before.
\begin{definition}
    \label{def:weak_solution_dirac}
    Let $1 < s < 2$ and $2 < s' < \infty$ be conjugate exponents,
    i.e.\ $1/s + 1/s' = 1$.
    A function $u \in W^{1,s}(\Omega)$ is said to be a weak solution to the
    boundary value problem \eqref{eq:poissons_equation_with_dirac_delta} if
    \begin{enumerate}[(i)]
        \item it satisfies
        \begin{equation}
            \label{eq:variational_identity_dirac}
            \int_{\Omega} \nabla u \cdot \nabla v \diff x
            = v(x_0) + \sum_{j \in N} \int_{\Gamma_j} g_j v \diff S
        \end{equation}
        for all $v \in W^{1,s'}(\Omega)$ with $v|_{\Gamma_j} = 0$
        for all $j \in D$,
        \item $u|_{\Gamma_j} = g_j$ for all $j \in D$.
    \end{enumerate}
\end{definition}
An identical definition for the homogeneous Dirichlet problem
can be found in \cite{casas1985} and \cite{arayabehrens2006}.
The conjugate exponents are obviously needed for the expression
\eqref{eq:variational_identity_dirac} to make sense.
It will not be necessary to consider the cases $D \neq \varnothing$
and $D = \varnothing$ separately, other than that the boundary data must again satisfy
the compatibility condition when $D = \varnothing$. We will need to consider
the solvability of the classical weak formulation first before we are able
to find a weak solution to the general problem \eqref{eq:poissons_equation_with_dirac_delta}.

\subsection{Solvability of the Classical Weak Formulation}
\label{subsec:solvability_of_the_classical_weak_formulation}

As a reminder, the classical weak formulation of the boundary value problem 
\eqref{eq:classical_poissons_equation} is the following.
Let $f \in L^2(\Omega)$ and $g_j \in T(H^1(\Omega))$ for all $j \in N$.
It suffices to only consider homogeneous Dirichlet boundary conditions $g_j = 0$ for all $j \in D$.
When $D \neq \varnothing$, let
\begin{equation}
    \label{eq:VD_test_space}
    V_D = \{ v \in H^1(\Omega)
        : v|_{\Gamma_j} = 0 \text{ for all } j \in D \}.
\end{equation}
When $D = \varnothing$, let
\begin{equation}
    \label{eq:VN_test_space}
    V_N = \left\{ v \in H^1(\Omega) : \int_{\Omega} v \diff x = 0 \right\}.
\end{equation}
We wish to find a unique function $u \in V_D$ (resp.\ $u \in V_N$) such that
\begin{equation}
    \label{eq:weak_formulation_recap}
    \int_{\Omega} \nabla u \cdot \nabla v \diff x
    = \int_{\Omega} fv \diff x + \sum_{j \in N} \int_{\Gamma_j} g_j v \diff S
\end{equation}
for all $v \in V_D$ (resp.\ $v \in V_N$).

\subsubsection{Existence and Uniqueness of Solutions}

The identity \eqref{eq:weak_formulation_recap} can be written in the form
\begin{equation}
    \label{eq:abstract_variational_identity}
    a(u,v) = \varphi(v),
\end{equation}
where $a: V \times V \to \mathbb{R}$ and $\varphi: V \to \mathbb{R}$
are defined by
\begin{equation}
    \label{eq:weak_poisson_bilinear_form}
    a(u,v) = \int_{\Omega} \nabla u \cdot \nabla v \diff x
\end{equation}
and
\begin{equation}
    \label{eq:weak_poisson_functional}
    \varphi(v) = \int_{\Omega} fv \diff x
        + \sum_{j \in N} \int_{\Gamma_j} g_j v \diff S
\end{equation}
for all $u,v \in V$, where either $V = V_D$ or $V = V_N$.

For an arbitrary vector space $V$, a mapping of the form $a: V \times V \to \mathbb{R}$
is said to be bilinear if both univariate mappings
$u \mapsto a(u,v)$ and $v \mapsto a(u,v)$ are linear when the other argument
is kept fixed. The mapping $a$ is said to be symmetric if
$a(u,v) = a(v,u)$ for all $u,v \in V$.
Clearly, \eqref{eq:weak_poisson_bilinear_form} defines a symmetric bilinear 
mapping. The functional in \eqref{eq:weak_poisson_functional}
is also obviously linear.

When $V$ is a Hilbert space and the mappings $a$ and $\varphi$ are bilinear
and linear with some additional assumptions, there exists an abstract result
that asserts the existence and uniqueness of a vector $u \in V$ such that
\eqref{eq:abstract_variational_identity} holds for all $v \in V$.
This is the message of the well-known Lax-Milgram theorem.
\begin{theorem}[Lax-Milgram Theorem]
    \label{thm:lax_milgram}
    Let $V$ be a Hilbert space with the inner product $\innerprod{\cdot}{\cdot}$
    and the induced norm $\norm{\cdot}$.
    Let $a: V \times V \to \mathbb{R}$ be a symmetric bilinear
    mapping that satisfies the following two properties.
    \begin{enumerate}[(i)]
        \item Boundedness: There exists a constant $C > 0$ such that
        \begin{equation*}
            \abs{a(u,v)} \leq C \norm{u} \norm{v}
        \end{equation*}
        for all $u,v \in V$.
        \item Ellipticity: There exists a constant $\alpha > 0$ such that
        \begin{equation*}
            a(u,u) \geq \alpha \norm{u}^2
        \end{equation*}
        for all $u \in V$.
    \end{enumerate}
    Then for any $\varphi \in V'$, there exists a unique $u \in V$ such that
    \begin{equation*}
        a(u,v) = \varphi(v)
    \end{equation*}
    for all $v \in V$.
\end{theorem}
\begin{proof}
    From the boundedness and ellipticity it follows that $a(u,u) \geq 0$ for all $u \in V$ and
    $a(u,u) = 0$ if and only if $u=0$.
    Since the mapping $a$ is also bilinear and symmetric, it is
    an inner product on the space $V$.
    
    Let us consider the inner product space $(V, a(\cdot,\cdot))$
    with the induced norm $\norm{\cdot}_a$.
    By the boundedness of $a$, we have that
    \begin{equation*}
        \norm{u}_a^2
        = a(u,u)
        \leq C \norm{u}^2,
    \end{equation*}
    and by the ellipticity of $a$, we have that
    \begin{equation*}
        \norm{u}_a^2
        = a(u,u)
        \geq \alpha \norm{u}^2.
    \end{equation*}
    Combining these, we have that
    \begin{equation*}
        \sqrt{\alpha} \norm{u} \leq \norm{u}_a \leq \sqrt{C} \norm{u}
    \end{equation*}
    for all $u \in V$,
    which means that the norms $\norm{\cdot}$ and $\norm{\cdot}_a$
    are equivalent. It easily follows from this that $(V, a(\cdot,\cdot))$
    is also a Hilbert space, and the dual spaces of
    $(V,\innerprod{\cdot}{\cdot})$ and
    $(V, a(\cdot,\cdot))$ are the same.

    Finally, let $\varphi$ be a functional that belongs to the dual space
    of $(V,\innerprod{\cdot}{\cdot})$. Then $\varphi$ also belongs
    to the dual space of $(V, a(\cdot,\cdot))$,
    and, by the Riesz representation theorem (Theorem~\ref{thm:rieszrepresentationtheorem}),
    there exists a unique
    $u \in V$ such that $\varphi(v) = a(u,v)$ for all $v \in V$,
    which is what we wanted to show.
\end{proof}
The assumption that the mapping $a$ is symmetric is not necessary, but it
simplifies the proof quite a lot, and we will not be dealing with non-symmetric
mappings anyway. For a proof without the symmetricity assumption,
see \cite[Theorem~1 on p.~297]{evans2010}.

The Lax-Milgram theorem now provides a means to prove the existence of
a unique solution to the classical weak formulation of Poisson's equation
for which either $V = V_D$ or $V = V_N$ and the mappings $a$ and $\varphi$ are given by
\eqref{eq:weak_poisson_bilinear_form} and \eqref{eq:weak_poisson_functional}, respectively.
To fulfil the assumptions of the Lax-Milgram theorem,
we need to show the following.
\begin{enumerate}[(i)]
    \item The subspaces $V_D$ and $V_N$ of $H^1(\Omega)$ are Hilbert spaces.
    \item The symmetric bilinear mapping \eqref{eq:weak_poisson_bilinear_form}
    is bounded and elliptic.
    \item The linear functional \eqref{eq:weak_poisson_functional}
    is continuous, i.e.\ bounded, so it belongs to $V'$.
\end{enumerate}
To prove the first assertion, it is enough to show that the subspaces $V_D$ and $V_N$ are closed.
\begin{theorem}
    \label{thm:test_spaces_are_hilbert_spaces}
    Let the subspaces $V_D \subset H^1(\Omega)$ and $V_N \subset H^1(\Omega)$
    be as in \eqref{eq:VD_test_space} and \eqref{eq:VN_test_space}.
    Then both $V_D$ and $V_N$ are Hilbert spaces.
\end{theorem}
\begin{proof}
    It suffices to show that $V_D$ and $V_N$ are closed subspaces.
    Consider first the subspace $V_D$.
    Let $u \in H^1(\Omega)$ be a closure point of $V_D$, that is,
    there exists a sequence $(u_i)_{i=1}^{\infty}$ in $V_D$ that converges
    to $u$ in the $H^1(\Omega)$-norm. Now by the trace theorem (Theorem~\ref{thm:tracetheorem})
    and the fact that $u_i|_{\Gamma_j} = 0$ for all $j \in D$, we get for all $j \in D$ that
    \begin{align*}
        \norm{u}_{L^2(\Gamma_j)}
        &= \norm{u - u_i}_{L^2(\Gamma_j)} \\
        &\leq \norm{u - u_i}_{L^2(\partial \Omega)} \\
        &\leq C \norm{u - u_i}_{H^1(\Omega)} \\
        &\xrightarrow[]{i \to \infty} 0.
    \end{align*}
    Thus, $\norm{u}_{L^2(\Gamma_j)} = 0$, which implies that
    $u|_{\Gamma_j} = 0$ and $u \in V_D$.
    We conclude that $V_D$ is closed.

    Let us then consider the subspace $V_N$.
    Let $u \in H^1(\Omega)$ be a closure point of $V_N$, that is,
    there exists a sequence $(u_i)_{i=1}^{\infty}$ in $V_N$ that converges
    to $u$ in the $H^1(\Omega)$-norm.
    We may estimate the integral of $u$ by
    \begin{align*}
        \abs*{\int_{\Omega} u \diff x}
        &= \abs*{\int_{\Omega} u - u_i \diff x} \\
        &\leq \norm{u - u_i}_{L^1(\Omega)} \\
        &\leq C \norm{u - u_i}_{L^2(\Omega)} \\
        &\leq C \norm{u - u_i}_{H^1(\Omega)} \\
        &\xrightarrow[]{i \to \infty} 0,
    \end{align*}
    where the second inequality follows from Theorem~\ref{thm:lpimbedding}.
    This means that the integral of $u$ is zero, i.e.\ $u \in V_N$,
    which implies that $V_N$ is closed.
\end{proof}

Proving the ellipticity of the bilinear mapping $a$ is by far the most demanding step.
For that, we need the following result which is a variant of Poincaré's inequality in
Theorem~\ref{thm:poincare_inequality}.
\begin{theorem}
    \label{thm:friedrichs_inequality}
    Let $\Omega \subset \mathbb{R}^n$ be a bounded polygonal domain.
    Then there exists a constant $C > 0$ such that
    \begin{equation*}
        \norm{u}_{L^2(\Omega)} \leq C \abs{u}_{H^1(\Omega)}
    \end{equation*}
    for all $u \in V$, where either $V = V_D$ or $V = V_N$.
\end{theorem}
\begin{proof}
    We proceed via proof by contradiction.
    Assume that the claim is not true for any constant $C > 0$.
    Then there exists a sequence $(u_i)_{i=1}^{\infty}$ in $V$ such that
    \begin{equation*}
        \norm{u_i}_{L^2(\Omega)} > i \abs{u_i}_{H^1(\Omega)},
        \quad i=1,2,\dotsc.
    \end{equation*}
    Dividing both sides by $\norm{u_i}_{L^2(\Omega)}$
    and then by $i$ yields
    \begin{equation*}
        \abs{v_i}_{H^1(\Omega)} < \frac{1}{i},
        \quad i=1,2,\dotsc,
    \end{equation*}
    where $v_i = u_i / \norm{u_i}_{L^2(\Omega)}$.
    Clearly, $\norm{v_i}_{L^2(\Omega)} = 1$ for all $i$.
    Note that the sequence $(v_i)_{i=1}^{\infty}$ is bounded in
    $V \subset H^1(\Omega)$:
    \begin{align*}
        \norm{v_i}_{H^1(\Omega)}^2
        &= \norm{v_i}_{L^2(\Omega)}^2 + \abs{v_i}_{H^1(\Omega)}^2 \\
        &\leq 1 + \frac{1}{i^2} \\
        &\leq 2,
        \qquad i=1,2,\dotsc.
    \end{align*}
    By Theorem~\ref{thm:sobolevimbedding}, the imbedding
    $H^1(\Omega) \subset L^2(\Omega)$ is compact. Thus, there exists
    a subsequence of the sequence $(v_i)_{i=1}^{\infty}$ that converges
    to some $v \in L^2(\Omega)$ with respect to the norm
    $\norm{\cdot}_{L^2(\Omega)}$. Without loss of generality,
    denote this subsequence by $(v_i)_{i=1}^{\infty}$ as well.

    Let us then show that $v \in H^1(\Omega)$ with $\nabla v = 0$.
    Let $\phi \in C_0^{\infty}(\Omega)$. First note that
    \begin{align*}
        \abs*{\int_{\Omega} v D_k \phi \diff x
            - \int_{\Omega} v_i D_k \phi \diff x}
        &\leq \int_{\Omega} \abs*{(v - v_i) D_k \phi} \diff x \\
        &\leq \norm{v - v_i}_{L^2(\Omega)} \norm{D_k \phi}_{L^2(\Omega)} \\
        &\xrightarrow[]{i \to \infty} 0
    \end{align*}
    for all $k=1,2,\dotsc,n$, where the second inequality follows from
    Hölder's inequality. Using the above limit and the definition of a weak partial derivative, we get
    \begin{align*}
        \abs*{\int_{\Omega} v D_k \phi \diff x}
        &= \abs*{\lim_{i \to \infty} \int_{\Omega} v_i D_k \phi \diff x} \\
        &= \lim_{i \to \infty} \hspace{0.7mm}
            \abs*{\int_{\Omega} D_k v_i \phi \diff x} \\
        &\leq \limsup_{i \to \infty} \int_{\Omega} \abs{D_k v_i \phi} \diff x \\
        &\leq \limsup_{i \to \infty} \hspace{0.7mm} \norm{D_k v_i}_{L^2(\Omega)}
            \norm{\phi}_{L^2(\Omega)} \\
        &\leq \limsup_{i \to \infty} \hspace{0.7mm} \abs{v_i}_{H^1(\Omega)}
            \norm{\phi}_{L^2(\Omega)} \\
        &\leq \limsup_{i \to \infty} \hspace{0.7mm}
            \frac{1}{i} \norm{\phi}_{L^2(\Omega)} \\
        &= 0.
    \end{align*}
    Note above also the use of Hölder's inequality
    and the bound $\abs{v_i}_{H^1(\Omega)} < 1/i$ for all $i$.
    The above implies that
    \begin{equation*}
        \int_{\Omega} v D_k \phi \diff x = 0
    \end{equation*}
    for all $k=1,2,\dotsc,n$, which then implies that
    $D_k v = 0$ for all $k=1,2,\dotsc,n$. In other words, $\nabla u = 0$.
    
    Since $\Omega$ is connected, the fact that $\nabla v = 0$ implies
    that $v$ is constant almost everywhere in $\Omega$. Say $v = c$.
    We show next that $c$ must be equal to zero. Assume first that $V = V_D$. 
    Clearly, $v|_{\partial \Omega} = c$.
    Recall that $v_i \in V_D$ and, thus, $v_i|_{\Gamma_j} = 0$ for all $j \in D$.
    Now by the trace theorem, we get for any $j \in D$ that
    \begin{align*}
        \norm{v}_{L^2(\Gamma_j)}^2
        &= \norm{v - v_i}_{L^2(\Gamma_j)}^2 \\
        &\leq \norm{v - v_i}_{L^2(\partial \Omega)}^2 \\
        &\leq C \norm{v - v_i}_{H^1(\Omega)}^2 \\
        &= C \left(\norm{v - v_i}_{L^2(\Omega)}^2
            + \abs{v_i}_{H^1(\Omega)}^2 \right) \\
        &\leq C \left(\norm{v - v_i}_{L^2(\Omega)}^2
            + \frac{1}{i^2} \right) \\
        &\xrightarrow[]{i \to \infty} 0.
    \end{align*}
    Thus,
    \begin{equation*}
        0
        = \norm{v}_{L^2(\Gamma_j)}
        = \norm{c}_{L^2(\Gamma_j)}
        = \abs{c} \abs{\Gamma_j},
    \end{equation*}
    where $\abs{\Gamma_j}$ is the $(n-1)$-dimensional measure of $\Gamma_j$.
    Since this measure is non-zero, the constant $c$ must be zero when $V = V_D$.

    Assume then that $V = V_N$. By Theorem~\ref{thm:lpimbedding}, we get
    \begin{align*}
        \abs*{\int_{\Omega} v \diff x}
        &= \abs*{\int_{\Omega} v - v_i \diff x} \\
        &\leq \norm{v - v_i}_{L^1(\Omega)} \\
        &\leq C \norm{v - v_i}_{L^2(\Omega)} \\
        &\xrightarrow[]{i \to \infty} 0.
    \end{align*}
    Thus,
    \begin{equation*}
        0
        = \int_{\Omega} v \diff x
        = c \abs{\Omega},
    \end{equation*}
    and since $\abs{\Omega} > 0$, the constant $c$ must be zero.

    However, the fact that $\norm{v_i}_{L^2(\Omega)} = 1$ for all $i$
    implies that $\norm{v}_{L^2(\Omega)} = 1$ as follows. By the inverse
    triangle inequality, we get
    \begin{equation*}
        \abs*{\norm{v}_{L^2(\Omega)} - \norm{v_i}_{L^2(\Omega)}}
        \leq \norm{v - v_i}_{L^2(\Omega)}
        \xrightarrow[]{i \to \infty} 0,
    \end{equation*}
    from which we get that
    \begin{equation*}
        \norm{v}_{L^2(\Omega)}
        = \lim_{i \to \infty} \norm{v_i}_{L^2(\Omega)}
        = 1.
    \end{equation*}
    We have now arrived at a contradiction between the results $v = 0$
    almost everywhere in $\Omega$ and $\norm{v}_{L^2(\Omega)} = 1$.
    The initial claim must thus hold.
\end{proof}

We may now prove that the mappings $a$ and $\varphi$ in the weak formulation of Poisson's equation
satisfy the assumptions of the Lax-Milgram theorem.
\begin{theorem}
    \label{thm:weak_poisson_reqs_2_and_3}
    Let either $V = V_D$ or $V = V_N$.
    The bilinear mapping $a: V \times V \to \mathbb{R}$ in
    \eqref{eq:weak_poisson_bilinear_form} is bounded and elliptic,
    and the linear functional $\varphi: V \to \mathbb{R}$ in
    \eqref{eq:weak_poisson_functional} is bounded.
\end{theorem}
\begin{proof}
    By the Cauchy-Schwarz and Hölder's inequalities, we have
    \begin{align*}
        \abs{a(u,v)}
        &\leq \int_{\Omega} \abs{\nabla u \cdot \nabla v} \diff x \\
        &\leq \int_{\Omega} \abs{\nabla u} \abs{\nabla v} \diff x \\
        &\leq \norm{\nabla u}_{L^2(\Omega)} \norm{\nabla v}_{L^2(\Omega)} \\
        &\leq \norm{u}_{H^1(\Omega)} \norm{v}_{H^1(\Omega)}
    \end{align*}
    for all $u,v \in V$. Thus, $a$ is bounded.

    By Theorem~\ref{thm:friedrichs_inequality}, we have
    \begin{align*}
        a(u,u)
        &= \int_{\Omega} \nabla u \cdot \nabla u \diff x \\
        &= \abs{u}_{H^1(\Omega)}^2 \\
        &= \frac{1}{2} \abs{u}_{H^1(\Omega)}^2
            + \frac{1}{2} \abs{u}_{H^1(\Omega)}^2 \\
        &\geq \frac{1}{2C} \norm{u}_{L^2(\Omega)}^2
            + \frac{1}{2} \abs{u}_{H^1(\Omega)}^2 \\
        &\geq \min\left\{ \frac{1}{2C}, \frac{1}{2} \right\}
            \left( \norm{u}_{L^2(\Omega)}^2 + \abs{u}_{H^1(\Omega)}^2 \right) \\
        &= \min\left\{ \frac{1}{2C}, \frac{1}{2} \right\}
            \norm{u}_{H^1(\Omega)}^2 \\
        &= \alpha \norm{u}_{H^1(\Omega)}^2
    \end{align*}
    for some constant $\alpha > 0$ and for all $u \in V$. Thus, $a$ is elliptic.

    Finally, by Hölder's inequality and the trace theorem, we have
    \begin{align*}
        \abs{\varphi(v)}
        &\leq \int_{\Omega} \abs{fv} \diff x
            + \sum_{j \in N} \int_{\Gamma_j} \abs{g_j v} \diff S \\
        &\leq \norm{f}_{L^2(\Omega)} \norm{v}_{L^2(\Omega)}
            + \sum_{j \in N} \norm{g_j}_{L^2(\partial \Omega)} \norm{v}_{L^2(\partial \Omega)} \\
        &\leq \norm{f}_{L^2(\Omega)} \norm{v}_{H^1(\Omega)}
            + \sum_{j \in N} \norm{g_j}_{L^2(\partial \Omega)} C \norm{v}_{H^1(\Omega)} \\
        &= \left( \norm{f}_{L^2(\Omega)} + C \sum_{j \in N} \norm{g_j}_{L^2(\partial \Omega)} 
            \right) \norm{v}_{H^1(\Omega)}
    \end{align*}
    for all $v \in V$. Thus, $\varphi$ is bounded and $\varphi \in V'$.
\end{proof}

The existence of a unique solution to the classical weak formulation of Poisson's equation
with homogeneous Dirichlet boundary conditions or with the zero mean value requirement
follows now directly from the Lax-Milgram theorem.
For completeness, let us also consider the existence of weak solutions
as per Definition~\ref{def:weak_solution}.
\begin{theorem}
    \label{thm:weak_poisson_is_solvable}
    Let $\Omega \subset \mathbb{R}^2$ be a bounded polygonal domain.
    Let $f \in L^2(\Omega)$, $g_j \in T(H^2(\Omega))$ for all $j \in D$
    and $g_j \in T(H^1(\Omega))$ for all $j \in N$.
    Assume that there exists a function $g_D \in H^2(\Omega)$ such that
    $g_D|_{\Gamma_j} = g_j$ for all $j \in D$, and if $D = \varnothing$,
    assume that $f$ and $g_j$ satisfy the compatibility condition.
    Then the boundary value problem
    \begin{equation*}
        \left\{
            \begin{aligned}
                -\Delta u &= f && \text{in } \Omega \\
                u &= g_j && \text{on } \Gamma_j, \quad j \in D \\
                \frac{\partial u}{\partial n} &= g_j && \text{on } \Gamma_j,
                \quad j \in N
            \end{aligned}
        \right.
    \end{equation*}
    has a weak solution $u \in H^1(\Omega)$.
    When $D \neq \varnothing$, $u$ is unique.
    When $D = \varnothing$, $u$ is unique up to an additive constant.
\end{theorem}
\begin{proof}
    When $D \neq \varnothing$, the existence of a weak solution follows
    directly from the Lax-Milgram theorem, i.e.\ Theorem~\ref{thm:lax_milgram},
    and the discussion at the beginning of this section.
    If $u$ and $v$ are weak solutions, then $u-v$ is clearly
    a weak solution to the problem
    \begin{equation}
        \label{eq:weak_poisson_is_solvable_homogeneous}
        \left\{
            \begin{aligned}
                -\Delta w &= 0 && \text{in } \Omega \\
                u &= 0 && \text{on } \Gamma_j, \quad j \in D \\
                \frac{\partial u}{\partial n} &= 0 && \text{on } \Gamma_j,
                \quad j \in N.
            \end{aligned}
        \right.
    \end{equation}
    Clearly, $w = 0$ is a weak solution to the problem 
    \eqref{eq:weak_poisson_is_solvable_homogeneous}, and it is unique
    by the Lax-Milgram theorem. Thus, $u - v = 0$, which implies that the solution is unique.

    When $D = \varnothing$, the Lax-Milgram theorem implies the existence
    of a unique weak solution in the space $V_N$. If now $u$ and $v$ are two
    weak solutions, not necessarily in the space $V_N$, then $u-v$ is again
    a weak solution to the homogeneous problem 
    \eqref{eq:weak_poisson_is_solvable_homogeneous}.
    So is the function $u-v - (\overline{u} - \overline{v}) \in V_N$, where
    \begin{equation*}
        \overline{u} = \frac{1}{\abs{\Omega}} \int_{\Omega} u \diff x.
    \end{equation*}
    By the Lax-Milgram theorem, $u-v-(\overline{u} - \overline{v}) = 0$,
    i.e.\ $u = v + C$ with the constant $C = \overline{u} - \overline{v}$,
    which implies that a weak solution is unique up to an additive constant.
\end{proof}

The Lax-Milgram theorem is a very general existence result.
It could be used to show the existence and uniqueness of solutions
to other second-order elliptic boundary value problems as well
and in higher dimensions than $n=2$. One could also consider more
general domains with Lipschitz boundaries with little modifications.
For additional information, see \cite[Chapter 6]{evans2010}.

\subsubsection{Regularity of the Weak Solutions}
\label{subsubsec:regularity_of_the_weak_solutions}

Poisson's equation contains the second-order differential operator $\Delta$,
which motivates the question whether a weak solution $u \in H^1(\Omega)$
actually belongs to the space $H^2(\Omega)$.
This is of particular importance in finite element analysis as typical convergence
results rely on the answer being yes. Whether the answer is indeed yes
depends on the domain $\Omega$. For a polygonal domain, we need the following
additional assumptions.
\begin{assumption}
    \label{ass:regular_polygonal_domain}
    Let $\Omega \subset \mathbb{R}^2$ be a bounded polygonal domain.
    The linear boundary segments $\Gamma_j$, $j = 1,2,\dotsc,J$,
    that constitute the boundary $\partial \Omega$ are arranged so that
    $\Gamma_j$ is followed by $\Gamma_{j+1}$.
    For $j=J$, set $\Gamma_{J+1} = \Gamma_1$.
    Denote the angle between the segments $\Gamma_j$ and $\Gamma_{j+1}$
    by $\theta_j$. The angle is the one inside $\Omega$.
    Assume that the angles $\theta_j$ satisfy the following assumptions.
    \begin{itemize}[(i)]
        \item $0 < \theta_j < \pi$ for all $j=1,2,\dotsc,J$,
        i.e.\ $\Omega$ is convex.
        \item For all $j=1,2,\dotsc,J$, if $\Gamma_j$ has a prescribed
        Dirichlet boundary condition and $\Gamma_{j+1}$ has a prescribed
        Neumann boundary condition, or the other way around, then
        $0 < \theta_j < \pi / 2$.
    \end{itemize}
\end{assumption}

Now we have the following regularity result from \cite[Theorem 4.4.4.13 on p.~245]{grisvard2011}
and \cite[Theorem 1]{grisvard1976}.
\begin{theorem}
    \label{thm:H2_regularity}
    Let $\Omega \subset \mathbb{R}^2$ be a polygonal domain that
    satisfies Assumption~\ref{ass:regular_polygonal_domain}.
    Then the weak solutions in Theorem~\ref{thm:weak_poisson_is_solvable}
    belong to the space $H^2(\Omega)$.
\end{theorem}
When $u \in H^2(\Omega)$, then $\Delta u \in L^2(\Omega)$
and $\partial u / \partial n$ on $\Gamma_j$ exists in the sense of traces
for all $j \in N$.
It turns out that when the weak solution belongs
to the space $H^2(\Omega)$, Poisson's equation holds in the sense of $L^2$
and the Neumann boundary conditions hold in the sense of traces.
Note that the Dirichlet boundary conditions already hold in the sense
of traces because it is embedded into the solution space.
\begin{theorem}
    \label{thm:weak_solution_is_strong_solution}
    Let $u \in H^1(\Omega)$ be a weak solution provided by
    Theorem~\ref{thm:weak_poisson_is_solvable}. Assume that $u \in H^2(\Omega)$.
    Then
    \begin{enumerate}[(i)]
        \item $-\Delta u = f$,
        \item $\frac{\partial u}{\partial n}|_{\Gamma_j} = g_j$ for all $j \in N$.
    \end{enumerate}
\end{theorem}
For a proof, see for example
\cite[Proposition 5.1.9 on p.~131]{scottbrenner2007}.
Let us finish off the discussion on the classical weak
formulation of Poisson's equation
by proving an a priori $H^2$-norm estimate for the weak solution
with homogeneous boundary values $g_j = 0$ for all $j=1,2,\dotsc,J$.
This estimate will be useful later.
\begin{theorem}
    \label{thm:a_priori_H2_estimate}
    By Theorem~\ref{thm:weak_poisson_is_solvable},
    let $u \in H^1(\Omega)$ be the unique weak solution
    to the boundary value problem
    \begin{equation*}
        \left\{
            \begin{aligned}
                -\Delta u &= f && \text{in } \Omega \\
                u &= 0 && \text{on } \Gamma_j, \quad j \in D \\
                \frac{\partial u}{\partial n} &= 0 && \text{on } \Gamma_j,
                \quad j \in N.
            \end{aligned}
        \right.
    \end{equation*}
    When $D = \varnothing$, the uniqueness is enforced by requiring
    that $\int_{\Omega} u \diff x = 0$. Assume that $u \in H^2(\Omega)$.
    Then there exists a constant $C > 0$ independent of $u$ and $f$ such that
    \begin{equation*}
        \norm{u}_{H^2(\Omega)} \leq C \norm{f}_{L^2(\Omega)}.
    \end{equation*}
\end{theorem}
\begin{proof}
    Note that $u \in V_D$ or $u \in V_N$ depending
    on the type of the boundary value problem.
    Theorem~\ref{thm:friedrichs_inequality} now implies that
    \begin{align}
        \norm{u}_{H^2(\Omega)}^2
        &= \norm{u}_{L^2(\Omega)}^2
            + \abs{u}_{H^1(\Omega)}^2
                + \abs{u}_{H^2(\Omega)}^2 \nonumber \\
        &\leq C^2 \abs{u}_{H^1(\Omega)}^2
            + \abs{u}_{H^1(\Omega)}^2
                + \abs{u}_{H^2(\Omega)}^2 \nonumber \\
        \label{eq:a_priori_H2_estimate_intmed1}
        &= (C^2 + 1) \abs{u}_{H^1(\Omega)}^2 + \abs{u}_{H^2(\Omega)}^2,
    \end{align}
    where $C > 0$ is some constant independent of $u$ and $f$.

    Consider next the seminorm $\abs{u}_{H^1(\Omega)}$.
    By the definition of a weak solution, Hölder's inequality
    and Theorem~\ref{thm:friedrichs_inequality}, we get
    \begin{align*}
        \abs{u}_{H^1(\Omega)}^2
        &= \int_{\Omega} \nabla u \cdot \nabla u \diff x \\
        &= \int_{\Omega} f u \diff x \\
        &\leq \norm{f}_{L^2(\Omega)} \norm{u}_{L^2(\Omega)} \\
        &\leq \norm{f}_{L^2(\Omega)} C \abs{u}_{H^1(\Omega)},
    \end{align*}
    where $C$ is the same constant as in \eqref{eq:a_priori_H2_estimate_intmed1}.
    Dividing both sides by $\abs{u}_{H^1(\Omega)}$ gives
    \begin{equation}
        \label{eq:a_priori_H2_estimate_intmed2}
        \abs{u}_{H^1(\Omega)} \leq C \norm{f}_{L^2(\Omega)}.
    \end{equation}
    If $\abs{u}_{H^1(\Omega)} = 0$, this inequality is also obviously true.

    By \cite[Proof of Theorem 4.3.1.4 on p.~199]{grisvard2011}, it holds that
    \begin{equation*}
        \abs{u}_{H^2(\Omega)} \leq \norm{\Delta u}_{L^2(\Omega)},
    \end{equation*}
    which combined with the result $-\Delta u = f$ from
    Theorem~\ref{thm:weak_solution_is_strong_solution} yields
    \begin{equation}
        \label{eq:a_priori_H2_estimate_intmed3}
        \abs{u}_{H^2(\Omega)} \leq \norm{f}_{L^2(\Omega)}.
    \end{equation}

    Substituting now \eqref{eq:a_priori_H2_estimate_intmed2}
    and \eqref{eq:a_priori_H2_estimate_intmed3}
    into \eqref{eq:a_priori_H2_estimate_intmed1} gives
    \begin{align*}
        \norm{u}_{H^2(\Omega)}^2
        &\leq (C^2+1) C^2 \norm{f}_{L^2(\Omega)}^2 + \norm{f}_{L^2(\Omega)}^2 \\
        &= (C^4 + C^2 + 1) \norm{f}_{L^2(\Omega)}^2
    \end{align*}
    Taking the square root from both sides finishes the proof.
\end{proof}

\subsection{Solvability of Poisson's Equation with a Dirac Delta Load}
\label{subsec:poissons_equation_with_a_concentrated_load}

Let us recall what we mean by a weak solution to the boundary value problem
\begin{equation}
    \label{eq:poissons_eq_with_dirac_delta_recap}
    \left\{
        \begin{aligned}
            -\Delta u &= \delta_{x_0} && \text{in } \Omega \\
            u &= g_j && \text{on } \Gamma_j, \quad j \in D \\
            \frac{\partial u}{\partial n} &= g_j && \text{on } \Gamma_j,
            \quad j \in N,
        \end{aligned}
    \right.
\end{equation}
where the boundary data $g_j$ for $j=1,2,\dotsc,J$ are the same as before. Let $1 < s < 2$.
A function $u \in W^{1,s}(\Omega)$ is a weak solution
if $u|_{\Gamma_j} = g_j$ for all $j \in D$, and it satisfies
\begin{equation*}
    \int_{\Omega} \nabla u \cdot \nabla v \diff x
    = v(x_0) + \sum_{j \in N} \int_{\Gamma_j} g_j v \diff S
\end{equation*}
for all $v \in W^{1,s'}(\Omega)$ with $v|_{\Gamma_j} = 0$ for all $j \in D$,
where $2 < s' < \infty$ is the conjugate exponent of $s$.

The Lax-Milgram theorem cannot now be used to deduce the existence of a unique
solution because the problem is not formulated in the Hilbert space $H^1(\Omega)$.
In particular, $\delta_{x_0} \notin H^1(\Omega)'$ as there is no obvious
well-defined meaning to the expression $\delta_{x_0}(v) = v(x_0)$ for
an arbitrary $v \in H^1(\Omega)$.
Fortunately, the problem does have a solution,
and when $\Omega$ satisfies Assumption~\ref{ass:regular_polygonal_domain},
it is possible to show that a solution is also unique.
Casas \cite{casas1985} proves the existence and uniqueness of
a solution to the pure Dirichlet problem with homogeneous boundary
values in convex polygonal domains. The proof by Casas is an abstract
existence proof that uses results from functional analysis.
An outline for a somewhat more constructive proof is given by
Araya et al.\ \cite{arayabehrens2006} to the same problem that Casas considers.
Namely, there exists a fundamental solution to a specific instance of
the problem \eqref{eq:poissons_eq_with_dirac_delta_recap} that arises from the
classical theory of partial differential equations.
This fundamental solution is commonly called Green's function,
and it can be used to deduce the existence of a weak solution to the general problem
\eqref{eq:poissons_eq_with_dirac_delta_recap}.

\subsubsection{Green's Function}
\label{subsubsec:greens_function}

Consider Poisson's equation in free space:
\begin{equation}
    \label{eq:poissons_eq_in_free_space}
    -\Delta u = f \quad \text{in } \mathbb{R}^2.
\end{equation}
When $f \in C_0^2(\mathbb{R}^2)$, the problem \eqref{eq:poissons_eq_in_free_space} has
a classical solution $u \in C^2(\mathbb{R}^2)$ with the explicit formula
\begin{equation*}
    u(x) = \int_{\mathbb{R}^2} \Phi(x-y) f(y) \diff y,
\end{equation*}
where $\Phi$ is the fundamental solution of Laplace's equation, that is,
the equation $\Delta u = 0$ \cite[Theorem~1 on p.~23]{evans2010}.
The fundamental solution $\Phi$ is given by
\begin{equation*}
    \Phi(x) = -\frac{1}{2 \pi} \ln \abs{x}
\end{equation*}
for $x \in \mathbb{R}^2 \setminus \{ 0 \}$.
By a direct calculation,
\begin{align*}
    \Delta \Phi(x)
    &= \frac{\partial^2 \Phi}{\partial x_1^2}(x)
        + \frac{\partial^2 \Phi}{\partial x_2^2}(x) \\
    &= -\frac{1}{2\pi} \frac{x_2^2 - x_1^2}{\abs{x}^4}
        -\frac{1}{2\pi} \frac{x_1^2 - x_2^2}{\abs{x}^4} \\
    &= 0
\end{align*}
for all $x \in \mathbb{R}^2 \setminus \{ 0 \}$.

We may shift the singularity at the origin to an arbitrary point
$x_0 \in \mathbb{R}^2$ and define
\begin{equation}
    \label{eq:greens_function}
    \Phi_{x_0}(x) = -\frac{1}{2 \pi} \ln \abs{x - x_0}
\end{equation}
for $x \in \mathbb{R}^2 \setminus \{ x_0 \}$.
Clearly, $\Delta \Phi_{x_0}(x) = 0$ for all $x \in \mathbb{R}^2 \setminus \{x_0\}$.
Set $\Phi_{x_0}(x_0) = 0$.
The function $\Phi_{x_0}$ turns out to solve the problem 
\eqref{eq:poissons_eq_with_dirac_delta_recap} when the boundary values
are set accordingly.
This is illustrated by the following theorem.
\begin{theorem}
    \label{thm:greens_function_is_solution}
    Let $\Omega \subset \mathbb{R}^2$ be a bounded polygonal domain.
    Let $x_0 \in \Omega$, and define $\Phi_{x_0}$ by \eqref{eq:greens_function}.
    Let $1 < s < 2$. Then $\Phi_{x_0} \in W^{1,s}(\Omega)$,
    and $\Phi_{x_0}$ is a weak solution to the boundary value problem
    \begin{equation}
        \label{eq:greens_function_is_solution_problem}
        \left\{
            \begin{aligned}
                -\Delta u &= \delta_{x_0} && \text{in } \Omega \\
                u &= \Phi_{x_0} && \text{on } \Gamma_j, \quad j \in D \\
                \frac{\partial u}{\partial n} &=
                \frac{\partial \Phi_{x_0}}{\partial n} && \text{on } \Gamma_j,
                \quad j \in N.
            \end{aligned}
        \right.
    \end{equation}
\end{theorem}
\begin{proof}
    The gradient of $\Phi_{x_0}$ is given by
    \begin{equation*}
        \nabla \Phi_{x_0}(x) = -\frac{1}{2\pi} \frac{x-x_0}{\abs{x-x_0}^2}
    \end{equation*}
    for $x \in \mathbb{R}^2 \setminus \{ x_0 \}$.
    Set $\nabla \Phi_{x_0}(x_0) = 0$.
    Let us show that $\Phi_{x_0} \in L^s(\Omega)$ and
    $\nabla \Phi_{x_0} \in L^s(\Omega) \times L^s(\Omega)$.
    Since $\Omega$ is bounded, let $B(x_0,R) \subset \mathbb{R}^2$
    be a ball that is centered at $x_0$ with radius $R > 1$
    such that $\Omega \subset B(x_0,R)$. Now
    \begin{align}
        \norm{\Phi_{x_0}}_{L^s(\Omega)}^s
        &\leq \norm{\Phi_{x_0}}_{L^s(B(x_0,R))}^s \nonumber \\
        &= \frac{1}{(2\pi)^s} \int_{B(x_0,R)} \abs*{\ln\abs{x-x_0}}^s \diff x \nonumber \\
        &= \frac{1}{(2\pi)^s} \int_{0}^{R} \int_{\partial B(x_0,r)}
            \abs*{\ln r}^s \diff S \diff r \nonumber \\
        &= \frac{1}{(2\pi)^{s-1}} \int_{0}^{R} r \abs*{\ln r}^s \diff r \nonumber \\
        \label{eq:greens_func_weak_diff_L2_intmed1}
        &= \frac{1}{(2\pi)^{s-1}} \left(
            \int_{0}^{1} r (-\ln r)^s \diff r
                + \int_{1}^{R} r \ln^s r \diff r
                \right).
    \end{align}
    The second integral in \eqref{eq:greens_func_weak_diff_L2_intmed1} is obviously finite.
    For the first integral, the loose estimate $x \leq e^x$
    for all $x \in \mathbb{R}$ gives
    \begin{align*}
        \int_{0}^{1} r (-\ln r)^s \diff r
        &\leq \int_{0}^{1} r \left(e^{-\ln r} \right)^s \diff r \\
        &= \int_{0}^{1} r^{1-s} \diff r \\
        &= \lim_{\varepsilon \to 0} \int_{\varepsilon}^{1} r^{1-s} \diff r \\
        &= \lim_{\varepsilon \to 0} \left(
            \frac{1}{2-s} \left( 1 - \varepsilon^{2-s} \right)
                \right) \\
        &= \frac{1}{2-s}.
    \end{align*}
    The limit is finite since $2-s > 0$. Thus,
    $\norm{\Phi_{x_0}}_{L^s(\Omega)} < \infty$ and $\Phi_{x_0} \in L^s(\Omega)$.

    Consider then the integrability of the derivatives:
    \begin{align*}
        \norm{D_i \Phi_{x_0}}_{L^s(\Omega)}^s
        &\leq \norm{D_i \Phi_{x_0}}_{L^s(B(x_0,R))}^s \\
        &= \frac{1}{(2\pi)^s} \int_{B(x_0,R)}
            \frac{(x_i - x_{0i})^s}{\abs{x-x_0}^{2s}} \diff x \\
        &\leq \frac{1}{(2\pi)^s} \int_{B(x_0,R)}
            \frac{\abs{x-x_0}^s}{\abs{x-x_0}^{2s}} \diff x \\
        &= \frac{1}{(2\pi)^s} \int_{B(x_0,R)}
            \frac{1}{\abs{x-x_0}^s} \diff x \\
        &= \frac{1}{(2\pi)^s} \int_{0}^{R} \int_{\partial B(x_0,r)}
            \frac{1}{r^s} \diff S \diff r \\
        &= \frac{1}{(2\pi)^{s-1}} \int_{0}^{R} r^{1-s} \diff r,
        \quad i=1,2.
    \end{align*}
    This is the same integral as above which is finite.
    Thus, $\nabla \Phi_{x_0} \in L^s(\Omega) \times L^s(\Omega)$.

    To conclude that $\Phi_{x_0} \in W^{1,s}(\Omega)$, we need to show that
    $\nabla \Phi_{x_0}$ is the weak gradient of $\Phi_{x_0}$.
    This is not obvious due to the singularity at the point $x_0$.
    Let $\varepsilon > 0$ be small so that
    $B(x_0,\varepsilon) \subset \Omega$.
    We need to consider $\Phi_{x_0}$ inside and outside this ball separately.
    Outside the ball, $\Phi_{x_0}$ is smooth and bounded
    so we may use classical results from calculus.
    
    Let $\phi \in C_0^{\infty}(\Omega)$. Then for $i=1$ and $i=2$, we have
    \begin{equation}
        \label{eq:greens_function_weak_diff_intmed1}
        \int_{\Omega} \Phi_{x_0} D_i \phi \diff x
        = \int_{B(x_0,\varepsilon)} \Phi_{x_0} D_i \phi \diff x
            + \int_{\Omega \setminus \overline{B(x_0,\varepsilon)}}
                \Phi_{x_0} D_i \phi \diff x.
    \end{equation}
    Bringing $\varepsilon$ to zero, the first integral in
    \eqref{eq:greens_function_weak_diff_intmed1} becomes
    \begin{equation}
        \label{eq:greens_function_weak_diff_intmed2}
        \lim_{\varepsilon \to 0}
            \int_{B(x_0,\varepsilon)} \Phi_{x_0} D_i \phi \diff x
        = \int_{\Omega} \lim_{\varepsilon \to 0} \boldone_{B(x_0,\varepsilon)}
            \Phi_{x_0} D_i \phi \diff x
        = 0,   
    \end{equation}
    where we used the dominated convergence theorem with the estimate
    $\abs{\Phi_{x_0} D_i \phi} \leq \abs{\Phi_{x_0}} \norm{D_i\phi}_{L^{\infty}(\Omega)}
    \in L^1(\Omega)$. By integration by parts, the second integral in
    \eqref{eq:greens_function_weak_diff_intmed1} becomes
    \begin{equation}
        \label{eq:greens_function_weak_diff_intmed3}
        \int_{\Omega \setminus \overline{B(x_0,\varepsilon)}}
            \Phi_{x_0} D_i \phi \diff x
        = -\int_{\Omega \setminus \overline{B(x_0,\varepsilon)}}
            D_i \Phi_{x_0} \phi \diff x
                - \int_{\partial B(x_0,\varepsilon)} \Phi_{x_0} \phi n_i \diff S,
    \end{equation}
    where $n=(n_1, n_2)$ is the exterior unit normal to $\partial B(x_0,\varepsilon)$.
    By the dominated convergence theorem, the first integral in
    \eqref{eq:greens_function_weak_diff_intmed3} has the limit
    \begin{align*}
        \lim_{\varepsilon \to 0}
            \int_{\Omega \setminus \overline{B(x_0,\varepsilon)}}
                D_i \Phi_{x_0} \phi \diff x
        &= \int_{\Omega}
                \lim_{\varepsilon \to 0}
                    \boldone_{\Omega \setminus \overline{B(x_0,\varepsilon)}}
                        D_i \Phi_{x_0} \phi \diff x \\
        &= \int_{\Omega} D_i \Phi_{x_0} \phi \diff x,
    \end{align*}
    where we used the estimate $\abs{D_i \Phi_{x_0} \phi} \leq
    \abs{D_i \Phi_{x_0}}\norm{\phi}_{L^{\infty}(\Omega)} \in L^1(\Omega)$.
    The boundary term in \eqref{eq:greens_function_weak_diff_intmed3}
    can be estimated by
    \begin{align*}
        \abs*{\int_{\partial B(x_0,\varepsilon)} \Phi_{x_0} \phi n_i \diff S}
        &\leq \int_{\partial B(x_0,\varepsilon)} \abs{\Phi_{x_0} \phi n_i} \diff S\\
        &\leq \norm{\phi}_{L^{\infty}(\Omega)}
            \int_{\partial B(x_0,\varepsilon)} \abs*{\ln \varepsilon} \diff S \\
        &= \norm{\phi}_{L^{\infty}(\Omega)} 2 \pi \varepsilon \abs{\ln \varepsilon} \\
        &\xrightarrow[]{\varepsilon \to 0} 0.
    \end{align*}
    The limit follows from a simple application of L'Hôpital's rule.
    Thus, the boundary term vanishes as $\varepsilon \to 0$.
    Bringing now $\varepsilon$ to zero in 
    \eqref{eq:greens_function_weak_diff_intmed1}, we get
    \begin{equation*}
        \int_{\Omega} \Phi_{x_0} D_i \phi \diff x
        = - \int_{\Omega} D_i \Phi_{x_0} \phi \diff x.
    \end{equation*}
    That is, the weak derivatives exist and $\Phi_{x_0} \in W^{1,s}(\Omega)$.

    It remains to show that $\Phi_{x_0}$ solves the problem
    \eqref{eq:greens_function_is_solution_problem} in the weak sense.
    The boundary values obviously hold.
    Let $v \in W^{1,s'}(\Omega)$ be such that $v|_{\Gamma_j} = 0$ for all $j \in D$ and
    $s'$ is the conjugate exponent of $s$. Now
    \begin{equation}
        \label{eq:greens_function_weak_diff_intmed4}
        \int_{\Omega} \nabla \Phi_{x_0} \cdot \nabla v \diff x
        = \int_{B(x_0,\varepsilon)} \nabla \Phi_{x_0} \cdot \nabla v \diff x
            + \int_{\Omega \setminus \overline{B(x_0,\varepsilon)}}
                \nabla \Phi_{x_0} \cdot \nabla v \diff x.
    \end{equation}
    By the Cauchy-Schwarz and Hölder's inequalities, we have
    \begin{equation*}
        \abs{\nabla \Phi_{x_0} \cdot \nabla v}
        \leq \abs{\nabla \Phi_{x_0}} \abs{\nabla v}
        \in L^1(\Omega).
    \end{equation*}
    Thus, we may apply the dominated convergence theorem to the limit of the
    first integral in \eqref{eq:greens_function_weak_diff_intmed4} as $\varepsilon$ tends to zero:
    \begin{equation*}
        \lim_{\varepsilon \to 0} \int_{B(x_0,\varepsilon)}
            \nabla \Phi_{x_0} \cdot \nabla v \diff x
        = \int_{\Omega} \lim_{\varepsilon \to 0} \boldone_{B(x_0,\varepsilon)}
            \nabla \Phi_{x_0} \cdot \nabla v \diff x
        = 0.
    \end{equation*}
    By Green's formula, the second integral in
    \eqref{eq:greens_function_weak_diff_intmed4} becomes
    \begin{align*}
        \int_{\Omega \setminus \overline{B(x_0,\varepsilon)}}
                \nabla \Phi_{x_0} \cdot \nabla v \diff x
        &= -\int_{\Omega \setminus \overline{B(x_0,\varepsilon)}}
            \Delta \Phi_{x_0} v \diff x
                + \sum_{j \in N} \int_{\Gamma_j}
                    \frac{\partial \Phi_{x_0}}{\partial n} v \diff S\\
        &\hspace{40.7mm} - \int_{\partial B(x_0,\varepsilon)}
                    \frac{\partial \Phi_{x_0}}{\partial n} v \diff S.
    \end{align*}
    The first integral vanishes because $\Delta \Phi_{x_0} = 0$
    everywhere except at the point $x_0$.
    The last boundary integral can be simplified as follows.
    Note that $n = (x-x_0)/\varepsilon$. Now
    \begin{align*}
        - \int_{\partial B(x_0,\varepsilon)}
            \frac{\partial \Phi_{x_0}}{\partial n} v \diff S
        &= \int_{\partial B(x_0,\varepsilon)}
            \left(\frac{1}{2\pi} \frac{x-x_0}{\abs{x-x_0}^2} \cdot
                \frac{x-x_0}{\varepsilon} \right) v \diff S \\
        &= \int_{\partial B(x_0,\varepsilon)}
            \frac{1}{2\pi\varepsilon} \frac{\abs{x-x_0}^2}{\abs{x-x_0}^2} v \diff S \\
        &= \frac{1}{2\pi\varepsilon} \int_{\partial B(x_0,\varepsilon)}
            v \diff S \\
        &\xrightarrow[]{\varepsilon \to 0} v(x_0).
    \end{align*}
    The limit follows from Theorem~\ref{thm:lebesgue_differentiation_theorem}.
    Thus, bringing $\varepsilon$ to zero in 
    \eqref{eq:greens_function_weak_diff_intmed4}, we get
    \begin{equation*}
        \int_{\Omega} \nabla \Phi_{x_0} \cdot \nabla v \diff x
        = v(x_0) + \sum_{j \in N} \int_{\Gamma_j}
            \frac{\partial \Phi_{x_0}}{\partial n} v \diff S,
    \end{equation*}
    which is what we wanted to show.
\end{proof}

The Green's function is commonly used as an auxiliary
function to find a general classical solution to Poisson's equation.
In the same spirit, we consider next how it can be used to prove
a result similar to Theorem~\ref{thm:greens_function_is_solution} but
with general boundary data.

\subsubsection{Existence and Uniqueness of Solutions with General Boundary Data}
\label{subsubsec:existence_and_uniqueness_of_solutions_with_arbitrary_data}

Theorem~\ref{thm:greens_function_is_solution} and the solvability of
the classical weak formulation of Poisson's equation imply that the problem
\eqref{eq:poissons_eq_with_dirac_delta_recap} has a weak solution.
\begin{theorem}
    \label{thm:dirac_load_is_solvable}
    Let $\Omega \subset \mathbb{R}^2$ be a bounded polygonal domain,
    and let $x_0 \in \Omega$.
    Let $g_j \in T(H^2(\Omega))$ for all $j \in D$ and $g_j \in T(H^1(\Omega))$
    for all $j \in N$. Assume that there exists a function $g_D \in H^2(\Omega)$
    such that $g_D|_{\Gamma_j} = g_j$ for all $j \in D$, and if $D = \varnothing$,
    assume that $g_j$ satisfy the compatibility condition.
    Let $1 < s < 2$. Then the boundary value problem
    \begin{equation}
        \label{eq:dirac_load_is_solvable_problem}
        \left\{
            \begin{aligned}
                -\Delta u &= \delta_{x_0} && \text{in } \Omega \\
                u &= g_j && \text{on } \Gamma_j, \quad j \in D \\
                \frac{\partial u}{\partial n} &= g_j && \text{on } \Gamma_j,
                \quad j \in N
            \end{aligned}
        \right.
    \end{equation}
    has a weak solution $u \in W^{1,s}(\Omega)$.
\end{theorem}
\begin{proof}
    Define $\Phi_{x_0}$ like before by \eqref{eq:greens_function}.
    The strategy is to consider the problem
    \begin{equation}
        \label{eq:dirac_load_is_solvable_aux_problem}
        \left\{
            \begin{aligned}
                -\Delta w &= 0 && \text{in } \Omega \\
                w &= g_j - \Phi_{x_0} && \text{on } \Gamma_j, \quad j \in D \\
                \frac{\partial w}{\partial n} &=
                    g_j - \frac{\partial \Phi_{x_0}}{\partial n}
                        && \text{on } \Gamma_j, \quad j \in N
            \end{aligned}
        \right.
    \end{equation}
    and use Theorem~\ref{thm:weak_poisson_is_solvable} and
    Theorem~\ref{thm:greens_function_is_solution}.
    
    Assume that Theorem~\ref{thm:weak_poisson_is_solvable} implies the
    existence of a weak solution $w \in H^1(\Omega)$ to the problem
    \eqref{eq:dirac_load_is_solvable_aux_problem}.
    Then $u = \Phi_{x_0} + w$ is a weak solution to the problem
    \eqref{eq:dirac_load_is_solvable_problem} as follows.
    First, $u \in W^{1,s}(\Omega)$ because $\Phi_{x_0} \in W^{1,s}(\Omega)$
    and the imbedding $H^1(\Omega) \subset W^{1,s}(\Omega)$ implies that $w \in W^{1,s}(\Omega)$.
    Second, it clearly holds that $u|_{\Gamma_j} = g_j$ for all $j \in D$.
    Finally, let $v \in W^{1,s'}(\Omega)$ be such that $v|_{\Gamma_j} = 0$
    for all $j \in D$ and $s' \in (2, \infty)$ is the conjugate exponent of $s$.
    By the definition of a weak solution, we then have
    \begin{align*}
        \int_{\Omega} \nabla u \cdot \nabla v \diff x
        &= \int_{\Omega} \nabla \Phi_{x_0} \cdot \nabla v \diff x 
            + \int_{\Omega} \nabla w \cdot \nabla v \diff x \\
        &= v(x_0)
            + \sum_{j \in N} \int_{\Gamma_j}
                \frac{\partial \Phi_{x_0}}{\partial n} v \diff S
            + \sum_{j \in N} \int_{\Gamma_j}
                \left( g_j - \frac{\partial \Phi_{x_0}}{\partial n} \right) v \diff S \\
        &= v(x_0) + \sum_{j \in N} \int_{\Gamma_j} g_j v \diff S.
    \end{align*}

    It remains to show that Theorem~\ref{thm:weak_poisson_is_solvable}
    is applicable to the problem \eqref{eq:dirac_load_is_solvable_aux_problem}.
    It is not clear whether the Dirichlet and Neumann boundary functions
    belong to the spaces $T(H^2(\Omega))$ and $T(H^1(\Omega))$, respectively.
    If $\Phi_{x_0} \in H^2(\Omega)$ were true, then this would obviously be true,
    but it is easy to show that $\Phi_{x_0}$ does not even belong to the space $H^1(\Omega)$
    because its gradient does not satisfy the integrability condition.
    
    The only issue is the singularity of $\Phi_{x_0}$ at the point $x_0$. 
    The function $\Phi_{x_0}$ is smooth everywhere else including the boundary.
    Since $\Phi_{x_0}$ is radial around the point $x_0$,
    let us remove the singularity by truncating 
    $\Phi_{x_0}$ over a ball $B(x_0,\varepsilon) \subset \Omega$ and denote the truncated
    function by $\Phi_{x_0,\varepsilon}$ which is defined by
    \begin{equation*}
        \Phi_{x_0,\varepsilon}(x)
        = (1 - \boldone_{B(x_0,\varepsilon)}(x)) \Phi_{x_0}(x)
            + \boldone_{B(x_0,\varepsilon)}(x) \Phi_{x_0}(x_0 + \varepsilon r)
    \end{equation*}
    for all $x \in \overline{\Omega}$ and
    where $r \in \mathbb{R}^2$ is an arbitrary fixed unit vector. Clearly,
    $\Phi_{x_0,\varepsilon}|_{\partial \Omega} = \Phi_{x_0}|_{\partial \Omega}$.
    
    Let us now smooth out $\Phi_{x_0,\varepsilon}$ over the edge
    $\partial B(x_0, \varepsilon)$.
    Assume that $\varepsilon$ is chosen so that the distance between the ball
    $B(x_0,\varepsilon)$ and the boundary $\partial \Omega$
    is at least $\varepsilon$. By a standard mollification argument,
    there exists a function $\eta \in C_0^{\infty}(\mathbb{R})$ such that
    $\eta(t) = 1$ whenever $\abs{t} < \varepsilon/4$
    and $\eta(t) = 0$ whenever $\abs{t} > \varepsilon/2$.
    Consider then the function
    \begin{equation*}
        g_{\Phi}(x)
        = (1 - \eta(\abs{x-x_0}-\varepsilon)) \Phi_{x_0,\varepsilon}(x)
            + \eta(\abs{x-x_0}-\varepsilon) \Phi_{x_0,\varepsilon}(x_0)
    \end{equation*}
    for all $x \in \overline{\Omega}$.
    The first term vanishes near the edge $\partial B(x_0, \varepsilon)$
    so that only the smooth second term remains.
    Moreover, $g_{\Phi}$ is constant over the ball $B(x_0, \varepsilon / 2)$,
    which implies that $g_{\Phi}$ is also smooth at the point $x_0$.
    In particular, $g_{\Phi} \in C^{\infty}(\overline{\Omega}) \subset H^2(\Omega)$.

    The fact that $g_{\Phi} = \Phi_{x_0}$ near and on the boundary
    implies that $\Phi_{x_0}|_{\partial \Omega} \in T(H^2(\Omega))$ and
    $\partial \Phi_{x_0} / \partial n \in T(H^1(\Omega))$ on
    each boundary segment $\Gamma_j$ for $j \in N$. Moreover,
    \begin{equation*}
        T(g_D - g_{\Phi})
        = Tg_D - Tg_{\Phi}
        = g_j - \Phi_{x_0}
    \end{equation*}
    for all $j \in D$ and $g_D - g_{\Phi} \in H^2(\Omega)$.
    We may thus apply Theorem~\ref{thm:weak_poisson_is_solvable}
    to the problem \eqref{eq:dirac_load_is_solvable_aux_problem},
    which concludes the proof.
\end{proof}

Theorem~\ref{thm:dirac_load_is_solvable} does not say anything about
the uniqueness of the weak solutions. When the domain $\Omega$ satisfies
Assumption~\ref{ass:regular_polygonal_domain}, uniqueness follows as well.
\begin{theorem}
    \label{thm:uniqueness_of_dirac_solution}
    Let $\Omega \subset \mathbb{R}^2$ be a bounded polygonal domain
    that satisfies Assumption~\ref{ass:regular_polygonal_domain}.
    Let $u \in W^{1,s}(\Omega)$ be a weak solution to the problem
    \eqref{eq:dirac_load_is_solvable_problem}.
    When $D \neq \varnothing$, $u$ is unique.
    When $D = \varnothing$, $u$ is unique up to an additive constant.
\end{theorem}
\begin{proof}
    Let $v \in W^{1,s}(\Omega)$ be another weak solution to the problem
    \eqref{eq:dirac_load_is_solvable_problem}.
    Then $u - v \in W^{1,s}(\Omega)$ satisfies
    \begin{equation}
        \label{eq:uniqueness_of_dirac_solution_intmed1}
        \int_{\Omega} \nabla (u-v) \cdot \nabla w \diff x = 0
    \end{equation}
    for all $w \in W^{1,s'}(\Omega)$, where $s' \in (2, \infty)$ is the conjugate
    exponent of $s$. Moreover, it holds that $(u-v)|_{\Gamma_j} = 0$ for all $j \in D$.

    Consider then the boundary value problem
    \begin{equation}
        \label{eq:uniqueness_of_dirac_solution_aux_problem}
        \left\{
            \begin{aligned}
                -\Delta h &= u-v && \text{in } \Omega \\
                h &= 0 && \text{on } \Gamma_j, \quad j \in D \\
                \frac{\partial h}{\partial n} &= 0 && \text{on } \Gamma_j,
                \quad j \in N,
            \end{aligned}
        \right.
    \end{equation}
    where $u-v \in L^2(\Omega)$ by the Sobolev imbedding theorem.
    When $D = \varnothing$, the load term needs to satisfy the compatibility condition
    \begin{equation}
        \label{eq:uniqueness_of_dirac_solution_compatibility}
        \int_{\Omega} u - v \diff x = 0.
    \end{equation}
    The general case, for which \eqref{eq:uniqueness_of_dirac_solution_compatibility}
    does not necessarily hold, is considered at the end.

    By Theorem~\ref{thm:weak_poisson_is_solvable}
    and Theorem~\ref{thm:H2_regularity}, there exists a weak solution
    $h \in H^2(\Omega)$ to the problem 
    \eqref{eq:uniqueness_of_dirac_solution_aux_problem}.
    Moreover, by Theorem~\ref{thm:weak_solution_is_strong_solution},
    it holds that $-\Delta h = u-v$
    and $\partial h / \partial n = 0$ on each $\Gamma_j$ for $j \in N$.
    Now by these facts and Green's formula, we get
    \begin{align*}
        \norm{u-v}_{L^2(\Omega)}^2
        &= \int_{\Omega} (u-v)^2 \diff x \\
        &= - \int_{\Omega} (u-v) \Delta h  \diff x \\
        &= \int_{\Omega} \nabla (u-v) \cdot \nabla h \diff x
            - \sum_{j \in D} \int_{\Gamma_j}
                \frac{\partial h}{\partial n} (u-v) \diff S
            - \sum_{j \in N} \int_{\Gamma_j}
                \frac{\partial h}{\partial n} (u-v) \diff S \\
        &= 0.
    \end{align*}
    Note that the first term is zero because the Sobolev imbedding theorem
    implies that $H^2(\Omega) \subset W^{1,s'}(\Omega)$ and we may thus
    apply the identity \eqref{eq:uniqueness_of_dirac_solution_intmed1}.
    The integrals over the Dirichlet boundary segments vanish because
    $(u-v)|_{\Gamma_j} = 0$ for all $j \in D$.
    
    The result $\norm{u-v}_{L^2(\Omega)}^2 = 0$ implies that $u = v$
    almost everywhere in $\Omega$. This concludes the proof for the
    case $D \neq \varnothing$. This also concludes the proof for the case
    $D = \varnothing$ with the condition 
    \eqref{eq:uniqueness_of_dirac_solution_compatibility}, i.e.\
    $u-v \in V_N$. If the condition 
    \eqref{eq:uniqueness_of_dirac_solution_compatibility} does not hold,
    then as usual we consider the normalized function
    $u-v-(\overline{u} - \overline{v}) \in V_N$, where
    \begin{equation*}
        \overline{u} = \frac{1}{\abs{\Omega}} \int_{\Omega} u \diff x.
    \end{equation*}
    Using the normalized function as the load term for the problem
    \eqref{eq:uniqueness_of_dirac_solution_aux_problem} then
    implies that $u-v-(\overline{u} - \overline{v}) = 0$,
    i.e.\ $u$ and $v$ differ by the constant $\overline{u} - \overline{v}$.
    This concludes the proof.
\end{proof}

An easy corollary to Theorem~\ref{thm:uniqueness_of_dirac_solution}
is that the solution is the same for all $s \in (1,2)$.
We skip the proof, however.

\clearpage

\section{The Finite Element Method}
\label{sec:finite_element_method}

This section introduces the finite element method as a general procedure
for obtaining approximate solutions to weakly formulated elliptic boundary value problems.
After the introduction, we consider the error of the approximations
for the $p$-version of the finite element method.
This is done through the approximation properties of high-order polynomials
in one and two dimensions.

\subsection{Definition of a Finite Element Solution}
\label{subsec:basic_properties_of_finite_element_solutions}

Consider the task of finding a solution to the weakly formulated
boundary value problem
\begin{equation}
    \label{eq:fem_departure}
    \text{Find } u \in U \text{ s.t.\ }
    a(u,v) = \varphi(v) \text{ for all } v \in V,
\end{equation}
where $U$ and $V$ are Sobolev subspaces, the mapping $a: U \times V \to \mathbb{R}$
is bilinear and $\varphi \in V'$.
As an example, consider the classical weak formulation of Poisson's
equation with homogeneous Dirichlet boundary conditions for which
\begin{equation*}
    U = V
    = \left\{ v \in H^1(\Omega)
        : v|_{\Gamma_j} = 0 \text{ for all } j \in D \right\}
\end{equation*}
or, if $D = \varnothing$,
\begin{equation*}
    U = V
    = \left\{ v \in H^1(\Omega) : \int_{\Omega} v \diff x = 0 \right\}.
\end{equation*}
With a Dirac delta load, the spaces $U$ and $V$ are otherwise
defined identically but $U \subset W^{1,s}(\Omega)$
and $V \subset W^{1,s'}(\Omega)$, where
$s \in (1, 2)$ and $s' \in (2, \infty)$ are conjugate exponents.

Assume that the problem \eqref{eq:fem_departure} has a unique solution.
Based on the previous section,
the validity of this assumption typically relies on the applicability
of the Lax-Milgram theorem. The Lax-Milgram theorem does not, however,
provide a method for constructing the solution for computational purposes.
In fact, it is rare to find an explicit formula for the solution,
but we may still try to approximate it.

Let $S_U$ and $S_V$ be finite-dimensional subspaces of $U$ and $V$, and
assume that $\dim S_U = \dim S_V$.
Denote the dimension of the subspaces by $m \in \mathbb{N}$,
and let $\{ u_i \}_{i=1}^{m}$ and $\{ v_i \}_{i=1}^{m}$ be bases for
$S_U$ and $S_V$, respectively. Consider then the discretized problem
\begin{equation}
    \label{eq:fem_discretized}
    \text{Find } u_S \in S_U \text{ s.t.\ }
    a(u_S,v) = \varphi(v) \text{ for all } v \in S_V.
\end{equation}
%A solution to the problem \eqref{eq:fem_discretized} essentially
%defines the idea of a finite element solution to \eqref{eq:fem_departure}
%which is to search for an approximate solution in a suitable finite-dimensional
%subspace instead of considering the whole space which is virtually always
%infinite-dimensional. One of the most emblematic features of
%the finite element method is the choice of the subspaces $S_U$ and $S_V$.
%Before considering that in more detail, let us first consider when the general
%discretized problem \eqref{eq:fem_discretized} actually has a solution.
%This will also steer us towards the construction of finite element solutions
%in practice.
Note that a possible solution can be written as
\begin{equation}
    \label{eq:uS_wrt_basis}
    u_S = \sum_{i=1}^{m} b_i u_i
\end{equation}
for some coefficient vector $b = (b_1,\dotsc,b_m) \in \mathbb{R}^m$.
It turns out that the problem \eqref{eq:fem_discretized} is equivalent
to solving a linear system of equations.
\begin{theorem}
    \label{thm:discretized_problem_solution_equivalence}
    Let $S_U \subset U$ and $S_V \subset V$ as above with the bases
    $\{ u_i \}_{i=1}^{m}$ and $\{ v_i \}_{i=1}^{m}$,
    and consider the discretized problem \eqref{eq:fem_discretized}.
    Define a matrix $K \in \mathbb{R}^{m \times m}$ such that
    $K_{ij} = a(u_j,v_i)$, and define a vector $r \in \mathbb{R}^m$
    such that $r_i = \varphi(v_i)$. Then $u_S$ is a solution
    to the discretized problem
    if and only if the coefficient vector $b \in \mathbb{R}^m$ of $u_S$ solves the system
    of equations $Kb = r$.
\end{theorem}
\begin{proof}
    Assume first that $u_S$ is a solution to \eqref{eq:fem_discretized}.
    Let $v \in S_V$ which can be written as
    \begin{equation}
        \label{eq:v_wrt_basis}
        v = \sum_{i=1}^{m} c_i v_i
    \end{equation}
    for some $c = (c_1,\dotsc,c_m) \in \mathbb{R}^m$.
    Substituting \eqref{eq:uS_wrt_basis} and \eqref{eq:v_wrt_basis} into
    \eqref{eq:fem_discretized} and using the linearity of $a$ and $\varphi$
    gives
    \begin{equation}
        \label{eq:fem_discretized_intmed1}
        \sum_{i=1}^{m} c_i \sum_{j=1}^{m} b_j a(u_j,v_i)
        = \sum_{i=1}^{m} c_i \varphi(v_i).
    \end{equation}
    Note that \eqref{eq:fem_discretized_intmed1} can be written as
    \begin{equation}
        \label{eq:fem_discretized_intmed2}
        c^T K b = c^T r.
    \end{equation}
    The vector $c$ was chosen to be arbitrary, which means that by setting
    $c = Kb - r$ in \eqref{eq:fem_discretized_intmed2},
    we deduce that the coefficient vector $b$ must solve the system $Kb = r$.

    Assume then that $b$ solves the system $Kb = r$.
    Let $v \in S_V$ as in \eqref{eq:v_wrt_basis}.
    Backtracking the steps above, we conclude that
    \begin{equation*}
        a(u_S,v) = \varphi(v),
    \end{equation*}
    which means that $u_S$ solves the problem \eqref{eq:fem_discretized}.
\end{proof}

For Poisson's equation, the matrix $K$ is called the stiffness matrix, and
the vector $r$ is called the load vector.
The assumption that $\dim S_U = \dim S_V = m$ is now useful
because $K$ is a square matrix in that case.
In particular, the discretized problem \eqref{eq:fem_discretized}
has a unique solution if and only if $K$ is non-singular.
It turns out that the matrix $K$ is non-singular
precisely when the mapping $a$ and the subspaces $S_U$ and $S_V$
satisfy a certain regularity condition, which is presented in the following theorem.
For reference, see also \cite{schwab1998}.
\begin{theorem}
    \label{thm:nonsingularity_of_K}
    The matrix $K$ in Theorem~\ref{thm:discretized_problem_solution_equivalence}
    is non-singular if and only if for every
    $0 \neq u \in S_U$ there exists a $v \in S_V$ such that $a(u,v) \neq 0$.
\end{theorem}
\begin{proof}
    Assume first that $K$ is non-singular.
    Let $0 \neq u \in S_U$ and write
    \begin{equation}
        \label{eq:nonsingularity_of_K_u}
        u = \sum_{i=1}^{m} b_i u_i
    \end{equation}
    for some $b = (b_1,\dotsc,b_m) \in \mathbb{R}^m$.
    Since $u \neq 0$, also $b \neq 0$.
    Because $K$ is non-singular, $Kb \neq 0$.
    Let $v \in S_V$ be such that
    \begin{equation*}
        %\label{eq:nonsingularity_of_K_v}
        v = \sum_{i=1}^{m} c_i v_i,
    \end{equation*}
    where $c = Kb \in \mathbb{R}^m$. Now
    \begin{equation*}
        a(u,v) = c^T Kb = (Kb)^T Kb = \norm{Kb}^2 > 0.
    \end{equation*}
    That is, $a(u,v) \neq 0$.

    Assume then the other direction that for every
    $0 \neq u \in S_U$ there exists a $v \in S_V$ such that $a(u,v) \neq 0$.
    Aiming for a contradiction, assume that $K$ is singular.
    The singularity of $K$ implies that
    there exists a $0 \neq b \in \mathbb{R}^m$ such that $Kb = 0$.
    Defining $0 \neq u \in S_U$ as in \eqref{eq:nonsingularity_of_K_u},
    we now get that
    \begin{equation*}
        a(u,v) = c^T Kb = 0
    \end{equation*}
    for all $v \in S_V$.
    This contradicts the initial assumption, which means that $K$
    must be non-singular.
\end{proof}

Summarizing the above discussion,
the idea is that the solution of the discretized problem \eqref{eq:fem_discretized}
approximates the solution of the initial problem \eqref{eq:fem_departure},
and the solution of the discretized problem can be computed
by solving a linear system of equations that is guaranteed to have a unique solution
as long as the subspaces $S_U$ and $S_V$ are chosen suitably according to
Theorem~\ref{thm:nonsingularity_of_K}.
We generally assume that the mapping $a$ corresponds to an elliptic second-order
differential operator which is typically formulated in the Hilbert space $H^1(\Omega)$.
In this case, by setting $S_U = S_V = S \subset H^1(\Omega)$,
the ellipticity of $a$ implies that $a(u,u) > 0$ for all $0 \neq u \in S$,
which then implies via Theorem~\ref{thm:nonsingularity_of_K} that
the discretized problem has a unique solution. Moreover, if the mapping $a$
is also symmetric, as it typically is, the matrix $K$ is symmetric and
positive definite, which means that the system $Kb = r$ can be solved efficiently
on a computer by using e.g.\ the Cholesky decomposition.

The subspace $S$ should naturally be chosen so that
the matrix $K$ and the vector $r$ can be computed in practice.
Recall from the previous section that the entries of $K$ and $r$ typically
correspond to integrals over the domain $\Omega$.
In the finite element method, the subspace $S$ is chosen as the space of
certain piecewise polynomials of a given degree.
Polynomials are particularly suitable for integration and differentiation.
For a polygonal domain $\Omega \subset \mathbb{R}^2$,
the finite element space $S$ is more specifically obtained by modeling $\Omega$
as a mesh of triangles, quadrilaterals or both, which are called elements,
and then defining the functions in $S$ to be continuous over the domain $\Omega$ so that
the piecewise parts correspond to the elements of the mesh.

We denote a mesh on the domain $\Omega$ by $\mathcal{M}$
which is a finite set of triangles or quadrilaterals
whose closures' union is exactly the closure of $\Omega$.
That is, $\mathcal{M} = \{ E_i \}_{i=1}^{N}$, where
$N$ is the total number of elements and
$E_i$ is a non-degenerate triangle or quadrilateral for all $i=1,2,\dotsc,N$, and
\begin{equation*}
    \overline{\Omega} = \bigcup_{i=1}^{N} \overline{E_i}.
\end{equation*}
For simplicity, it is also assumed that the quadrilaterals are convex and the intersection
$\overline{E_i} \cap \overline{E_j}$ for all $i \neq j$
either is empty, consists of a common vertex or consists of an entire common side.
Figure~\ref{fig:example_mesh} contains an example of such a mesh.
\begin{figure}[t]
    \centering
    \begin{tikzpicture}
        %\draw[help lines] (0,0) grid (9,6);
        \path[draw] (3,0) -- (3,2) -- (2,1.73) -- (3,0);
        \path[draw] (3,2) -- (3,4) -- (1,3.46) -- (2,1.73);
        \path[draw] (3,4) -- (3,6) -- (0,5.19) -- (1,3.46);
        \path[draw] (3,0) -- (4,1.73) -- (3,2);
        \path[draw] (4,1.73) -- (5,3.46) -- (3,4);
        \path[draw] (5,3.46) -- (6,5.19) -- (3,6);
        \path[draw] (3,0) -- (4.73,1) -- (4,1.73);
        \path[draw] (4.73,1) -- (6.46,2) -- (5,3.46);
        \path[draw] (6.46,2) -- (8.19,3) -- (6,5.19);
        \path[draw] (3,0) -- (5,0) -- (4.73,1);
        \path[draw] (5,0) -- (7,0) -- (6.46,2);
        \path[draw] (7,0) -- (9,0) -- (8.19,3);
    \end{tikzpicture}
    \caption{A finite element mesh.}
    \label{fig:example_mesh}
\end{figure}

Denote arbitrary triangular and quadrilateral domains by $T$ and $Q$,
respectively. Define also a reference triangle by
\begin{equation*}
    \widehat{T} = \{ (x_1,x_2) \in \mathbb{R}^2 : 
        0 < x_1 < 1,\text{ } 0 < x_2 < 1 - x_1 \}
\end{equation*}
and a reference quadrilateral by $\widehat{Q} = (-1,1) \times (-1,1)$,
see Figure~\ref{fig:reference_elements}.
The symbol $\widehat{E}$ is used to mean the corresponding reference element
of the element $E$.

For each $i=1,2,\dotsc,N$, let $F_i: \widehat{E_i} \to E_i$ be a bijective mapping
between the element $E_i$ and its corresponding reference element $\widehat{E_i}$.
The finite element space $S$ is now defined by
\begin{align}
    S
    &= S(\Omega, \mathcal{M}, p) \nonumber \\
    \label{eq:fem_space}
    &= \{
        u \in C(\overline{\Omega})
            : u \circ F_i \in \mathcal{P}_p(\widehat{E_i}),
                \text{ } i=1,2,\dotsc,N(\mathcal{M}),
                    \text{ } Bu = 0
    \},
\end{align}
where $\mathcal{P}_p(\widehat{E})$ is the space of bivariate polynomials of degree
$p \in \mathbb{N}$ and $Bu = 0$ corresponds to the homogeneous Dirichlet boundary
conditions or, in the case of a pure Neumann problem, to the zero mean value requirement
in order to fix the solution.
The degree $p$ of the polynomials can be interpreted as
the maximal sum of the powers of each monomial,
so that for a monomial $x^i y^j$ it holds that $i+j \leq p$,
or as the degree of each independent variable, i.e.\ $i \leq p$ and $j \leq p$.
These two polynomial spaces are called the trunk space and the product space, respectively,
although their definitions may slightly differ depending on whether the element is
a triangle or a quadrilateral \cite{szabobabuska2011}.
We consider these polynomial spaces for both triangles and quadrilaterals later.

The bijective element mapping $F_i: \widehat{E_i} \to E_i$ is commonly defined
as the linear combination of the vertices of the element $E_i$,
which is the definition we will always use as well.
When the element $E_i$ is a triangle or a parallelogram, such a mapping is affine.
It is easy to show that an affine mapping preserves polynomials, which means that
when the mesh consists of triangles or parallelograms, a function $u \in S$
is truly a piecewise polynomial. However, a similarly defined mapping for
a quadrilateral that is not a parallelogram does not preserve the polynomials.
For a general quadrilateral, the convexity assumption is also necessary for the bijectivity
\cite{strang1973}.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            %\draw[help lines] (0,0) grid (6,6);
            
            \draw (1,1) -- (5,1);
            \node at (1,0.6) {$(0,0)$};
            \filldraw[black] (1,1) circle (1.5pt);
            
            \draw (5,1) -- (1,5);
            \node at (5,0.6) {$(1,0)$};
            \filldraw[black] (5,1) circle (1.5pt);
            
            \draw (1,1) -- (1,5);
            \node at (1,5.35) {$(0,1)$};
            \filldraw[black] (1,5) circle (1.5pt);
        \end{tikzpicture}
        \caption{The reference triangle $\widehat{T}$.}
        \label{fig:reference_elements_tri}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            %\draw[help lines] (0,0) grid (6,6);

            \draw (1,1) -- (5,1);
            \node at (1,0.6) {$(-1,-1)$};
            \filldraw[black] (1,1) circle (1.5pt);
            
            \draw (5,1) -- (5,5);
            \node at (5,0.6) {$(1,-1)$};
            \filldraw[black] (5,1) circle (1.5pt);

            \draw (5,5) -- (1,5);
            \node at (5,5.35) {$(1,1)$};
            \filldraw[black] (5,5) circle (1.5pt);
            
            \draw (1,5) -- (1,1);
            \node at (1,5.35) {$(-1,1)$};
            \filldraw[black] (1,5) circle (1.5pt);
        \end{tikzpicture}
        \caption{The reference quadrilateral $\widehat{Q}$.}
        \label{fig:reference_elements_quad}
    \end{subfigure}
    \caption{The reference elements.}
    \label{fig:reference_elements}
\end{figure}

Other choices are also possible for the element mappings \cite{szabobabuska2011}.
A reasonable requirement for the element mappings is that
if $u \in W^{k,s}(E_i)$, then $u \circ F_i \in W^{k,s}(\widehat{E_i})$, and
if $\hat{u} \in W^{k,s}(\widehat{E_i})$, then $\hat{u} \circ F_{i}^{-1} \in W^{k,s}(E_i)$
for all $k \in \mathbb{N}$ and $s \in [1, \infty)$.
This is true for the mappings that linearly combine
the vertices of each element \cite[Theorem 1 on p.~13]{mazya2011}.
With this requirement, it is easy to show that the finite element space $S$ is
a subspace of $W^{1,s}(\Omega)$ for all $s \in [1, \infty)$ since
polynomials on the reference elements obviously belong to the Sobolev spaces
$W^{1,s}(\widehat{E_i})$ and the functions in $S$ are continuous.
We skip the proof of this result, but see \cite[Theorem~5.2 on p.~62]{braess2007} for reference.

Using the weak formulation of Poisson's equation as an example,
the computation of the entries of the stiffness matrix $K$ can be reduced to element-wise
computations:
\begin{align}
    K_{ij}
    &= a(u_j, u_i) \nonumber \\
    &= \int_{\Omega} \nabla u_j \cdot \nabla u_i \diff x \nonumber \\
    &= \sum_{k=1}^{N} \int_{E_k} \nabla u_j \cdot \nabla u_i \diff x \nonumber \\
    &= \sum_{k=1}^{N} \int_{\widehat{E}_k}
        \nabla u_j (F_k(\hat{x})) \cdot \nabla u_i (F_k(\hat{x})) \abs{\det J_{F_k} (\hat{x})}
            \diff \hat{x} \nonumber \\
    \label{eq:stiffness_matrix_entry_elementwise}
    &= \sum_{k=1}^{N} \int_{\widehat{E}_k}
            \hat{\nabla} \hat{u}_j(\hat{x})^T
            J_{F_k}^{-1}(\hat{x}) J_{F_k}^{-T}(\hat{x})
            \hat{\nabla} \hat{u}_i(\hat{x})
            \abs{\det J_{F_k} (\hat{x})} \diff \hat{x},
\end{align}
where we applied a change of variables to the reference elements via the element mappings.
Note that $\hat{\nabla}$ is the gradient with respect to the variable $\hat{x}$.
Note also that $\hat{u}_i = u_i \circ F_k \in \mathcal{P}_p(\widehat{E}_k)$.
The identity \eqref{eq:stiffness_matrix_entry_elementwise} implies that all computations
can be done in the reference elements regardless of the domain $\Omega$.
The same also applies to the assembly of the load vector.

When the element mappings are affine, i.e.\ $F_k(\hat{x}) = A_k \hat{x} + b_k$
for some matrix $A_k \in \mathbb{R}^{2 \times 2}$ and
some translation vector $b_k \in \mathbb{R}^2$,
then the Jacobian matrix $J_{F_k}$ is equal to $A_k$ and 
\eqref{eq:stiffness_matrix_entry_elementwise} can be written as
\begin{equation}
    \label{eq:stiffness_matrix_entry_elementwise_affine}
    K_{ij} = \sum_{k=1}^{N} \int_{\widehat{E}_k}
        \hat{\nabla} \hat{u}_j(\hat{x})^T A_k^{-1} A_k^{-T} \hat{\nabla} \hat{u}_i(\hat{x})
            \abs{\det A_k} \diff \hat{x}.
\end{equation}
Note that the integrands in \eqref{eq:stiffness_matrix_entry_elementwise_affine}
are polynomials so the integrals can be evaluated exactly.
In practice, numerical integration is used, and considering that the integrals consist of
polynomials, a Gauss-Legendre quadrature rule is an apt choice, see for example
\cite{Hussain2012AppropriateGQ} and \cite{islam2009}.

The difference between the exact solution $u \in U$
and the finite element solution $u_S \in S$ is often measured
in a suitable $W^{1,s}(\Omega)$-norm, typically the $H^1(\Omega)$-norm,
or in a suitable $L^s(\Omega)$-norm, typically the $L^2(\Omega)$-norm or
the $L^{\infty}(\Omega)$-norm.
There are three commonly used strategies for controlling the error
of the approximate solutions: making the mesh more refined,
increasing the degree $p$ of the polynomials
or using a combination of both of these strategies.
In the FEM nomenclature,
these three strategies are called the $h$-, $p$- and $hp$-version
of the finite element method, respectively.

The parameter $h$ is usually defined by
\begin{equation*}
    h = \max_{E \in \mathcal{M}} h_E,
\end{equation*}
where $h_E$ is the diameter of the smallest sphere that contains the element $E$.
By making the mesh more refined, the largest diameter $h$
becomes smaller and the quality of the finite element solution should hopefully improve.
Convergence analysis of the $h$-version attempts to
describe how the error behaves as $h \to 0$.
To make this analysis feasible, the refinements of the mesh are typically
assumed to satisfy some regularity condition. For example, letting
$\rho_E$ denote the diameter of the largest sphere contained inside the element $E$,
a collection of meshes $\{ \mathcal{M}_h \}_{h > 0}$
is said to be shape-regular if there exists
a constant $\tau > 0$ independent of $h$ such that
\begin{equation*}
    \frac{h_E}{\rho_E} \leq \tau
\end{equation*}
for every element $E \in \mathcal{M}_h$
and for every mesh $\mathcal{M}_h \in \{ \mathcal{M}_h \}_{h > 0}$.
It is common to use polynomials that have a low degree for the $h$-version,
e.g.\ $p=1$ or $p=2$.
The $h$-version is the classical version of the finite element method,
and it is the main topic of several standard text books,
see e.g.\ \cite{ciarlet2002}, \cite{braess2007} and \cite{scottbrenner2007}.

In the $p$-version, the quality of the approximation is improved
by increasing the degree $p$ of the polynomials while the mesh is kept fixed.
In the $hp$-version, the mesh is refined and the degree of the polynomials is increased
at the same time, which can boost the rate of convergence.
The mathematical foundations of the $p$-version and the $hp$-version
were developed after the $h$-version.
For references on the $p$-version, see e.g.\
\cite{babuskaszabokatz1981}, \cite{dorr1984}, \cite{babuskasuri1987} and
\cite{szabo2004}.
For references on the $hp$-version, see e.g.\
\cite{babuskadorr1981}, \cite{guo1986}, \cite{babuskasuri1987hp} and
\cite{babuskasuri1994}.
For a text book on both the $p$-version and the $hp$-version, see \cite{schwab1998}.

There exist several extensions to the finite element method.
Instead of elliptic second-order boundary value problems,
one could consider other types of problems.
Instead of two-dimensional polygonal domains,
one could consider, for example, three-dimensional curved domains with curvilinear elements.
Instead of using the same polynomial degree $p$ in every element, one could choose them
on an element-by-element basis if the approximation needs to be more accurate
in some specific subdomain only. Some of these generalizations
are discussed in \cite{szabobabuska2011},
but we shall not consider them any further.
The definition of a finite element solution remains more or less the same, nevertheless.

\subsection{Basis Functions}
\label{subsec:fem_basis_functions}

To compute the matrix $K$ and the vector $r$,
a basis is needed for the finite element space $S$.
The basis functions can be divided into
nodal basis functions, side basis functions and internal basis functions.
A nodal basis function is associated with a vertex of the mesh
so that the value of the function is one at the given vertex and
the function is supported on the elements sharing the given vertex.
There exists one nodal basis function for each vertex.
A side basis function is associated with a side of the mesh
so that the function is typically a polynomial on the given side and
the function is supported on the elements sharing the given side.
For each side, there exist $p-1$ side basis functions.
An internal basis function is supported on a given element, and
the number of internal basis functions depends on the degree $p$ and
whether the polynomial space in the reference elements is the product space or the trunk space.
Figure~\ref{fig:basis_support} illustrates the supports of the different types of basis functions.
A notable implication of such a basis is that the matrix $K$ becomes relatively sparse.

\begin{figure}[b]
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \begin{tikzpicture}
            \fill[gray!40] (2,1) rectangle (4,3);
            \draw (0,0) grid (4,4);
        \end{tikzpicture}
        \caption{Nodal basis function.}
        \label{fig:basis_support_nodal}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \begin{tikzpicture}
            \fill[gray!40] (1,1) rectangle (3,2);
            \draw (0,0) grid (4,4);
        \end{tikzpicture}
        \caption{Side basis function.}
        \label{fig:basis_support_side}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \begin{tikzpicture}
            \fill[gray!40] (2,2) rectangle (3,3);
            \draw (0,0) grid (4,4);
        \end{tikzpicture}
        \caption{Internal basis function.}
        \label{fig:basis_support_internal}
    \end{subfigure}
    \caption{The supports of the three types of basis functions.}
    \label{fig:basis_support}
\end{figure}

By the definition of the finite element space,
the restriction of a basis function on a given element is a polynomial
after the change of variables to the corresponding reference element.
Thus, the basis functions can be defined by defining a polynomial basis on the reference elements.
The polynomials in the reference element basis are called shape functions,
and they are also divided into nodal, side and internal shape functions
according to the corresponding basis function.
There exist several options for how to construct the shape functions.
We present here hierarchic shape functions that can be found in \cite{szabobabuska2011}.
Hierarchic means that the set of shape functions for any degree $p$
is a subset of the set of shape functions for the degree $p+1$.
The basis for the finite element space is then hierarchic as well.

We consider the shape functions for both quadrilaterals and triangles
and for both the product space and the trunk space.
The only difference between the product and trunk spaces is the number of internal shape functions.

\subsubsection{Hierarchic Shape Functions for Quadrilaterals}
\label{subsubsec:quad_shape_functions}

The nodal shape functions are given by
\begin{align*}
    &N_1(\hat{x}_1,\hat{x}_2) = \frac{1}{4}(1 - \hat{x}_1)(1 - \hat{x}_2) \\[0.5em]
    &N_2(\hat{x}_1,\hat{x}_2) = \frac{1}{4}(1 + \hat{x}_1)(1 - \hat{x}_2) \\[0.5em]
    &N_3(\hat{x}_1,\hat{x}_2) = \frac{1}{4}(1 + \hat{x}_1)(1 + \hat{x}_2) \\[0.5em]
    &N_4(\hat{x}_1,\hat{x}_2) = \frac{1}{4}(1 - \hat{x}_1)(1 + \hat{x}_2).
\end{align*}

For the side and internal shape functions, define the auxiliary functions
\begin{equation}
    \label{eq:phi_shape}
    \phi_k(s) = (k-1) k \int_{-1}^{s} P_{k-1}(t) \diff t
\end{equation}
for $k=2,3,\dotsc,p$, where $P_{k-1}$ is the Legendre polynomial of degree $k-1$.
Note that $\phi_k$ is a polynomial of degree $k$.
The side shape functions are now given by
\begin{align*}
    &N_k^{(1)}(\hat{x}_1,\hat{x}_2) = \frac{1}{2} (1 - \hat{x}_2) \phi_k(\hat{x}_1) \\[0.5em]
    &N_k^{(2)}(\hat{x}_1,\hat{x}_2) = \frac{1}{2} (1 + \hat{x}_1) \phi_k(\hat{x}_2) \\[0.5em]
    &N_k^{(3)}(\hat{x}_1,\hat{x}_2) = \frac{1}{2} (1 + \hat{x}_2) \phi_k(-\hat{x}_1) \\[0.5em]
    &N_k^{(4)}(\hat{x}_1,\hat{x}_2) = \frac{1}{2} (1 - \hat{x}_1) \phi_k(-\hat{x}_2)
\end{align*}
for $k=2,3,\dotsc,p$. The argument of $\phi_k$ is negative for the sides 3 and 4
so that the orientation of the polynomials is the same on all the sides.
This way a side basis function can always be constructed by multiplying
one of the corresponding side shape functions by $-1$,
assuming that the element mappings preserve the orientation,
i.e.\ their Jacobian determinants are positive.

The internal shape functions for the product space are given by
\begin{equation}
    \label{eq:quad_internal_shape_function}
    N_p^{(k,l)}(\hat{x}_1,\hat{x}_2) = \phi_k(\hat{x}_1) \phi_l(\hat{x}_2)
\end{equation}
for $k=2,3,\dotsc,p$ and $l=2,3,\dotsc,p$.
The product space is denoted by $\mathcal{P}_p^{(pr)}(\widehat{Q})$.

The internal shape functions for the trunk space are also given by
\eqref{eq:quad_internal_shape_function}, but the indices $k=2,3,\dotsc,p$ and
$l=2,3,\dotsc,p$ satisfy $k + l \leq p$. This implies that there are no internal shape
functions until $p=4$. Note also that the trunk space for quadrilaterals
contains the monomials $\hat{x}_1^p \hat{x}_2^{\vphantom{p}}$ and
$\hat{x}_1^{\vphantom{p}} \hat{x}_2^p$ for all $p$.
The trunk space is denoted by $\mathcal{P}_p^{(tr)}(\widehat{Q})$.

\subsubsection{Hierarchic Shape Functions for Triangles}
\label{subsubsec:tri_shape_functions}

The nodal shape functions are given by
\begin{align*}
    &L_1(\hat{x}_1,\hat{x}_2) = 1 - \hat{x}_1 - \hat{x}_2 \\[0.5em]
    &L_2(\hat{x}_1,\hat{x}_2) = \hat{x}_1 \\[0.5em]
    &L_3(\hat{x}_1,\hat{x}_2) = \hat{x}_2.
\end{align*}

For the side shape functions, define the auxiliary functions
\begin{equation}
    \label{eq:tilde_phi_shape}
    \widetilde{\phi}_k(s) = 4 \frac{\phi_k(s)}{1-s^2} = 4 P_{k-1}'(s)
\end{equation}
for $k=2,3,\dotsc,p$, where $\phi_k$ is given by \eqref{eq:phi_shape} and
the second equality follows from the properties of the Legendre polynomials \cite{andrews1998}.
The side shape functions are now given by
\begin{align*}
    &N_k^{(1)}(\hat{x}_1,\hat{x}_2) = L_1 L_2 \widetilde{\phi}_k(L_2 - L_1) \\[0.5em]
    &N_k^{(2)}(\hat{x}_1,\hat{x}_2) = L_2 L_3 \widetilde{\phi}_k(L_3 - L_2) \\[0.5em]
    &N_k^{(3)}(\hat{x}_1,\hat{x}_2) = L_3 L_1 \widetilde{\phi}_k(L_1 - L_3)
\end{align*}
for $k=2,3,\dotsc,p$. The constant $4$ in \eqref{eq:tilde_phi_shape} is needed
to make the side shape functions for triangles compatible with
the side shape functions for quadrilaterals.
The orientation is already the same for both.

The internal shape functions for the trunk space are given by
\begin{equation}
    \label{eq:tri_internal_shape_function}
    N_p^{(k,l)}(\hat{x}_1,\hat{x}_2) = L_1 L_2 L_3 P_k(2\hat{x}_1 - 1) P_l(2\hat{x}_2 - 1)
\end{equation}
for $k=0,1,\dotsc,p-3$ and $l=0,1,\dotsc,p-3$ with $k+l \leq p-3$.
The trunk space is denoted by $\mathcal{P}_p^{(tr)}(\widehat{T})$.

The internal shape functions for the product space are also given by 
\eqref{eq:tri_internal_shape_function} but with the indices
$k=0,1,\dotsc,p-2$ and $l=0,1,\dotsc,p-2$.
However, note that the resulting space does not contain the monomial $\hat{x}_1^p \hat{x}_2^p$,
for example. We still refer to the space as the product space and denote it by
$\mathcal{P}_p^{(pr)}(\widehat{T})$.

Let us finally discuss how the basis functions are related to the boundary conditions
of the boundary value problems, which is relevant for both quadrilateral and triangular elements.
For homogeneous Dirichlet boundary conditions,
consider the nodal and side basis functions that determine the value of a function $u \in S$
on the corresponding boundary segments. To then satisfy the Dirichlet boundary conditions,
one only needs to set the coefficients of those basis functions to zero.
To achieve this in partice, it is easier to first assemble the system $Kb = r$
without considering any boundary conditions and then eliminate the rows and columns
that correspond to the aforementioned nodal and side basis functions,
which essentially zeroes the corresponding coefficients.

For a Neumann boundary value problem that has a unique solution,
the basis functions do not need any special handling
since the boundary conditions are embedded into the load vector instead of the solution space.
However, a pure Neumann problem for Poisson's equation does not have a unique solution,
and we enforced uniqueness by requiring that the solution has zero mean value over the domain.
The above basis does not satisfy the zero mean value requirement,
and it would be cumbersome to try to construct such a basis.
The system $Kb = r$ can still be assembled with the basis above,
but it does not have a unique solution because the stiffness matrix $K$ has a one-dimensional 
kernel. Fortunately, it is possible to fix the solution by fixing its value at one
of the vertices to zero.
This is equivalent to setting the coefficient of the corresponding nodal basis function
to zero in the coefficient vector $b$ and then eliminating the corresponding row and column
from the system $Kb = r$. The resulting system has a unique solution, and
the dropped equation holds as a consequence of the Neumann compatibility condition
\cite{braess2007}. Finally, the solution can be normalized to have zero mean value.

\subsection{Convergence of the \texorpdfstring{$p$}{p}-Version of the Finite Element Method}
\label{subsec:convergence_properties_of_the_p_version}

One of the main objectives of this thesis is to study the convergence of the $p$-version
of the finite element method when applied to Poisson's equation with a Dirac delta load.
As in Section~\ref{sec:poissons_equation_in_a_polygon}
regarding the solvability of the weak formulations, we carry out the convergence
analysis in two phases. In the first phase, we again consider the classical elliptic weak
formulation in the Hilbert space setting.
In the second phase, the convergence results of the first phase
are applied to the Dirac delta problem via a duality argument by Casas \cite{casas1985}.
The rest of this section is concerned with the first phase, and
the second phase is covered in the next section.

\subsubsection{Céa's Lemma}
\label{subsubsec:ceas_lemma}

We have seen that the finite element method is a rather complicated
process with a lot of varying factors such as the mesh and the degree $p$
of the polynomials. Trying to measure the error between the exact solution
and the finite element solution directly is unwieldy.
It, however, turns out that the error is directly comparable to
how well the finite element space $S$ approximates the ambient space
where the exact solution resides.
This result is known as Céa's lemma by Jean Céa \cite{cea1964}.
\begin{theorem}[Céa's lemma]
    \label{thm:ceas_lemma}
    Let $V$ be a Hilbert space,
    $a: V \times V \to \mathbb{R}$ an elliptic bounded bilinear mapping
    and $\varphi \in V'$.
    Let $u \in V$ be the unique vector that satisfies
    \begin{equation*}
        a(u,v) = \varphi(v)
    \end{equation*}
    for all $v \in V$.
    Let $S$ be a non-empty closed subspace of $V$,
    and let $u_S \in S$ be the unique vector that satisfies
    \begin{equation*}
        a(u_S,v) = \varphi(v)
    \end{equation*}
    for all $v \in S$.
    Then there exists a constant $C > 0$ dependent only on the bilinear mapping
    such that
    \begin{equation*}
        \norm{u - u_S}_V \leq C \inf_{v \in S} \norm{u - v}_V.
    \end{equation*}
\end{theorem}
\begin{proof}
    We begin by observing that for all $v \in S$ it holds that
    \begin{equation*}
        a(u-u_S,v) = a(u,v) - a(u_S,v) = \varphi(v) - \varphi(v) = 0.
    \end{equation*}
    This property is commonly called Galerkin orthogonality.

    Let $v \in S$.
    By the Galerkin orthogonality and the ellipticity and boundedness of $a$, we get that
    \begin{align*}
        \norm{u-u_S}_V^2
        &\leq \frac{1}{\alpha} a(u-u_S,u-u_S) \\
        &= \frac{1}{\alpha} a(u-u_S,u-v) \\
        &\leq \frac{C}{\alpha} \norm{u-u_S}_V \norm{u-v}_V.
    \end{align*}
    Dividing both sides by $\norm{u-u_S}_V$
    and taking the infimum over all $v \in S$,
    the claim follows.
\end{proof}

For our purposes, the Hilbert space $V$ is a subspace of the Sobolev space $H^1(\Omega)$,
and the approximation properties of the finite element space $S$ in the $H^1(\Omega)$-norm
correspond to the general approximation properties of polynomials with respect to the degree $p$.

\subsubsection{Approximation Properties of Polynomials}
\label{subsubsec:approximation_properties_of_polynomials}

Let us begin with the approximation properties of polynomials in one dimension,
after which we consider the two-dimensional setting.
The one-dimensional analysis is based on the Legendre series
which is discussed in great detail in e.g.\ \cite{schwab1998} and \cite{andrews1998}.

Let $I = (-1,1) \subset \mathbb{R}$ and $u \in L^2(I)$.
The Legendre series of $u$ is given by
\begin{equation}
    \label{eq:legendre_series}
    u(x) = \sum_{i=0}^{\infty} a_i P_i(x),
\end{equation}
where $P_i$ is the Legendre polynomial of degree $i$.
The coefficients of the series are given by
\begin{equation*}
    a_i = \frac{2i+1}{2} \int_{-1}^{1} u(x) P_i(x) \diff x
\end{equation*}
for all $i=0,1,\dotsc$,
which follows from the orthogonality of the Legendre polynomials:
\begin{equation*}
    \int_{-1}^{1} P_i(x) P_j(x) \diff x =
    \begin{cases}
        \frac{2}{2i+1}, & \text{if } i = j \\
        0, & \text{otherwise}.
    \end{cases}
\end{equation*}

Convergence of the series \eqref{eq:legendre_series} can always be understood
in the sense of $L^2$. That is, it holds that
\begin{equation*}
    \lim_{p \to \infty} \norm{u - \sum_{i=0}^{\vphantom{n}\smash{p}} a_i P_i}_{L^2(I)} = 0.
\end{equation*}
This implies that the series converges pointwise almost everywhere in $I$.
If $u$ has a continuous derivative, then the pointwise convergence holds everywhere in $I$.

When $u \in H^2(I)$, the Legendre series can be used to prove the following
polynomial approximation theorem.
\begin{theorem}
    \label{thm:polynomial_approximation_1d}
    Let $u \in H^2(I)$ and $p \in \mathbb{N}$.
    Then there exists a polynomial $u_p \in \mathcal{P}_p(I)$ and
    a constant $C > 0$ independent of $u$ and $p$ such that
    \begin{equation}
        \label{eq:polynomial_approximation_1d_endpoint_equality}
        u(\pm 1) = u_p(\pm 1)
    \end{equation}
    and
    \begin{align}
        \label{eq:polynomial_approximation_1d_res1}
        &\norm{u-u_p}_{L^2(I)} \leq C p^{-2} A(u), \\[0.5em]
        \label{eq:polynomial_approximation_1d_res2}
        &\norm{u-u_p}_{H^1(I)} \leq C p^{-1} A(u), \\[0.5em]
        \label{eq:polynomial_approximation_1d_res3}
        &\norm{u-u_p}_{L^{\infty}(I)} \leq C p^{-1} A(u),
    \end{align}
    where
    \begin{equation*}
        A(u) = \left(
            \int_{-1}^{1} \abs{u''(x)}^2 (1-x^2) \diff x
            \right)^{\frac{1}{2}}.
    \end{equation*}
\end{theorem}
\begin{proof}
    For the claims \eqref{eq:polynomial_approximation_1d_endpoint_equality},
    \eqref{eq:polynomial_approximation_1d_res1} and \eqref{eq:polynomial_approximation_1d_res2},
    see \cite[Lemma~3.3]{babuskasuri1987} or \cite{guibabuska1986}.
    The polynomial $u_p$ is constructed by letting its derivative be the
    $(p-1)$th order truncated Legendre series of the derivative of $u$, that is,
    \begin{equation*}
        u_p'(x) = \sum_{i=0}^{p-1} b_i P_i(x),
    \end{equation*}
    where
    \begin{equation*}
        b_i = \frac{2i+1}{2} \int_{-1}^{1} u'(x) P_i(x) \diff x.
    \end{equation*}
    See also \cite[Theorem~3.14 on p.~73]{schwab1998} for the same construction.
    Moreover, by \cite[Theorem 3.10 on p.\ 71]{schwab1998},
    the $A(u)$ term and the coefficients $b_i$ are related by
    \begin{equation}
        \label{eq:polynomial_approximation_1d_A}
        A(u)^2
        = \int_{-1}^{1} \abs{u''(x)}^2 (1-x^2) \diff x
        = \sum_{i=1}^{\infty} \frac{2}{2i+1} \frac{(i+1)!}{(i-1)!} \abs{b_i}^2.
    \end{equation}
    Let us use the above construction to prove \eqref{eq:polynomial_approximation_1d_res3}
    as well which is not included in the results of the aforementioned references.

    Let $x \in I$. By the fundamental theorem of calculus
    and the definition of $u_p'$, we get that
    \begin{align}
        u(x) - u_p(x)
        &= \int_{-1}^{x} u'(y) - u_p'(y) \diff y \nonumber \\
        &= \int_{-1}^{x} \sum_{i=p}^{\infty} b_i P_i(y) \diff y \nonumber \\
        \label{eq:polynomial_approximation_1d_pointwise_intmed1}
        &= \sum_{i=p}^{\infty} b_i \int_{-1}^{x} P_i(y) \diff y.
    \end{align}
    By \cite[p.~151]{andrews1998}, the Legendre polynomials satisfy
    \begin{equation*}
        P_i(y) = \frac{1}{2i+1} (P_{i+1}'(y) - P_{i-1}'(y))
    \end{equation*}
    for all $i=1,2,\dotsc$. We may thus compute the integral in
    \eqref{eq:polynomial_approximation_1d_pointwise_intmed1} as
    \begin{align}
        \int_{-1}^{x} P_i(y) \diff y
        &= \frac{1}{2i+1} \int_{-1}^{x} P_{i+1}'(y) - P_{i-1}'(y) \diff y \nonumber \\
        \label{eq:polynomial_approximation_1d_pointwise_intmed2}
        &= \frac{1}{2i+1} (P_{i+1}(x) - P_{i-1}(x)),
    \end{align}
    where we also used the property of the Legendre polynomials that
    $P_i(-1) = (-1)^i$ for all $i=0,1,\dotsc$.
    The Legendre polynomials satisfy $\abs{P_i(x)} \leq 1$ for all $x \in I$
    and $i=0,1,\dotsc$, which now implies together with
    \eqref{eq:polynomial_approximation_1d_pointwise_intmed1}
    and \eqref{eq:polynomial_approximation_1d_pointwise_intmed2}
    the pointwise estimate
    \begin{align*}
        \abs{u(x) - u_p(x)}
        &\leq \sum_{i=p}^{\infty} \abs{b_i} \frac{1}{2i+1}
            (\abs{P_{i+1}(x)} + \abs{P_{i-1}(x)}) \\
        &\leq \sum_{i=p}^{\infty} \frac{2}{2i+1} \abs{b_i} \\
        &= \sum_{i=p}^{\infty}
            \left(
                \left( \frac{2}{2i+1} \right)^{\frac{1}{2}}
                \left( \frac{(i-1)!}{(i+1)!} \right)^{\frac{1}{2}}
            \right)
            \left(
                \left( \frac{2}{2i+1} \right)^{\frac{1}{2}}
                \left( \frac{(i+1)!}{(i-1)!} \right)^{\frac{1}{2}}
                \abs{b_i}
            \right).
    \end{align*}
    Hölder's inequality and \eqref{eq:polynomial_approximation_1d_A} imply that
    \begin{align}
        \abs{u(x) - u_p(x)}^2
        &\leq \left( \sum_{i=p}^{\infty} \frac{2}{2i+1} \frac{(i-1)!}{(i+1)!}
            \right)
            \left( \sum_{i=p}^{\infty} \frac{2}{2i+1} \frac{(i+1)!}{(i-1)!} 
            \abs{b_i}^2 \right) \nonumber \\
        &\leq \left( \sum_{i=p}^{\infty} \frac{2}{2i+1} \frac{(i-1)!}{(i+1)!}
            \right)
            \left( \sum_{i=1}^{\infty} \frac{2}{2i+1} \frac{(i+1)!}{(i-1)!} 
            \abs{b_i}^2 \right) \nonumber \\
        \label{eq:polynomial_approximation_1d_pointwise_intmed3}
        &= \left( \sum_{i=p}^{\infty} \frac{2}{2i+1} \frac{(i-1)!}{(i+1)!}
            \right) A(u)^2.
    \end{align}
    The remaining series in \eqref{eq:polynomial_approximation_1d_pointwise_intmed3}
    can be estimated by
    \begin{align*}
        \sum_{i=p}^{\infty} \frac{2}{2i+1} \frac{(i-1)!}{(i+1)!}
        &= \sum_{i=p}^{\infty} \frac{2}{(2i+1)(i+1)i} \\
        &\leq \sum_{i=p}^{\infty} i^{-3} \\
        &\leq p^{-3} + \int_{p}^{\infty} t^{-3} \diff t \\
        &= p^{-3} + \frac{1}{2} p^{-2} \\
        &\leq \frac{3}{2} p^{-2}.
    \end{align*}
    This implies with \eqref{eq:polynomial_approximation_1d_pointwise_intmed3} that
    \begin{equation*}
        \abs{u(x) - u_p(x)}^2
        \leq \frac{3}{2} p^{-2} A(u)^2
    \end{equation*}
    for all $x \in I$, which then finally implies 
    \eqref{eq:polynomial_approximation_1d_res3}.
\end{proof}

Theorem~\ref{thm:polynomial_approximation_1d}
has a two-dimensional counterpart for the reference quadrilateral
and the reference triangle.
The polynomial space is assumed to be the product space for the quadrilateral
and the trunk space for the triangle. This assumption will be relaxed later.
\begin{theorem}
    \label{thm:polynomial_approximation_2d}
    Let $\widehat{E} = \widehat{Q}$ (resp.\ $\widehat{E} = \widehat{T}$).
    Let $u \in H^2(\widehat{E})$ and $p \in \mathbb{N}$.
    Then there exists a $u_p \in \mathcal{P}_p^{(pr)}(\widehat{Q})$
    (resp.\ $u_p \in \mathcal{P}_p^{(tr)}(\widehat{T})$) and
    a constant $C > 0$ independent of $u$ and $p$ such that
    \begin{align}
        \label{eq:polynomial_approximation_2d_res1}
        &\norm{u-u_p}_{H^1(\widehat{E})}
        \leq C p^{-1} \norm{u}_{H^2(\widehat{E})}, \\[0.5em]
        \label{eq:polynomial_approximation_2d_res2}
        &\norm{u-u_p}_{L^{\infty}(\widehat{E})}
        \leq C p^{-1} \norm{u}_{H^2(\widehat{E})}, \\[0.5em]
        \label{eq:polynomial_approximation_2d_res3}
        &\norm{u-u_p}_{L^2(\gamma)}
        \leq C p^{-3/2} \norm{u}_{H^2(\widehat{E})}, \\[0.5em]
        \label{eq:polynomial_approximation_2d_res4}
        &\norm{u-u_p}_{H^1(\gamma)}
        \leq C p^{-1/2} \norm{u}_{H^2(\widehat{E})},
    \end{align}
    where $\gamma$ is any side of $\widehat{E}$.
\end{theorem}
\begin{proof}
    The proof for the case $\widehat{E} = \widehat{Q}$
    can be found in \cite[Lemma~3.1]{babuskasuri1987},
    where the result follows from the approximation properties
    of truncated Fourier series.

    Let us use the result for the quadrilateral
    and extend it to the case $\widehat{E} = \widehat{T}$.
    We do this by
    dividing the quadrilateral $\widehat{Q}$ into two triangles
    along the diagonal line $x_2 = x_1$.
    Let $\widetilde{T}$ denote the resulting bottom triangle,
    and let $F: \widehat{T} \to \widetilde{T}$ be a bijective affine mapping
    between the reference triangle and the bottom triangle.
    
    Define $\tilde{u} = u \circ F^{-1} \in H^2(\widetilde{T})$.
    By \cite[Theorem~5 on p.~181]{stein1970},
    there exists an extension $\tilde{U} \in H^2(\widehat{Q})$ of $\tilde{u}$
    such that $\tilde{U}|_{\widetilde{T}} = \tilde{u}$ and
    \begin{equation}
        \label{eq:polynomial_approximation_2d_extension_continuity}
        \norm{\tilde{U}}_{H^2(\widehat{Q})}
        \leq C_1 \norm{\tilde{u}}_{H^2(\widetilde{T})},
    \end{equation}
    where the constant $C_1 > 0$ is independent of $\tilde{u}$.

    Assume for now that $p \geq 2$.
    If $p$ is even, let $q = p/2 \in \mathbb{N}$,
    and if $p$ is odd, let $q = (p-1)/2 \in \mathbb{N}$.
    Now there exists a $\tilde{U}_q \in \mathcal{P}_q^{(pr)}(\widehat{Q})
    \subset \mathcal{P}_p^{(tr)}(\widehat{Q})$
    such that $\tilde{U}$ and $\tilde{U}_q$ satisfy
    \eqref{eq:polynomial_approximation_2d_res1}-\eqref{eq:polynomial_approximation_2d_res4}
    with $q$ instead of $p$.
    
    Let $\tilde{u}_p \in \mathcal{P}_p^{(tr)}(\widetilde{T})$
    be the restriction of $\tilde{U}_q$ on $\widetilde{T}$.
    Using \eqref{eq:polynomial_approximation_2d_extension_continuity},
    we may now prove
    \eqref{eq:polynomial_approximation_2d_res1}-\eqref{eq:polynomial_approximation_2d_res4}
    for $\tilde{u}$ and $\tilde{u}_p$ in the triangle $\widetilde{T}$.
    For example,
    \begin{align*}
        \norm{\tilde{u}-\tilde{u}_p}_{H^1(\widetilde{T})}
        &\leq \norm{\tilde{U}-\tilde{U}_q}_{H^1(\widehat{Q})} \\
        &\leq C q^{-1} \norm{\tilde{U}}_{H^2(\widehat{Q})} \\
        &\leq 4C p^{-1} \norm{\tilde{U}}_{H^2(\widehat{Q})} \\
        &\leq 4C C_1 p^{-1} \norm{\tilde{u}}_{H^2(\widetilde{T})}.
    \end{align*}
    The other estimates follow more or less analogously.
    Regarding the side of $\widetilde{T}$ that corresponds to the diagonal line $x_2=x_1$,
    it is shown in \cite[Lemma~3.1]{babuskasuri1987} that
    \eqref{eq:polynomial_approximation_2d_res3}
    and \eqref{eq:polynomial_approximation_2d_res4}
    hold for it as well when $\widehat{E} = \widehat{Q}$.

    The desired estimates for the case $\widehat{E} = \widehat{T}$
    now follow by applying the affine coordinate transformation $F$
    to $\tilde{u}$ and $\tilde{u}_p$.
    The resulting approximation $u_p = \tilde{u}_p \circ F$
    belongs to the trunk polynomial space $\mathcal{P}_p^{(tr)}(\widehat{T})$
    because an affine mapping preserves polynomials.

    It was assumed above that $p \geq 2$.
    If $p=1$, we can simply choose
    $u_p = 0 \in \mathcal{P}_{1}^{(tr)}(\widehat{T})$, and the estimates
    \eqref{eq:polynomial_approximation_2d_res1}-\eqref{eq:polynomial_approximation_2d_res4}
    follow from the Sobolev imbedding theorem and the trace theorem.
\end{proof}

Theorem~\ref{thm:polynomial_approximation_2d} does not provide precise error bounds
since the value of the constant $C$ is not known,
but it enables us to consider whether the approximations converge as $p \to \infty$.

\subsubsection{Approximation Properties of the Finite Element Space}
\label{subsubsec:approximation_properties_of_the_finite_element_space}

We defined the finite element space by
\begin{align*}
    S
    &= S(\Omega, \mathcal{M}, p) \\
    &= \{
        u \in C(\overline{\Omega})
            : u \circ F_i \in \mathcal{P}_p(\widehat{E_i}),
                \text{ } i=1,2,\dotsc,N(\mathcal{M}),
                    \text{ } Bu = 0
    \},
\end{align*}
where $\mathcal{P}_p(\widehat{E_i})$ is either the product space or the trunk space.

Let $u \in H^2(\Omega)$ be such that it satisfies the condition $Bu = 0$.
By e.g.\ \cite{babuskasuri1987} and \cite{babuskasuri1987hp},
there exists a $u_p \in S$ and a constant $C > 0$ independent of $u$ and $p$ such that
\begin{equation}
    \label{eq:fem_conv_goal1}
    \norm{u-u_p}_{H^1(\Omega)} \leq C p^{-1} \norm{u}_{H^2(\Omega)}.
\end{equation}
Céa's lemma then immediately implies the same estimate
for the $H^1(\Omega)$ error between the exact solution
and the finite element solution when the exact solution belongs to the Sobolev space $H^2(\Omega)$.

To be able to prove an $L^2(\Omega)$ error estimate for the Dirac delta problem,
we will also need the pointwise estimate
\begin{equation}
    \label{eq:fem_conv_goal2}
    \norm{u-u_p}_{L^{\infty}(\Omega)} \leq C p^{-1} \norm{u}_{H^2(\Omega)},
\end{equation}
where the functions $u$ and $u_p$ are as above.
The estimate \eqref{eq:fem_conv_goal2} does not seem to be covered as such
by the existing standard literature so we will prove it in full detail.

The estimate \eqref{eq:fem_conv_goal2} can be proven with the same approach that
is used in \cite{babuskasuri1987} to prove the estimate \eqref{eq:fem_conv_goal1}.
The idea is to use Theorem~\ref{thm:polynomial_approximation_2d} element-wise,
but note that the resulting approximation is not necessarily continuous yet, i.e.\ it does not
belong to the space $S$. The element-wise approximations need to be
stitched together over the sides of the elements. This is achieved by adding suitable
auxiliary functions to each element-wise approximation. By suitable it is meant
that the estimates of Theorem~\ref{thm:polynomial_approximation_2d} continue to hold.
Homogeneous Dirichlet boundary conditions can be fixed with the same approach as well.
We consider these auxiliary functions first.

There are two types of auxiliary functions.
The first type is used to fix the values at the vertices of the mesh,
and the second type is used to fix the values over the sides after the vertices have been fixed.
Both types can be constructed with the help of the following theorem.
More or less the same constructions and their relevant norm estimates that are needed to prove
\eqref{eq:fem_conv_goal1} can be found in \cite{babuskasuri1987}.
In particular, see the proof of Theorem~4.1 in \cite{babuskasuri1987}.
We supplement the estimates with the necessary $L^{\infty}(\Omega)$-norm estimates
that are needed to prove \eqref{eq:fem_conv_goal2}.
\begin{theorem}
    \label{thm:auxiliary_auxiliary_function}
    Let $I=(-1,1)$ and $p \in \mathbb{N}$.
    Then there exists a polynomial $\psi_p \in \mathcal{P}_p(I)$ and
    a constant $C > 0$ independent of $p$ such that
    \begin{equation}
        \label{eq:auxiliary_auxiliary_function_endpoints}
        \psi_p(-1)=1 \quad \text{and} \quad \psi_p(1)=0
    \end{equation}
    and
    \begin{align}
        \label{eq:auxiliary_auxiliary_function_res1}
        &\norm{\psi_p}_{L^2(I)} \leq C p^{-1/2}, \\[0.5em]
        \label{eq:auxiliary_auxiliary_function_res2}
        &\norm{\psi_p}_{H^1(I)} \leq C p^{1/2}, \\[0.5em]
        \label{eq:auxiliary_auxiliary_function_res3}
        &\norm{\psi_p}_{L^{\infty}(I)} \leq C.
    \end{align}
\end{theorem}
\begin{proof}
    Like in \cite{babuskasuri1987}, define
    \begin{equation*}
        \phi_p(x) = \frac{e^{-p(x+1)} - e^{-2p}}{1 - e^{-2p}}.
    \end{equation*}
    Clearly, $\phi_p(-1)=1$ and $\phi_p(1)=0$.
    Applying now Theorem~\ref{thm:polynomial_approximation_1d} to the function $\phi_p$
    gives the desired polynomial $\psi_p \in \mathcal{P}_p(I)$ that satisfies
    \eqref{eq:auxiliary_auxiliary_function_endpoints},
    \eqref{eq:auxiliary_auxiliary_function_res1} and
    \eqref{eq:auxiliary_auxiliary_function_res2}, see \cite{babuskasuri1987}.
    It remains to prove the claim \eqref{eq:auxiliary_auxiliary_function_res3}.

    The function $\phi_p$ is strictly decreasing in $I$, which implies that
    \begin{equation}
        \label{eq:phi_p_Linfty}
        \norm{\phi_p}_{L^{\infty}(I)} \leq 1.
    \end{equation}
    By \cite{babuskasuri1987}, the term $A(\phi_p)$ in
    Theorem~\ref{thm:polynomial_approximation_1d} has the upper bound
    \begin{equation*}
        A(\phi_p) \leq C p
    \end{equation*}
    for some constant $C > 0$ independent of $p$. Thus,
    \begin{equation}
        \label{eq:phi_psi_Linfty_new}
        \norm{\phi_p-\psi_p}_{L^{\infty}(I)} \leq C.
    \end{equation}
    Combining \eqref{eq:phi_p_Linfty} and \eqref{eq:phi_psi_Linfty_new} gives
    the desired estimate \eqref{eq:auxiliary_auxiliary_function_res3}:
    \begin{equation*}
        \norm{\psi_p}_{L^{\infty}(I)}
        \leq \norm{\psi_p - \phi_p}_{L^{\infty}(I)} + \norm{\phi_p}_{L^{\infty}(I)}
        \leq C + 1.
    \end{equation*}
\end{proof}

The next theorem is used to fix the values at the vertices of the elements.
\begin{theorem}
    \label{thm:auxiliary_vertex}
    Let $\widehat{E} = \widehat{Q}$ (resp.\ $\widehat{E} = \widehat{T}$).
    Let $V_i$ be a vertex of $\widehat{E}$ for some $i \in \{1,2,3,4\}$
    (resp.\ $i \in \{1,2,3\}$). Let $p \in \mathbb{N}$.
    Then there exists a polynomial $\nu_p \in \mathcal{P}_p^{(pr)}(\widehat{Q})$
    (resp.\ $\nu_p \in \mathcal{P}_p^{(tr)}(\widehat{T})$) and
    a constant $C > 0$ independent of $p$ such that
    \begin{equation}
        \label{eq:auxiliary_vertex_vertices}
        \nu_p(V_i) = 1
        \quad \text{and} \quad
        \nu_p(V_j) = 0 \text{ } \text{ for all } j \neq i
    \end{equation}
    and
    \begin{align}
        \label{eq:auxiliary_vertex_res1}
        &\norm{\nu_p}_{H^1(\widehat{E})} \leq C, \\[0.5em]
        \label{eq:auxiliary_vertex_res2}
        &\norm{\nu_p}_{L^{\infty}(\widehat{E})} \leq C, \\[0.5em]
        \label{eq:auxiliary_vertex_res3}
        &\norm{\nu_p}_{L^2(\gamma)} \leq C p^{-1/2}, \\[0.5em]
        \label{eq:auxiliary_vertex_res4}
        &\norm{\nu_p}_{H^1(\gamma)} \leq C p^{1/2},
    \end{align}
    where $\gamma$ is any edge of $\widehat{E}$.
\end{theorem}
\begin{proof}
    Let first $\widehat{E} = \widehat{Q}$.
    Without loss of generality, assume that $V_i = (-1,-1)$.
    Define the polynomial $\nu_p$ by
    \begin{equation*}
        \nu_p(x) = \psi_p(x_1) \psi_p(x_2) \in \mathcal{P}_p^{(pr)}(\widehat{Q}),
    \end{equation*}
    where $\psi_p \in \mathcal{P}_p(I)$ is given by Theorem~\ref{thm:auxiliary_auxiliary_function}.
    The claim for the case $\widehat{E} = \widehat{Q}$ now follows from the properties
    of $\psi_p$ in Theorem~\ref{thm:auxiliary_auxiliary_function}, see \cite{babuskasuri1987}.
    Let us only consider the estimate \eqref{eq:auxiliary_vertex_res2}:
    \begin{equation*}
        \norm{\nu_p}_{L^{\infty}(\widehat{Q})}
        \leq \norm{\psi_p}_{L^{\infty}(I)}^2
        \leq C^2.
    \end{equation*}

    Consider then the case $\widehat{E} = \widehat{T}$.
    Without loss of generality, assume that $V_i = (0,0)$.
    If $p$ is even, let $q = p/2 \in \mathbb{N}$ and define
    \begin{equation*}
        \nu_p(x) = \psi_q(2x_1-1) \psi_q(2x_2-1)
        \in \mathcal{P}_p^{(tr)}(\widehat{T}).
    \end{equation*}
    If $p > 1$ and $p$ is odd, let $q = (p-1)/2 \in \mathbb{N}$ and define
    \begin{equation*}
        \nu_p(x) = \psi_q(2x_1-1) \psi_{q+1}(2x_2-1)
        \in \mathcal{P}_p^{(tr)}(\widehat{T}).
    \end{equation*}
    The polynomial $\psi_q \in \mathcal{P}_q(I)$ is again given by 
    Theorem~\ref{thm:auxiliary_auxiliary_function}.
    The claim now follows with similar arguments as above.
    If $p = 1$, then $v_p$ can be chosen as the nodal shape function
    \begin{equation*}
        \nu_p(x) = 1-x_1-x_2 \in \mathcal{P}_1^{(tr)}(\widehat{T}).
    \end{equation*}
\end{proof}

After Theorem~\ref{thm:auxiliary_vertex} has been applied,
the next theorem is used to fix the values across the sides.
\begin{theorem}
    \label{thm:auxiliary_edge}
    Let $\widehat{E} = \widehat{Q}$ (resp.\ $\widehat{E} = \widehat{T}$).
    Let $\gamma$ be a side of $\widehat{E}$ with the vertices $V_1$ and $V_2$ at the endpoints.
    Let $w_p \in \mathcal{P}_p(\gamma) \cong \mathcal{P}_p(I)$,
    $p \in \mathbb{N}$, be a polynomial on the side such that $w_p(V_1) = w_p(V_2) = 0$.
    Then there exists a polynomial $\xi_p \in \mathcal{P}_p^{(pr)}(\widehat{Q})$
    (resp.\ $\xi_p \in \mathcal{P}_{2p}^{(tr)}(\widehat{T})$) and
    a constant $C > 0$ independent of $w_p$ and $p$ such that
    \begin{equation}
        \label{eq:auxiliary_edge_boundary}
        \xi_p = w_p \text{ } \text{ on } \gamma
        \quad \text{and} \quad
        \xi_p = 0 \text{ } \text{ on } \partial \widehat{E} \setminus \gamma
    \end{equation}
    and
    \begin{align}
        \label{eq:auxiliary_edge_res1}
        &\norm{\xi_p}_{H^1(\widehat{E})}
        \leq C p^{-1/2} \norm{w_p}_{H^1(\gamma)}
            + C p^{1/2} \norm{w_p}_{L^2(\gamma)}, \\[0.5em]
        \label{eq:auxiliary_edge_res2}
        &\norm{\xi_p}_{L^{\infty}(\widehat{E})}
        \leq C \norm{w_p}_{L^{\infty}(\gamma)}.
    \end{align}
\end{theorem}
\begin{proof}
    Let first $\widehat{E} = \widehat{Q}$.
    Without loss of generality, assume that $\gamma$ is the side corresponding to the
    horizontal line $x_2 = -1$. Define the polynomial $\xi_p$ by
    \begin{equation*}
        \xi_p(x) = w_p(x_1) \psi_p(x_2) \in \mathcal{P}_p^{(pr)}(\widehat{Q}),
    \end{equation*}
    where $\psi_p \in \mathcal{P}_p(I)$ is given by Theorem~\ref{thm:auxiliary_auxiliary_function}.
    The claim for the case $\widehat{E} = \widehat{Q}$ now follows from the properties of
    $\psi_p$ in Theorem~\ref{thm:auxiliary_auxiliary_function} and the assumption that
    $w_p(V_1) = w_p(V_2) = 0$, see \cite{babuskasuri1987}. We only consider the estimate
    \eqref{eq:auxiliary_edge_res2} which follows easily:
    \begin{equation*}
        \norm{\xi_p}_{L^{\infty}(\widehat{Q})}
        \leq \norm{\psi_p}_{L^{\infty}(I)} \norm{w_p}_{L^{\infty}(\gamma)}
        \leq C \norm{w_p}_{L^{\infty}(\gamma)}.
    \end{equation*}

    Assume then that $\widehat{E} = \widehat{T}$.
    Without loss of generality, let $\gamma$ be the side corresponding
    to the line $x_2=1-x_1$. The side has the parametrization
    $\gamma(t) = [t,1-t]^T$ for $t \in [0,1]$, and we write
    $w_p(t)$ to mean $w_p(\gamma(t))$.
    For $p \geq 2$, define $\xi_p$ now by
    \begin{equation}
        \label{eq:auxiliary_side_tri_xi}
        \xi_p(x) = \psi_{p-1}(2(1-x_1-x_2)-1) [ x_2 w_p(x_1) + x_1 w_p(1-x_2) ]
        \in \mathcal{P}_{2p}^{(tr)}(\widehat{T}),
    \end{equation}
    where $\psi_{p-1} \in \mathcal{P}_{p-1}(I)$ is again given by
    Theorem~\ref{thm:auxiliary_auxiliary_function}.
    The claim follows with similar arguments as above.
    In particular, the estimate \eqref{eq:auxiliary_edge_res2} is again easy to prove:
    \begin{align*}
        \norm{\xi_p}_{L^{\infty}(\widehat{T})}
        &\leq \norm{\psi_{p-1}}_{L^{\infty}(I)}
            (\norm{w_p}_{L^{\infty}(\gamma)} + \norm{w_p}_{L^{\infty}(\gamma)}) \\
        &\leq C \norm{w_p}_{L^{\infty}(\gamma)}.
    \end{align*}
    
    Note that we have not defined $\psi_{p-1}$ for $p=1$ in \eqref{eq:auxiliary_side_tri_xi},
    but if $p=1$, then $w_p(V_1) = w_p(V_2) = 0$ implies that $w_p = 0$ on $\gamma$ and
    then $\xi_p = 0$ obviously has the desired properties.
\end{proof}

We are now ready to take the above auxiliary theorems into use
and consider the estimates \eqref{eq:fem_conv_goal1} and \eqref{eq:fem_conv_goal2},
although we provide the full proof only for the second one.
\begin{theorem}
    \label{thm:approximation_properties_of_S}
    Let $u \in H^2(\Omega)$ be such that it satisfies the condition $Bu = 0$.
    Then there exists a $u_p \in S(\Omega, \mathcal{M}, p)$ and
    a constant $C > 0$ independent of $u$ and $p$ such that
    \begin{align}
        \label{eq:approximation_properties_of_S_res1}
        &\norm{u-u_p}_{H^1(\Omega)} \leq C p^{-1} \norm{u}_{H^2(\Omega)}, \\[0.5em]
        \label{eq:approximation_properties_of_S_res2}
        &\norm{u-u_p}_{L^{\infty}(\Omega)} \leq C p^{-1} \norm{u}_{H^2(\Omega)}.
    \end{align}
\end{theorem}
\begin{proof}
    Babu{\v s}ka and Suri \cite[Theorem~4.1]{babuskasuri1987} prove the existence of an
    approximation $u_{p+1} \in S(\Omega, \mathcal{M}, p+1)$ such that 
    \eqref{eq:approximation_properties_of_S_res1} holds when the polynomial space in the
    definition of $S$ is the product space. The steps of their proof
    can essentially be mapped to applications of the above theorems
    \ref{thm:polynomial_approximation_2d}, \ref{thm:auxiliary_auxiliary_function},
    \ref{thm:auxiliary_vertex} and \ref{thm:auxiliary_edge}.
    Let us prove the estimate \eqref{eq:approximation_properties_of_S_res2}
    with the same approach but assume that the polynomial space is the trunk space.
    This is enough to cover the product space as well since
    $\mathcal{P}_p^{(tr)}(\widehat{E}) \subset \mathcal{P}_p^{(pr)}(\widehat{E})$.
    The arguments below could also easily be substituted into the proof of the estimate
    \eqref{eq:approximation_properties_of_S_res1} in \cite{babuskasuri1987}
    to show that it holds for the trunk space as well and the $(p+1)$th order approximation
    $u_{p+1}$ can be replaced with a $p$th order approximation $u_p \in S(\Omega, \mathcal{M}, p)$.

    Let $E_i \in \mathcal{M}$ for any $i \in \{ 1,2,\dotsc,N(\mathcal{M}) \}$.
    Define $\hat{u}^{(i)} = u \circ F_i \in H^2(\widehat{E}_i)$.
    If $p$ is even, let $q = p/2 \in \mathbb{N}$.
    If $p \geq 2$ is odd, let $q = (p-1)/2 \in \mathbb{N}$.
    If $p=1$, let $q=1$. Note that $q^{-1} \leq 4 p^{-1}$.
    Now by Theorem~\ref{thm:polynomial_approximation_2d},
    depending on whether $\widehat{E}_i = \widehat{Q}$ or $\widehat{E}_i = \widehat{T}$,
    there exists a $\hat{u}_p^{(i)} \in \mathcal{P}_q^{(pr)}(\widehat{Q})
    \subset \mathcal{P}_p^{(tr)}(\widehat{Q})$ or a
    $\hat{u}_p^{(i)} \in \mathcal{P}_q^{(tr)}(\widehat{T})
    \subset \mathcal{P}_p^{(tr)}(\widehat{T})$ such that
    \begin{equation}
        \label{eq:approximation_properties_of_S_piecewise_res2}
        \norm{\hat{u}^{(i)}-\hat{u}_p^{(i)}}_{L^{\infty}(\widehat{E}_i)}
        \leq C q^{-1} \norm{\hat{u}^{(i)}}_{H^2(\widehat{E}_i)}
        \leq 4 C p^{-1} \norm{\hat{u}^{(i)}}_{H^2(\widehat{E}_i)},
    \end{equation}
    where the constant $C > 0$ is independent of $u$ and $p$.

    Define $u_p^{(i)} = \hat{u}_p^{(i)} \circ F_i^{-1}$ for all $i=1,2,\dotsc,N$.
    We could try to define the desired approximation $u_p$ such that
    $u_p|_{E_i} = u_p^{(i)}$ for all $i=1,2,\dotsc,N$, but the resulting function
    does not necessarily satisfy the continuity requirement or the condition that $Bu_p = 0$.
    In other words, $u_p \notin S$. However, if $u_p \in S$ were true, then
    \eqref{eq:approximation_properties_of_S_piecewise_res2} would imply
    \eqref{eq:approximation_properties_of_S_res2} as we will see.
    Thus, the next step is to modify each $\hat{u}_p^{(i)}$ so that $u_p \in S$
    while preserving the estimate \eqref{eq:approximation_properties_of_S_piecewise_res2}.
    This can be achieved with the help of Theorem~\ref{thm:auxiliary_vertex}
    and Theorem~\ref{thm:auxiliary_edge}.

    Let us begin with the continuity requirement.
    We first modify each $\hat{u}_p^{(i)}$ so that $u_p^{(i)}(V_j) = u(V_j)$
    for each vertex $V_j$ of the element $E_i$. Let $\widehat{V}_j = F_i^{-1}(V_j)$
    denote the corresponding vertex of the reference element $\widehat{E}_i$.
    By Theorem~\ref{thm:auxiliary_vertex}, there exists a polynomial
    $\hat{\nu}_q \in \mathcal{P}_q^{(pr)}(\widehat{Q})$ or
    $\hat{\nu}_q \in \mathcal{P}_q^{(tr)}(\widehat{T})$ depending on the type of $E_i$ such that
    $\hat{\nu}_q(\widehat{V}_j) = 1$ and $\hat{\nu}_q(\widehat{V}_k) = 0$ for all $k \neq j$,
    and it satisfies the pointwise estimate
    \begin{equation}
        \label{eq:approx_props_of_S_piecewise_vertex_update}
        \norm{\hat{\nu}_q}_{L^{\infty}(\widehat{E}_i)} \leq C
    \end{equation}
    for some constant $C > 0$ that is independent of $p$.
    Redefining now $\hat{u}_p^{(i)}$ to be the polynomial
    $\hat{u}_p^{(i)} + (u - u_p^{(i)})(V_j) \hat{\nu}_q \in \mathcal{P}_p^{(tr)}(\widehat{E}_i)$
    satisfies the condition $u_p^{(i)}(V_j) = u(V_j)$.
    Moreover, the updated polynomial still satisfies the estimate
    \eqref{eq:approximation_properties_of_S_piecewise_res2}:
    \begin{align*}
        &\norm{\hat{u}^{(i)} -
        [\hat{u}_p^{(i)} + (u - u_p^{(i)})(V_j) \hat{\nu}_q]}_{L^{\infty}(\widehat{E}_i)} \\
        &\leq \norm{\hat{u}^{(i)} - \hat{u}_p^{(i)}}_{L^{\infty}(\widehat{E}_i)}
            + \abs{(u - u_p^{(i)})(V_j)} \norm{\hat{\nu}_q}_{L^{\infty}(\widehat{E}_i)} \\
        &= \norm{\hat{u}^{(i)} - \hat{u}_p^{(i)}}_{L^{\infty}(\widehat{E}_i)}
            + \abs{(\hat{u}^{(i)} - \hat{u}_p^{(i)})(\widehat{V}_j)}
                \norm{\hat{\nu}_q}_{L^{\infty}(\widehat{E}_i)} \\
        &\leq \norm{\hat{u}^{(i)} - \hat{u}_p^{(i)}}_{L^{\infty}(\widehat{E}_i)}
            + \norm{\hat{u}^{(i)} - \hat{u}_p^{(i)}}_{L^{\infty}(\widehat{E}_i)}
                \norm{\hat{\nu}_q}_{L^{\infty}(\widehat{E}_i)} \\
        &\leq C p^{-1} \norm{\hat{u}^{(i)}}_{H^2(\widehat{E}_i)},
    \end{align*}
    where the last inequality follows from \eqref{eq:approximation_properties_of_S_piecewise_res2}
    and \eqref{eq:approx_props_of_S_piecewise_vertex_update}.
    This step is repeated for every vertex of every element,
    which results in the equality $u_p = u$ at every vertex of the mesh.

    The next step is to achieve continuity across the sides of the elements.
    Let $E_i$ and $E_j$ be two elements with a common side $\gamma_{ij}$.
    Define the function $w_q^{(ij)} = u_p^{(j)} - u_p^{(i)}$ over the side $\gamma_{ij}$.
    Notice that $w_q^{(ij)}(V_1) = w_q^{(ij)}(V_2) = 0$,
    where $V_1$ and $V_2$ are the endpoints of $\gamma_{ij}$.
    Changing the variables via $F_i$ yields the polynomial
    $\hat{w}_q^{(ij)} = w_q^{(ij)} \circ F_i \in \mathcal{P}_q(\hat{\gamma}_i)$,
    where $\hat{\gamma}_i = F_i^{-1}(\gamma_{ij})$, which satisfies
    $\hat{w}_q^{(ij)}(\widehat{V}_1) = \hat{w}_q^{(ij)}(\widehat{V}_2) = 0$.
    Now by Theorem~\ref{thm:auxiliary_edge}, there exists an extension
    $\hat{\xi}_q \in \mathcal{P}_q^{(pr)}(\widehat{Q})$ or
    $\hat{\xi}_q \in \mathcal{P}_{2q}^{(tr)}(\widehat{T}) \subset \mathcal{P}_p^{(tr)}(\widehat{T})$
    of $\hat{w}_q^{(ij)}$ such that $\hat{\xi}_q = \hat{w}_q^{(ij)}$ on $\hat{\gamma}_i$
    and $\hat{\xi}_q = 0$ on $\partial \widehat{E}_i \setminus \hat{\gamma}_i$, and
    it satisfies the pointwise estimate
    \begin{align}
        \norm{\hat{\xi}_q}_{L^{\infty}(\widehat{E}_i)}
        &\leq C \norm{\hat{w}_q^{(ij)}}_{L^{\infty}(\hat{\gamma}_i)} \nonumber \\
        &= C \norm{w_q^{(ij)}}_{L^{\infty}(\gamma_{ij})} \nonumber \\
        &\leq C \left( \norm{u - u_p^{(i)}}_{L^{\infty}(\gamma_{ij})}
                + \norm{u - u_p^{(j)}}_{L^{\infty}(\gamma_{ij})} \right) \nonumber \\
        &= C \left( \norm{\hat{u}^{(i)} - \hat{u}_p^{(i)}}_{L^{\infty}(\hat{\gamma}_i)}
                + \norm{\hat{u}^{(j)} - \hat{u}_p^{(j)}}_{L^{\infty}(\hat{\gamma}_j)} \right)
                \nonumber \\
        &\leq C \left( \norm{\hat{u}^{(i)} - \hat{u}_p^{(i)}}_{L^{\infty}(\widehat{E}_i)}
                + \norm{\hat{u}^{(j)} - \hat{u}_p^{(j)}}_{L^{\infty}(\widehat{E}_j)} \right)
                \nonumber \\
        \label{eq:approx_props_of_S_piecewise_side_update}
        &\leq C p^{-1} \left( \norm{\hat{u}^{(i)}}_{H^2(\widehat{E}_i)}
            + \norm{\hat{u}^{(j)}}_{H^2(\widehat{E}_j)} \right).
    \end{align}
    By redefining now $\hat{u}_p^{(i)}$ to be
    $\hat{u}_p^{(i)} + \hat{\xi}_q \in \mathcal{P}_p^{(tr)}(\widehat{E}_i)$,
    we achieve continuity over the side $\gamma_{ij}$ while keeping the other sides unmodified.
    Moreover, by the estimates \eqref{eq:approximation_properties_of_S_piecewise_res2}
    and \eqref{eq:approx_props_of_S_piecewise_side_update},
    the new polynomial satisfies a slightly different form of the estimate
    \eqref{eq:approximation_properties_of_S_piecewise_res2}:
    \begin{align}
        \norm{\hat{u}^{(i)} - [\hat{u}_p^{(i)} + \hat{\xi}_q]}_{L^{\infty}(\widehat{E}_i)}
        &\leq \norm{\hat{u}^{(i)} - \hat{u}_p^{(i)}}_{L^{\infty}(\widehat{E}_i)}
            + \norm{\hat{\xi}_q}_{L^{\infty}(\widehat{E}_i)} \nonumber \\
        \label{eq:approximation_properties_of_S_piecewise_edge_res2}
        &\leq C p^{-1} \left( \norm{\hat{u}^{(i)}}_{H^2(\widehat{E}_i)}
            + \norm{\hat{u}^{(j)}}_{H^2(\widehat{E}_j)} \right).
    \end{align}
    
    After repeating the above procedure for every side of every element,
    the approximation $u_p$ is continuous.
    Moreover, by \eqref{eq:approximation_properties_of_S_piecewise_edge_res2},
    the approximation satisfies
    \begin{equation}
        \label{eq:approx_prop_S_piecewise_side_final_est}
        \norm{\hat{u}^{(i)} - \hat{u}_p^{(i)}}_{L^{\infty}(\widehat{E}_i)}
        \leq C p^{-1} \Bigg( \norm{\hat{u}^{(i)}}_{H^2(\widehat{E}_i)} +
            \sum_{\overline{E}_i \cap \overline{E}_j = \gamma_{ij}}
            \norm{\hat{u}^{(j)}}_{H^2(\widehat{E}_j)} \Bigg)
    \end{equation}
    for all $i=1,2,\dotsc,N$, where the summation iterates over the neighboring elements of $E_i$
    with a common side $\gamma_{ij}$.

    If the condition $Bu = 0$ corresponds to homogeneous Dirichlet boundary conditions,
    then the boundary values can be fixed with the exact same strategy that was used
    above to fix the values over the sides. After the boundary values have been fixed,
    the approximation $u_p$ belongs to the space $S$.
    We consider the pure Neumann case,
    i.e.\ the zero mean value requirement, after proving the estimate
    \eqref{eq:approximation_properties_of_S_res2} for the approximation that we have now obtained.

    The desired estimate \eqref{eq:approximation_properties_of_S_res2}
    follows from the estimate \eqref{eq:approx_prop_S_piecewise_side_final_est}:
    \begin{align}
        \norm{u - u_p}_{L^{\infty}(\Omega)}
        &= \max_{i=1,2,\dotsc,N} \norm{u - u_p}_{L^{\infty}(E_i)} \nonumber \\
        &= \max_{i=1,2,\dotsc,N}
            \norm{\hat{u}^{(i)} - \hat{u}_p^{(i)}}_{L^{\infty}(\widehat{E}_i)} \nonumber \\
        &\leq \max_{i=1,2,\dotsc,N} C p^{-1} \Bigg( \norm{\hat{u}^{(i)}}_{H^2(\widehat{E}_i)} +
            \sum_{\overline{E}_i \cap \overline{E}_j = \gamma_{ij}}
            \norm{\hat{u}^{(j)}}_{H^2(\widehat{E}_j)} \Bigg) \nonumber \\
        &\leq \max_{i=1,2,\dotsc,N} C p^{-1} \Bigg( \norm{u}_{H^2(E_i)} +
            \sum_{\overline{E}_i \cap \overline{E}_j = \gamma_{ij}}
            \norm{u}_{H^2(E_j)} \Bigg) \nonumber \\
        \label{eq:approx_prop_S_pointwise_final}
        &\leq C p^{-1} \norm{u}_{H^2(\Omega)}.
    \end{align}
    The second to last inequality follows from the equivalence of the norms
    $\norm{\cdot}_{H^2(\widehat{E}_i)}$ and $\norm{\cdot}_{H^2(E_i)}$
    under the change of variables $F_i: \widehat{E}_i \to E_i$ \cite[Theorem 1 on p.~13]{mazya2011}.
    The same approximation also satisfies \eqref{eq:approximation_properties_of_S_res1}
    \cite{babuskasuri1987}.
    This concludes the proof for the Dirichlet case.

    When $Bu = 0$ corresponds to the zero mean value requirement,
    the desired approximation is given by $u_p - \overline{u}_p \in S$, where
    $u_p$ is the unnormalized approximation obtained above that already satisfies the desired
    estimates and
    \begin{equation*}
        \overline{u}_p = \frac{1}{\abs{\Omega}} \int_{\Omega} u_p \diff x.
    \end{equation*}
    This follows from the following result that the constant $\overline{u}_p$
    is bounded by the same estimate:
    \begin{align*}
        \abs{\overline{u}_p}
        &= \abs{\overline{u} - \overline{u}_p} \\
        &\leq C \int_{\Omega} \abs{u - u_p} \diff x \\
        &\leq C \norm{u - u_p}_{L^2(\Omega)} \\
        &\leq C \norm{u - u_p}_{H^1(\Omega)} \\
        &\leq C p^{-1} \norm{u}_{H^2(\Omega)}.
    \end{align*}
    We used above the assumption that $\overline{u} = 0$, Hölder's inequality and
    the estimate \eqref{eq:approximation_properties_of_S_res1}. Thus,
    \begin{equation*}
        \norm{u - (u_p - \overline{u}_p)}_{L^{\infty}(\Omega)}
        \leq \norm{u - u_p}_{L^{\infty}(\Omega)} + \norm{\overline{u}_p}_{L^{\infty}(\Omega)}
        \leq C p^{-1} \norm{u}_{H^2(\Omega)}.
    \end{equation*}
    A similar argument can be used to prove
    the estimate \eqref{eq:approximation_properties_of_S_res1} as well.
\end{proof}

Consider then the usual weakly formulated boundary value problem
\begin{equation}
    \label{eq:H1_convergence_thm_problem}
    \text{Find } u \in V \text{ s.t.\ }
    a(u,v) = \varphi(v) \text{ for all } v \in V,
\end{equation}
where the space $V \subset H^1(\Omega)$ is defined
according to the type of the boundary value problem,
$a: V \times V \to \mathbb{R}$ is an elliptic bounded bilinear mapping and $\varphi \in V'$.
When the solution $u$ also belongs to the Sobolev space $H^2(\Omega)$, then
Céa's lemma and Theorem~\ref{thm:approximation_properties_of_S} immediately imply that
the corresponding finite element solutions converge towards the exact solution $u$ as
the degree $p$ is increased.
\begin{theorem}
    \label{thm:H1_convergence_of_p_version}
    Let $u \in V$ be the unique solution to the problem 
    \eqref{eq:H1_convergence_thm_problem}.
    Assume that $u \in H^2(\Omega)$.
    Let $u_S \in S(\Omega, \mathcal{M}, p)$ be the corresponding
    finite element solution. Then there exists a constant $C > 0$
    independent of $u$ and $p$ such that
    \begin{equation*}
        \norm{u-u_S}_{H^1(\Omega)} \leq C p^{-1} \norm{u}_{H^2(\Omega)}.
    \end{equation*}
\end{theorem}
\begin{proof}
    Let $u_p \in S$ be the approximation of $u$ provided by
    Theorem~\ref{thm:approximation_properties_of_S}.
    Now by Céa's lemma, i.e.\ Theorem~\ref{thm:ceas_lemma}, we get that
    \begin{align*}
        \norm{u - u_S}_{H^1(\Omega)}
        &\leq C \inf_{v \in S} \norm{u - v}_{H^1(\Omega)} \\
        &\leq C \norm{u - u_p}_{H^1(\Omega)} \\
        &\leq C p^{-1} \norm{u}_{H^2(\Omega)},
    \end{align*}
    where the constant $C > 0$ is independent of $u$ and $p$.
\end{proof}

Theorem~\ref{thm:H1_convergence_of_p_version} is the standard
well-known result regarding the convergence of the $p$-version
of the finite element method. In \cite{babuskasuri1987},
it is shown that the rate of convergence $p^{-1}$ is optimal in the general case.
The optimal convergence estimate for the $h$-version with first-order polynomials
is analogous to Theorem~\ref{thm:H1_convergence_of_p_version}
but the $p^{-1}$ is replaced with $h$, see e.g.\ \cite{braess2007}.

Continuing with the $h$-version, there also exist error estimates
in the $L^{\infty}(\Omega)$-norm.
For example, in \cite{braess2007}, it is shown that
\begin{equation*}
    \norm{u - u_S}_{L^{\infty}(\Omega)} \leq C h \norm{u}_{H^2(\Omega)},
\end{equation*}
and \cite{ciarlet2002} contains an even stronger estimate.
For the $p$-version, however, similar pointwise estimates do not seem to be
covered by the standard literature. Proving convergence estimates in the $L^{\infty}(\Omega)$-norm
is in general more difficult than in the $H^1(\Omega)$-norm
because Céa's lemma is not directly applicable.
Nevertheless, we are still able to prove a pointwise estimate for the $p$-version by
using a strategy identical to \cite[p.\ 93]{braess2007}.
The main enablers for this application are the pointwise estimate in
Theorem~\ref{thm:approximation_properties_of_S} and a certain inverse estimate
in \cite{schwab1998}. We will need this result in the next section for the
main theorem of this thesis.
\begin{theorem}
    \label{thm:pointwise_convergence_of_p_version}
    Let $u \in V$ be the unique solution to the problem \eqref{eq:H1_convergence_thm_problem}.
    Assume that $u \in H^2(\Omega)$.
    Let $u_S \in S(\Omega, \mathcal{M}, p)$ be the corresponding
    finite element solution. Then there exists a constant $C > 0$
    independent of $u$ and $p$ such that
    \begin{equation*}
        \norm{u-u_S}_{L^{\infty}(\Omega)}
        \leq C p^{-1} \left( 1 + \sqrt{\ln(p+1)} \right) \norm{u}_{H^2(\Omega)}.
    \end{equation*}
\end{theorem}
\begin{proof}
    Let $u_p \in S$ be the approximation of $u$ provided by
    Theorem~\ref{thm:approximation_properties_of_S}.
    The estimation is divided into two parts via the triangle inequality:
    \begin{equation}
        \label{eq:pointwise_convergence_of_p_fem_init_split}
        \norm{u-u_S}_{L^{\infty}(\Omega)}
        \leq \norm{u-u_p}_{L^{\infty}(\Omega)}
            + \norm{u_p-u_S}_{L^{\infty}(\Omega)}.
    \end{equation}
    The first term can be estimated by \eqref{eq:approximation_properties_of_S_res2}
    in Theorem~\ref{thm:approximation_properties_of_S}.
    For the second term, notice that $u_p - u_S \in S$ and
    \begin{equation}
        \label{eq:pointwise_of_p_version_before_inverse}
        \norm{u_p-u_S}_{L^{\infty}(\Omega)}
        = \max_{i=1,\dotsc,N} \norm{u_p - u_S}_{L^{\infty}(E_i)}
        = \max_{i=1,\dotsc,N}
            \norm{\hat{u}_p^{(i)} - \hat{u}_S^{(i)}}_{L^{\infty}(\widehat{E}_i)},
    \end{equation}
    where $\hat{u}_p^{(i)} = u_p \circ F_i \in \mathcal{P}_p(\widehat{E}_i)$
    and $\hat{u}_S^{(i)} = u_S \circ F_i \in \mathcal{P}_p(\widehat{E}_i)$.

    By \cite[Theorem~4.76 on p.~208]{schwab1998} (covers both quadrilaterals and triangles)
    or \cite[Proposition~3.1]{boillat1997} (covers only triangles),
    the polynomials $\hat{u}_p^{(i)} - \hat{u}_S^{(i)} \in \mathcal{P}_p(\widehat{E}_i)$
    satisfy the inverse inequality
    \begin{equation}
        \label{eq:pointwise_of_p_version_inverse}
        \norm{\hat{u}_p^{(i)} - \hat{u}_S^{(i)}}_{L^{\infty}(\widehat{E}_i)}
        \leq C \sqrt{\ln(p+1)}
            \norm{\hat{u}_p^{(i)} - \hat{u}_S^{(i)}}_{H^1(\widehat{E}_i)},
    \end{equation}
    where the constant $C > 0$ is independent of $u$ and $p$.
    
    Define $\hat{u}^{(i)} = u \circ F_i \in H^2(\widehat{E}_i)$ for all $i=1,2,\dotsc,N$.
    The $H^1$-norm on the right-hand side of \eqref{eq:pointwise_of_p_version_inverse}
    can be estimated by
    \begin{align}
        \norm{\hat{u}_p^{(i)} - \hat{u}_S^{(i)}}_{H^1(\widehat{E}_i)}
        &\leq \norm{\hat{u}_p^{(i)} - \hat{u}^{(i)}}_{H^1(\widehat{E}_i)}
            + \norm{\hat{u}^{(i)} - \hat{u}_S^{(i)}}_{H^1(\widehat{E}_i)} \nonumber \\
        &\leq C \norm{u_p - u}_{H^1(E_i)} + C \norm{u - u_S}_{H^1(E_i)} \nonumber \\
        &\leq C \norm{u_p - u}_{H^1(\Omega)} + C \norm{u - u_S}_{H^1(\Omega)} \nonumber \\
        \label{eq:pointwise_of_p_version_inverse_refined}
        &\leq C p^{-1} \norm{u}_{H^2(\Omega)},
    \end{align}
    where we used the equivalence of the norms $\norm{\cdot}_{H^1(\widehat{E}_i)}$ and
    $\norm{\cdot}_{H^1(E_i)}$ under the change of variables $F_i: \widehat{E}_i \to E_i$ and then
    Theorems \ref{thm:approximation_properties_of_S} and \ref{thm:H1_convergence_of_p_version}.

    Substituting \eqref{eq:pointwise_of_p_version_inverse_refined} into
    \eqref{eq:pointwise_of_p_version_inverse} and then
    \eqref{eq:pointwise_of_p_version_inverse} into
    \eqref{eq:pointwise_of_p_version_before_inverse}, the second term in
    \eqref{eq:pointwise_convergence_of_p_fem_init_split} has the bound
    \begin{equation*}
        \norm{u_p-u_S}_{L^{\infty}(\Omega)}
        \leq  C p^{-1} \sqrt{\ln(p+1)} \norm{u}_{H^2(\Omega)}.
    \end{equation*}
    We now arrive at the desired estimate:
    \begin{align*}
        \norm{u-u_S}_{L^{\infty}(\Omega)}
        &\leq \norm{u-u_p}_{L^{\infty}(\Omega)}
            + \norm{u_p-u_S}_{L^{\infty}(\Omega)} \\
        &\leq C p^{-1} \norm{u}_{H^2(\Omega)}
            + C p^{-1} \sqrt{\ln(p+1)} \norm{u}_{H^2(\Omega)} \\
        &\leq C p^{-1} \left( 1 + \sqrt{\ln(p+1)} \right) \norm{u}_{H^2(\Omega)}.
    \end{align*}
\end{proof}

\clearpage

\section{\texorpdfstring{$p$}{p}- Finite Element Method for Poisson's Equation with a Dirac Delta Load}
\label{sec:finite_element_solutions_with_a_concentrated_load}

We are now ready to consider the $L^2$ convergence of the
$p$- finite element method for the problem
\begin{equation}
    \label{eq:poissons_eq_with_dirac_delta_recap_sec5}
    \left\{
        \begin{aligned}
            -\Delta u &= \delta_{x_0} && \text{in } \Omega \\
            u &= g_j && \text{on } \Gamma_j, \quad j \in D \\
            \frac{\partial u}{\partial n} &= g_j && \text{on } \Gamma_j,
            \quad j \in N,
        \end{aligned}
    \right.
\end{equation}
where $\Omega \subset \mathbb{R}^2$ is a bounded polygonal domain, $\delta_{x_0}$
is the Dirac delta functional for an arbitrary point $x_0 \in \Omega$ and
the boundary data satisfy $g_j \in T(H^2(\Omega))$ for all $j \in D$ and
$g_j \in T(H^1(\Omega))$ for all $j \in N$. If $D = \varnothing$,
the Neumann boundary data are assumed to satisfy the usual compatibility condition.
It is also assumed that the domain $\Omega$ satisfies Assumption \ref{ass:regular_polygonal_domain}.
Uniqueness of the solution for the pure Neumann problem
is again enforced with the zero mean value requirement.
We consider the $L^2$ convergence both analytically and numerically.

\subsection{An \texorpdfstring{$L^2$}{L2} Error Estimate}
\label{subsec:p_fem_L2_convergence_analytically}

Casas \cite{casas1985} provides an $L^2$ error estimate for the $h$-version
when applied to the pure Dirichlet variant of the problem
\eqref{eq:poissons_eq_with_dirac_delta_recap_sec5} with homogeneous boundary conditions.
The same proof strategy can be used to consider the $p$-version as well,
and for that we will need the error estimates in the previous section.
We consider both Dirichlet and Neumann boundary conditions,
and we also assume that the Dirichlet conditions are homogeneous to simplify the proof.
As in Section~\ref{sec:poissons_equation_in_a_polygon},
a problem with non-homogeneous Dirichlet boundary conditions could be transformed
into a new problem with homogeneous Dirichlet boundary conditions to which
the finite element method could then be applied.
\begin{theorem}
    \label{thm:L2_convergence_of_p_version_dirac_load}
    Consider the problem \eqref{eq:poissons_eq_with_dirac_delta_recap_sec5}
    with homogeneous Dirichlet boundary conditions.
    Assume that the domain $\Omega$ satisfies Assumption~\ref{ass:regular_polygonal_domain}.
    Let $u \in W^{1,s}(\Omega)$, where $s \in (1,2)$, denote the exact solution, and
    let $u_S \in S(\Omega,\mathcal{M},p)$ denote the finite element solution.
    Then there exists a constant $C > 0$ independent of $p$ such that
    \begin{equation*}
        \norm{u - u_S}_{L^2(\Omega)} \leq C p^{-1} \left( 1 + \sqrt{\ln(p+1)} \right).
    \end{equation*}
\end{theorem}
\begin{proof}
    By the Sobolev imbedding theorem, $u \in L^2(\Omega)$.
    Thus, $u - u_S \in L^2(\Omega)$. Consider now the weak formulation of the problem
    \begin{equation}
        \label{eq:dirac_L2_dual_problem}
        \left\{
            \begin{aligned}
                -\Delta v &= u - u_S && \text{in } \Omega \\
                v &= 0 && \text{on } \Gamma_j, \quad j \in D \\
                \frac{\partial v}{\partial n} &= 0 && \text{on } \Gamma_j,
                \quad j \in N.
            \end{aligned}
        \right.
    \end{equation}
    By Theorem~\ref{thm:weak_poisson_is_solvable}, the problem has a unique solution $v$,
    and by Theorem~\ref{thm:H2_regularity} and Theorem~\ref{thm:weak_solution_is_strong_solution},
    it holds that $v \in H^2(\Omega)$ and $-\Delta v = u-u_S$ almost everywhere in $\Omega$.
    Moreover, by Theorem~\ref{thm:a_priori_H2_estimate}, the solution $v$ satisfies the estimate
    \begin{equation}
        \label{eq:dirac_L2_conv_dual_apriori}
        \norm{v}_{H^2(\Omega)} \leq C \norm{u - u_S}_{L^2(\Omega)},
    \end{equation}
    where the constant $C > 0$ is independent of $u$ and $p$.

    We may use the function $v$ to rewrite $\norm{u - u_S}_{L^2(\Omega)}^2$.
    Substituting $u - u_S = -\Delta v$ and integrating by parts gives
    \begin{align}
        \norm{u - u_S}_{L^2(\Omega)}^2
        &= \int_{\Omega} \abs{u - u_S}^2 \diff x \nonumber \\
        &= -\int_{\Omega} (u - u_S) \Delta v \diff x \nonumber \\
        &= \int_{\Omega} \nabla (u-u_S) \cdot \nabla v \diff x
        - \int_{\partial \Omega} \frac{\partial v}{\partial n} (u - u_S) \diff S
            \nonumber \\
        \label{eq:dirac_L2_first_steps}
        &= \int_{\Omega} \nabla u \cdot \nabla v \diff x
            - \int_{\Omega} \nabla u_S \cdot \nabla v \diff x.
    \end{align}
    For the last equality, note that $u - u_S = 0$ on $\Gamma_j$ for all $j \in D$
    and $\partial v / \partial n = 0$ on $\Gamma_j$ for all $j \in N$.

    Since $v \in H^2(\Omega)$, the Sobolev imbedding theorem implies that
    $v \in W^{1,s'}(\Omega)$, where $s' \in (2, \infty)$ is the conjugate exponent of $s$.
    Moreover, $v = 0$ on $\Gamma_j$ for all $j \in D$.
    Hence, by the definition of a weak solution to the problem
    \eqref{eq:poissons_eq_with_dirac_delta_recap_sec5}, the first integral in
    \eqref{eq:dirac_L2_first_steps} can be written as
    \begin{equation}
        \label{eq:dirac_L2_est_v_test_fn}
        \int_{\Omega} \nabla u \cdot \nabla v \diff x
        = v(x_0) + \sum_{j \in N} \int_{\Gamma_j} g_j v \diff S.
    \end{equation}

    Let $v_S \in S$ denote the finite element solution to the problem
    \eqref{eq:dirac_L2_dual_problem}. The second integral in \eqref{eq:dirac_L2_first_steps}
    can now be written as
    \begin{equation}
        \label{eq:dirac_L2_est_second_int}
        \int_{\Omega} \nabla u_S \cdot \nabla v \diff x
        = \int_{\Omega} \nabla u_S \cdot \nabla v_S \diff x
        = v_S(x_0) + \sum_{j \in N} \int_{\Gamma_j} g_j v_S \diff S.
    \end{equation}
    The first equality follows the Galerkin orthogonality between $v$ and $v_S$.
    The second equality follows from the definition of $u_S$ as the finite element solution
    to the problem \eqref{eq:poissons_eq_with_dirac_delta_recap_sec5}.

    Substituting \eqref{eq:dirac_L2_est_v_test_fn} and \eqref{eq:dirac_L2_est_second_int}
    into \eqref{eq:dirac_L2_first_steps} and applying Hölder's inequality and
    the trace theorem yield
    \begin{align*}
        \norm{u - u_S}_{L^2(\Omega)}^2
        &= v(x_0) - v_S(x_0) + \sum_{j \in N} \int_{\Gamma_j} g_j (v - v_S) \diff S \\
        &\leq \norm{v - v_S}_{L^{\infty}(\Omega)}
            + \sum_{j \in N} \norm{g_j}_{L^2(\Gamma_j)} \norm{v - v_S}_{L^2(\Gamma_j)} \\
        &\leq \norm{v - v_S}_{L^{\infty}(\Omega)}
            + C \norm{v - v_S}_{H^1(\Omega)}.
    \end{align*}
    The errors $\norm{v - v_S}_{L^{\infty}(\Omega)}$ and $\norm{v - v_S}_{H^1(\Omega)}$
    can be further estimated by Theorem~\ref{thm:pointwise_convergence_of_p_version},
    Theorem~\ref{thm:H1_convergence_of_p_version} and
    the estimate \eqref{eq:dirac_L2_conv_dual_apriori}:
    \begin{align*}
        \norm{u - u_S}_{L^2(\Omega)}^2
        &\leq C p^{-1} \left( 1 + \sqrt{\ln(p+1)} \right) \norm{v}_{H^2(\Omega)}
            + C p^{-1} \norm{v}_{H^2(\Omega)} \\
        &\leq C p^{-1} \left( 1 + \sqrt{\ln(p+1)} \right) \norm{u - u_S}_{L^2(\Omega)}.
    \end{align*}
    Dividing both sides by $\norm{u - u_S}_{L^2(\Omega)}$ completes the proof.
\end{proof}

\subsection{Numerical Results}
\label{subsec:p_fem_numerical_L2_convergence}

Let us investigate the accuracy of the error estimate in
Theorem~\ref{thm:L2_convergence_of_p_version_dirac_load} through numerical experiments.
We consider the pure Neumann variant of the problem 
\eqref{eq:poissons_eq_with_dirac_delta_recap_sec5} in the domain $\Omega = (-1,1)^2$
with several locations for the load $x_0$. The boundary conditions are chosen such that
an exact solution is given by the Green's function
\begin{equation*}
    u(x) = -\frac{1}{2 \pi} \ln \abs{x - x_0}.
\end{equation*}
The exact solution and the finite element solution are defined up to an additive constant.
Thus, they are normalized to have zero mean value over the domain $\Omega$
to fix the solutions.

We consider two meshes: one that consists of only quadrilaterals and
one that consists of only triangles. For both meshes, we consider three locations
for the Dirac delta load: load at a vertex, on a side and in the interior of an element.
The different configurations are illustrated in Figure~\ref{fig:numerical_results_meshes_and_load}.
The exact locations of the load are $(0,0)$, $(1/4,0)$ and $(1/4,1/4)$
for the quadrilateral mesh and $(0,0)$, $(1/2,0)$ and $(1/3,1/3)$
for the triangle mesh.
We consider the product and trunk polynomial spaces for both meshes
such that a given configuration uses the same polynomial space for all elements of the mesh.

\begin{figure}[b]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \resizebox{0.85\textwidth}{!}{
            \begin{tikzpicture}
                \draw (0,0) grid (4,4);
                \node[circle, draw=none, fill=red, inner sep=0pt, minimum size=4pt] (a) at (2,2) {};
                \node[circle, draw=none, fill=red, inner sep=0pt, minimum size=4pt] (b) at (2.5,2) {};
                \node[circle, draw=none, fill=red, inner sep=0pt, minimum size=4pt] (c) at (2.5,2.5) {};
            \end{tikzpicture}
        }
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \resizebox{0.85\textwidth}{!}{
            \begin{tikzpicture}
                \draw[step=2cm] (0,0) grid (4,4);
                \draw (2,0) -- (0,2);
                \draw (4,0) -- (0,4);
                \draw (4,2) -- (2,4);
                \node[circle, draw=none, fill=red, inner sep=0pt, minimum size=4pt] (aa) at (2,2) {};
                \node[circle, draw=none, fill=red, inner sep=0pt, minimum size=4pt] (bb) at (3,2) {};
                \node[circle, draw=none, fill=red, inner sep=0pt, minimum size=4pt] (cc) at (2.66,2.66) {};
            \end{tikzpicture}
        }
    \end{subfigure}
    \caption{Meshes and locations of the Dirac delta load for the domain $\Omega = (-1,1)^2.$}
    \label{fig:numerical_results_meshes_and_load}
\end{figure}

The numerical error caused by finite-precision floating-point arithmetic
has been tried to be kept close to a minimum by performing most of the computations exactly
in rational number arithmetic with the GNU Multiple Precision Arithmetic Library (GMP) \cite{gmp}.
For example, the stiffness matrix has been assembled exactly with no numerical error
by using the shape polynomials presented in Section~\ref{subsec:fem_basis_functions}.
After the elimination of one row and column from the linear system,
as described at the end of Section~\ref{subsec:fem_basis_functions}
for solving the Neumann problem, the resulting linear system is also solved exactly
by using the Eigen software library \cite{eigen} in combination with the GMP library.
The only sources of numerical error are the computation of the load vector
due to the Neumann boundary integrals and the computation of the error
$\norm{u - u_S}_{L^2(\Omega)}$. Both of those have been approximated with the
Gauss-Legendre quadrature rule for which we have used precomputed weights and abscissae
with double precision provided by the GNU Scientific Library \cite{gsl}.
Quadrature over the reference triangle has been implemented by mapping the quadrature points
for the reference quadrilateral to the reference triangle and
then mitigating the crowding of points caused by such a mapping,
see \cite{Hussain2012AppropriateGQ} for more details.

Figures \ref{fig:L2_errors_node}, \ref{fig:L2_errors_side} and \ref{fig:L2_errors_int} below
show the error $\norm{u - u_S}_{L^2(\Omega)}$ as a function of $p \in \{ 1,2,\dotsc,10 \}$
in log-log scale when the Dirac delta load is located at a vertex, on a side and
in the interior of an element, respectively. Each figure shows the errors for both meshes
and for both product and trunk spaces.
The estimated error rate in Theorem~\ref{thm:L2_convergence_of_p_version_dirac_load}
has also been plotted for comparison.
The constant $C$ in Theorem~\ref{thm:L2_convergence_of_p_version_dirac_load}
is not known, but note that changing it only shifts the error curve
vertically when plotted in log-log scale. The constant is simply chosen so that
the estimated error curve is close to the observed errors.

\begin{figure}[b]
    \centering
    \begin{subfigure}[t]{0.475\textwidth}
        \centering
        \begin{tikzpicture}[baseline, trim axis left, trim axis right]
        \begin{loglogaxis}[
            log basis x=10,
            log basis y=10,
            width=0.9\textwidth,
            scale only axis=true,
            xmin=1, xmax=15,
            scale mode=stretch to fill,
            axis x line=bottom,
            axis y line=left,
            enlarge x limits=true,
            enlarge y limits=true,
            clip=false,
            xtick={1,2,5,10,15},
            extra x ticks={3,4,6,7,8,9,11,12,13,14},
            extra x tick labels={},
            xticklabel=\pgfmathparse{10^\tick}\pgfmathprintnumber{\pgfmathresult},
            tick label style={font=\footnotesize},
            legend style={font=\footnotesize},
            ticklabel shift=-0.5mm,
            %legend pos=south west,
            ymin=0.00085, ymax=0.05,
        ]
            \addplot [
                domain=0.95:10.5,
                samples=100,
                no markers,
                color=red,
            ] {0.022 * x^(-1) * (sqrt(ln(x+1)) + 1)};
            \addlegendentry{Est.~error rate}

            \pgfplotsset{cycle list shift=-1}
            \addplot+ [
                color=green!80!black,
                mark=*,
                mark options={fill=green!80!black, solid},
            ] table [
                header=false,
                row sep=\\,
            ] {
                1 0.02479509449669409\\
                2 0.01205957183561734\\
                3 0.006937940948503363\\
                4 0.004525361590227782\\
                5 0.003185164386823731\\
                6 0.002362701435560289\\
                7 0.001821627360055559\\
                8 0.001446813515814258\\
                9 0.001176564430520859\\
                10 0.0009753611993738186\\
            };
            \addlegendentry{Product space}
            
            \addplot [
                draw=none,
                forget plot,
            ] table [
                header=false,
                row sep=\\,
                y={create col/linear regression={y=[index]1}},
            ] {
                7 0.001821627360055559\\
                8 0.001446813515814258\\
                9 0.001176564430520859\\
                10 0.0009753611993738186\\
            };
            \xdef\productslope{\pgfplotstableregressiona} % -1.75063
            \xdef\productconst{\pgfplotstableregressionb} % -1.25949
            %\node at (axis cs:1.5,0.005) {\pgfmathprintnumber[fixed, precision=5]{\productslope}};
            %\node at (axis cs:1.5,0.002) {\pgfmathprintnumber[fixed, precision=5]{\productconst}};
            \addplot [
                color=black,
                forget plot,
                domain=7:12,
                yshift=2.2mm,
                dashed,
            ] {10^(-1.25949) * x^(-1.75063)}
                coordinate [pos=0.4] (productA)
                coordinate [pos=0.8] (productB)
            ;
            \draw (productA) -| (productB)
                node [
                    pos=0.75,
                    anchor=west,
                    xshift=-0.5mm,
                ] {\scriptsize\pgfmathprintnumber{\productslope}};
            
            \pgfplotsset{cycle list shift=-1}
            \addplot+ [
                color=blue,
                mark=square*,
                mark options={fill=blue!80!black, solid},
            ] table [
                header=false,
                row sep=\\,
            ] {
                1 0.02479509449669409\\
                2 0.01477906988761147\\
                3 0.01341363509244312\\
                4 0.01029121820276125\\
                5 0.00773500763122347\\
                6 0.006175261984337032\\
                7 0.005096606957798642\\
                8 0.004321812781802889\\
                9 0.003736164302975072\\
                10 0.003276763878633516\\
            };
            \addlegendentry{Trunk space}

            \addplot [
                draw=none,
                forget plot,
            ] table [
                header=false,
                row sep=\\,
                y={create col/linear regression={y=[index]1}},
            ] {
                7 0.005096606957798642\\
                8 0.004321812781802889\\
                9 0.003736164302975072\\
                10 0.003276763878633516\\
            };
            \xdef\trunkslope{\pgfplotstableregressiona} % -1.23795
            \xdef\trunkconst{\pgfplotstableregressionb} % -1.24646
            %\node at (axis cs:1.5,0.005) {\pgfmathprintnumber[fixed, precision=5]{\trunkslope}};
            %\node at (axis cs:1.5,0.002) {\pgfmathprintnumber[fixed, precision=5]{\trunkconst}}; 
            \addplot [
                color=black,
                forget plot,
                domain=7:12,
                yshift=2.2mm,
                dashed,
            ] {10^(-1.24646) * x^(-1.23795)}
                coordinate [pos=0.4] (trunkA)
                coordinate [pos=0.8] (trunkB)
            ;
            \draw (trunkA) -| (trunkB)
                node [
                    pos=0.75,
                    anchor=west,
                    xshift=-0.5mm,
                ] {\scriptsize\pgfmathprintnumber{\trunkslope}};
        \end{loglogaxis}
        \end{tikzpicture}
        \caption{Quadrilateral mesh; $x_0 = (0,0)$.}
        \label{fig:L2_errors_node_a}
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[t]{0.475\textwidth}  
        \centering 
        \begin{tikzpicture}[baseline, trim axis left, trim axis right]
        \begin{loglogaxis}[
            log basis x=10,
            log basis y=10,
            width=0.9\textwidth,
            scale only axis=true,
            xmin=1, xmax=15,
            scale mode=stretch to fill,
            axis x line=bottom,
            axis y line=left,
            enlarge x limits=true,
            enlarge y limits=true,
            clip=false,
            xtick={1,2,5,10,15},
            extra x ticks={3,4,6,7,8,9,11,12,13,14},
            extra x tick labels={},
            xticklabel=\pgfmathparse{10^\tick}\pgfmathprintnumber{\pgfmathresult},
            tick label style={font=\footnotesize},
            legend style={font=\footnotesize},
            ticklabel shift=-0.5mm,
            %legend pos=south west,
        ]
            \addplot [
                domain=0.95:10.5,
                samples=100,
                no markers,
                color=red,
            ] {0.037 * x^(-1) * (sqrt(ln(x+1)) + 1)};
            \addlegendentry{Est.~error rate}

            \pgfplotsset{cycle list shift=-1}
            \addplot+ [
                color=green!80!black,
                mark=*,
                mark options={fill=green!80!black, solid},
            ] table [
                header=false,
                row sep=\\,
            ] {
                1 0.059465155576505\\
                2 0.03345696267175907\\
                3 0.02068262576545919\\
                4 0.01443976592673438\\
                5 0.01072674989269077\\
                6 0.008307817199505577\\
                7 0.006672113858392402\\
                8 0.005496610573095799\\
                9 0.004624433490083547\\
                10 0.003955029061766184\\
            };
            \addlegendentry{Product space}

            \addplot [
                draw=none,
                forget plot,
            ] table [
                header=false,
                row sep=\\,
                y={create col/linear regression={y=[index]1}},
            ] {
                7 0.006672113858392402\\
                8 0.005496610573095799\\
                9 0.004624433490083547\\
                10 0.003955029061766184\\
            };
            \xdef\productslope{\pgfplotstableregressiona} % -1.46574
            \xdef\productconst{\pgfplotstableregressionb} % -0.9367
            %\node at (axis cs:1.5,0.005) {\pgfmathprintnumber[fixed, precision=5]{\productslope}};
            %\node at (axis cs:1.5,0.002) {\pgfmathprintnumber[fixed, precision=5]{\productconst}};
            \addplot [
                color=black,
                forget plot,
                domain=7:12,
                xshift=-2.2mm,
                dashed,
            ] {10^(-0.9367) * x^(-1.46574)}
                coordinate [pos=0.4] (productA)
                coordinate [pos=0.8] (productB)
            ;
            \draw (productA) |- (productB)
                node [
                    pos=0.25,
                    anchor=east,
                    xshift=0.5mm,
                ] {\scriptsize\pgfmathprintnumber{\productslope}};
            
            \pgfplotsset{cycle list shift=-1}
            \addplot+ [
                color=blue,
                mark=square*,
                mark options={fill=blue!80!black, solid},
            ] table [
                header=false,
                row sep=\\,
            ] {
                1 0.059465155576505\\
                2 0.03199959823137655\\
                3 0.02027047649615876\\
                4 0.01451480794574113\\
                5 0.01105948833582298\\
                6 0.008803220898216417\\
                7 0.007229728348497225\\
                8 0.006079040330631525\\
                9 0.005206532450382613\\
                10 0.004525825926829712\\
            };
            \addlegendentry{Trunk space}

            \addplot [
                draw=none,
                forget plot,
            ] table [
                header=false,
                row sep=\\,
                y={create col/linear regression={y=[index]1}},
            ] {
                7 0.007229728348497225\\
                8 0.006079040330631525\\
                9 0.005206532450382613\\
                10 0.004525825926829712\\
            };
            \xdef\trunkslope{\pgfplotstableregressiona} % -1.31247
            \xdef\trunkconst{\pgfplotstableregressionb} % -1.0314
            %\node at (axis cs:1.5,0.005) {\pgfmathprintnumber[fixed, precision=5]{\trunkslope}};
            %\node at (axis cs:1.5,0.002) {\pgfmathprintnumber[fixed, precision=5]{\trunkconst}}; 
            \addplot [
                color=black,
                forget plot,
                domain=7:12,
                yshift=2.4mm,
                dashed,
            ] {10^(-1.0314) * x^(-1.31247)}
                coordinate [pos=0.4] (trunkA)
                coordinate [pos=0.8] (trunkB)
            ;
            \draw (trunkA) -| (trunkB)
                node [
                    pos=0.75,
                    anchor=west,
                    xshift=-0.5mm,
                ] {\scriptsize\pgfmathprintnumber{\trunkslope}};
        \end{loglogaxis}
        \end{tikzpicture}
        \caption{Triangle mesh; $x_0 = (0,0)$.}
        \label{fig:L2_errors_node_b}
    \end{subfigure}
    \caption{Log-log plots of $\norm{u - u_S}_{L^2(\Omega)}$ vs.~$p$ for the quadrilateral and triangle meshes when the load is at a vertex. The slope triangles are used to estimate the exact convergence rates, and they have been computed via linear regression.}
    \label{fig:L2_errors_node}
\end{figure}

To quantify the exact observed error rates,
there are regression lines and their slopes next to each observed error curve.
The lines have been obtained via the method of least squares
from some of the last observed data points, and they have been shifted slightly for clarity.
Note that a linear error curve in log-log scale corresponds to the error bound
\begin{align*}
    \norm{u - u_S}_{L^2(\Omega)} \leq C p^{-k},
\end{align*}
where $C > 0$ is some constant independent of $p$ and $k > 0$ is the absolute value of the slope.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.475\textwidth}
        \centering
        \begin{tikzpicture}[baseline, trim axis left, trim axis right]
        \begin{loglogaxis}[
            log basis x=10,
            log basis y=10,
            width=0.9\textwidth,
            scale only axis=true,
            xmin=1, xmax=15,
            scale mode=stretch to fill,
            axis x line=bottom,
            axis y line=left,
            enlarge x limits=true,
            enlarge y limits=true,
            clip=false,
            xtick={1,2,5,10,15},
            extra x ticks={3,4,6,7,8,9,11,12,13,14},
            extra x tick labels={},
            xticklabel=\pgfmathparse{10^\tick}\pgfmathprintnumber{\pgfmathresult},
            tick label style={font=\footnotesize},
            legend style={font=\footnotesize},
            ticklabel shift=-0.5mm,
            %legend pos=south west,
        ]
            \addplot [
                domain=0.95:10.5,
                samples=100,
                no markers,
                color=red,
            ] {0.033 * x^(-1) * (sqrt(ln(x+1)) + 1)};
            \addlegendentry{Est.~error rate}

            \pgfplotsset{cycle list shift=-1}
            \addplot+ [
                color=green!80!black,
                mark=*,
                mark options={fill=green!80!black, solid},
            ] table [
                header=false,
                row sep=\\,
            ] {
                1 0.04995546520391592\\
                2 0.0165475506966627\\
                3 0.0161116812391616\\
                4 0.009806111166454922\\
                5 0.009795598873071866\\
                6 0.007035572774180134\\
                7 0.007036785549190836\\
                8 0.005489019432436235\\
                9 0.00548952548205688\\
                10 0.004499494156192432\\
            };
            \addlegendentry{Product space}
            
            \addplot [
                draw=none,
                forget plot,
            ] table [
                header=false,
                row sep=\\,
                y={create col/linear regression={y=[index]1}},
            ] {
                3 0.0161116812391616\\
                5 0.009795598873071866\\
                7 0.007036785549190836\\
                9 0.00548952548205688\\
            };
            \xdef\productslope{\pgfplotstableregressiona} % -0.97989
            \xdef\productconst{\pgfplotstableregressionb} % -1.32486
            %\node at (axis cs:1.5,0.005) {\pgfmathprintnumber[fixed, precision=5]{\productslope}};
            %\node at (axis cs:1.5,0.002) {\pgfmathprintnumber[fixed, precision=5]{\productconst}};
            \addplot [
                color=black,
                forget plot,
                domain=7:12,
                xshift=-2.5mm,
                yshift=-2mm,
                dashed,
            ] {10^(-1.32486) * x^(-0.97989)}
                coordinate [pos=0.4] (productA)
                coordinate [pos=0.8] (productB)
            ;
            \draw (productA) |- (productB)
                node [
                    pos=0.25,
                    anchor=east,
                    xshift=0.5mm,
                ] {\scriptsize\pgfmathprintnumber{\productslope}};
            
            \pgfplotsset{cycle list shift=-1}
            \addplot+ [
                color=blue,
                mark=square*,
                mark options={fill=blue!80!black, solid},
            ] table [
                header=false,
                row sep=\\,
            ] {
                1 0.04995546520391592\\
                2 0.020390867639844\\
                3 0.02038812181995322\\
                4 0.01288356316899283\\
                5 0.01215965174444617\\
                6 0.009298758352300547\\
                7 0.008635901772759532\\
                8 0.007155759126059213\\
                9 0.006661429018379623\\
                10 0.005776258054232616\\
            };
            \addlegendentry{Trunk space}

            \addplot [
                draw=none,
                forget plot,
            ] table [
                header=false,
                row sep=\\,
                y={create col/linear regression={y=[index]1}},
            ] {
                3 0.02038812181995322\\
                5 0.01215965174444617\\
                7 0.008635901772759532\\
                9 0.006661429018379623\\
            };
            \xdef\trunkslope{\pgfplotstableregressiona} % -1.01753
            \xdef\trunkconst{\pgfplotstableregressionb} % -1.2046
            %\node at (axis cs:1.5,0.005) {\pgfmathprintnumber[fixed, precision=5]{\trunkslope}};
            %\node at (axis cs:1.5,0.002) {\pgfmathprintnumber[fixed, precision=5]{\trunkconst}}; 
            \addplot [
                color=black,
                forget plot,
                domain=7:12,
                yshift=2mm,
                dashed,
            ] {10^(-1.2046) * x^(-1.01753)}
                coordinate [pos=0.4] (trunkA)
                coordinate [pos=0.8] (trunkB)
            ;
            \draw (trunkA) -| (trunkB)
                node [
                    pos=0.75,
                    anchor=west,
                    xshift=-0.5mm,
                ] {\scriptsize\pgfmathprintnumber{\trunkslope}};
        \end{loglogaxis}
        \end{tikzpicture}
        \caption{Quadrilateral mesh; $x_0 = (1/4,0)$.}
        \label{fig:L2_errors_side_a}
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[t]{0.475\textwidth}  
        \centering 
        \begin{tikzpicture}[baseline, trim axis left, trim axis right]
        \begin{loglogaxis}[
            log basis x=10,
            log basis y=10,
            width=0.9\textwidth,
            scale only axis=true,
            xmin=1, xmax=15,
            scale mode=stretch to fill,
            axis x line=bottom,
            axis y line=left,
            enlarge x limits=true,
            enlarge y limits=true,
            clip=false,
            xtick={1,2,5,10,15},
            extra x ticks={3,4,6,7,8,9,11,12,13,14},
            extra x tick labels={},
            xticklabel=\pgfmathparse{10^\tick}\pgfmathprintnumber{\pgfmathresult},
            tick label style={font=\footnotesize},
            legend style={font=\footnotesize},
            ticklabel shift=-0.5mm,
            %legend pos=south west,
        ]
            \addplot [
                domain=0.95:10.5,
                samples=100,
                no markers,
                color=red,
            ] {0.059 * x^(-1) * (sqrt(ln(x+1)) + 1)};
            \addlegendentry{Est.~error rate}

            \pgfplotsset{cycle list shift=-1}
            \addplot+ [
                color=green!80!black,
                mark=*,
                mark options={fill=green!80!black, solid},
            ] table [
                header=false,
                row sep=\\,
            ] {
                1 0.09567332759801221\\
                2 0.03756678670488138\\
                3 0.0306736116832411\\
                4 0.01947769247341842\\
                5 0.01890050566581389\\
                6 0.01379854771860367\\
                7 0.01366697361296496\\
                8 0.01077934932737372\\
                9 0.01071553621962116\\
                10 0.008855024831591972\\
            };
            \addlegendentry{Product space}

            \addplot [
                draw=none,
                forget plot,
            ] table [
                header=false,
                row sep=\\,
                y={create col/linear regression={y=[index]1}},
            ] {
                5 0.01890050566581389\\
                7 0.01366697361296496\\
                9 0.01071553621962116\\
            };
            \xdef\productslope{\pgfplotstableregressiona} % -0.96536
            \xdef\productconst{\pgfplotstableregressionb} % -1.0487
            %\node at (axis cs:1.5,0.005) {\pgfmathprintnumber[fixed, precision=5]{\productslope}};
            %\node at (axis cs:1.5,0.002) {\pgfmathprintnumber[fixed, precision=5]{\productconst}};
            \addplot [
                color=black,
                forget plot,
                domain=7:12,
                yshift=2.5mm,
                dashed,
            ] {10^(-1.0487) * x^(-0.96536)}
                coordinate [pos=0.4] (productA)
                coordinate [pos=0.8] (productB)
            ;
            \draw (productA) -| (productB)
                node [
                    pos=0.75,
                    anchor=west,
                    xshift=-0.5mm,
                ] {\scriptsize\pgfmathprintnumber{\productslope}};
            
            \pgfplotsset{cycle list shift=-1}
            \addplot+ [
                color=blue,
                mark=square*,
                mark options={fill=blue!80!black, solid},
            ] table [
                header=false,
                row sep=\\,
            ] {
                1 0.09567332759801221\\
                2 0.04099051105715428\\
                3 0.03168232774592026\\
                4 0.02192080600792841\\
                5 0.01941910732802077\\
                6 0.0150782456690167\\
                7 0.0139815816887863\\
                8 0.01153088400530833\\
                9 0.01092112976544507\\
                10 0.00934630734891713\\
            };
            \addlegendentry{Trunk space}
        \end{loglogaxis}
        \end{tikzpicture}
        \caption{Triangle mesh; $x_0 = (1/2,0)$.}
        \label{fig:L2_errors_side_b}
    \end{subfigure}
    \caption{Same as Figure~\ref{fig:L2_errors_node} but when the load is on a side.}
    \label{fig:L2_errors_side}
\end{figure}

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.475\textwidth}
        \centering
        \begin{tikzpicture}[baseline, trim axis left, trim axis right]
        \begin{loglogaxis}[
            log basis x=10,
            log basis y=10,
            width=0.9\textwidth,
            scale only axis=true,
            xmin=1, xmax=15,
            scale mode=stretch to fill,
            axis x line=bottom,
            axis y line=left,
            enlarge x limits=true,
            enlarge y limits=true,
            clip=false,
            xtick={1,2,5,10,15},
            extra x ticks={3,4,6,7,8,9,11,12,13,14},
            extra x tick labels={},
            xticklabel=\pgfmathparse{10^\tick}\pgfmathprintnumber{\pgfmathresult},
            tick label style={font=\footnotesize},
            legend style={font=\footnotesize},
            ticklabel shift=-0.5mm,
            %legend pos=south west,
        ]
            \addplot [
                domain=0.95:10.5,
                samples=100,
                no markers,
                color=red,
            ] {0.049 * x^(-1) * (sqrt(ln(x+1)) + 1)};
            \addlegendentry{Est.~error rate}

            \pgfplotsset{cycle list shift=-1}
            \addplot+ [
                color=green!80!black,
                mark=*,
                mark options={fill=green!80!black, solid},
            ] table [
                header=false,
                row sep=\\,
            ] {
                1 0.06570055593692038\\
                2 0.02101773916131866\\
                3 0.02100772957972314\\
                4 0.01265901695982992\\
                5 0.01265834204141346\\
                6 0.009060870476057304\\
                7 0.009060847498057639\\
                8 0.007056129094398674\\
                9 0.00705612613708387\\
                10 0.005777897788772907\\
            };
            \addlegendentry{Product space}
            
            \addplot [
                draw=none,
                forget plot,
            ] table [
                header=false,
                row sep=\\,
                y={create col/linear regression={y=[index]1}},
            ] {
                3 0.02100772957972314\\
                5 0.01265834204141346\\
                7 0.009060847498057639\\
                9 0.00705612613708387\\
            };
            \xdef\productslope{\pgfplotstableregressiona} % -0.99301
            \xdef\productconst{\pgfplotstableregressionb} % -1.20375
            %\node at (axis cs:1.5,0.005) {\pgfmathprintnumber[fixed, precision=5]{\productslope}};
            %\node at (axis cs:1.5,0.002) {\pgfmathprintnumber[fixed, precision=5]{\productconst}};
            \addplot [
                color=black,
                forget plot,
                domain=7:12,
                xshift=-2.5mm,
                yshift=-2mm,
                dashed,
            ] {10^(-1.20375) * x^(-0.99301)}
                coordinate [pos=0.4] (productA)
                coordinate [pos=0.8] (productB)
            ;
            \draw (productA) |- (productB)
                node [
                    pos=0.25,
                    anchor=east,
                    xshift=0.5mm,
                ] {\scriptsize\pgfmathprintnumber{\productslope}};
            
            \pgfplotsset{cycle list shift=-1}
            \addplot+ [
                color=blue,
                mark=square*,
                mark options={fill=blue!80!black, solid},
            ] table [
                header=false,
                row sep=\\,
            ] {
                1 0.06570055593692038\\
                2 0.03052321557911691\\
                3 0.03051435168415995\\
                4 0.01768599429436616\\
                5 0.01768936609456072\\
                6 0.01256562010921341\\
                7 0.01252132945885964\\
                8 0.00979043003141083\\
                9 0.0097487844699635\\
                10 0.008021634266860853\\
            };
            \addlegendentry{Trunk space}

            \addplot [
                draw=none,
                forget plot,
            ] table [
                header=false,
                row sep=\\,
                y={create col/linear regression={y=[index]1}},
            ] {
                3 0.03051435168415995\\
                5 0.01768936609456072\\
                7 0.01252132945885964\\
                9 0.0097487844699635\\
            };
            \xdef\trunkslope{\pgfplotstableregressiona} % -1.0401
            \xdef\trunkconst{\pgfplotstableregressionb} % -1.02165
            %\node at (axis cs:1.5,0.005) {\pgfmathprintnumber[fixed, precision=5]{\trunkslope}};
            %\node at (axis cs:1.5,0.002) {\pgfmathprintnumber[fixed, precision=5]{\trunkconst}}; 
            \addplot [
                color=black,
                forget plot,
                domain=7:12,
                yshift=2mm,
                dashed,
            ] {10^(-1.02165) * x^(-1.0401)}
                coordinate [pos=0.4] (trunkA)
                coordinate [pos=0.8] (trunkB)
            ;
            \draw (trunkA) -| (trunkB)
                node [
                    pos=0.75,
                    anchor=west,
                    xshift=-0.5mm,
                ] {\scriptsize\pgfmathprintnumber{\trunkslope}};
        \end{loglogaxis}
        \end{tikzpicture}
        \caption{Quadrilateral mesh; $x_0 = (1/4,1/4)$.}
        \label{fig:L2_errors_int_a}
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[t]{0.475\textwidth}  
        \centering 
        \begin{tikzpicture}[baseline, trim axis left, trim axis right]
        \begin{loglogaxis}[
            log basis x=10,
            log basis y=10,
            width=0.9\textwidth,
            scale only axis=true,
            xmin=1, xmax=15,
            scale mode=stretch to fill,
            axis x line=bottom,
            axis y line=left,
            enlarge x limits=true,
            enlarge y limits=true,
            clip=false,
            xtick={1,2,5,10,15},
            extra x ticks={3,4,6,7,8,9,11,12,13,14},
            extra x tick labels={},
            xticklabel=\pgfmathparse{10^\tick}\pgfmathprintnumber{\pgfmathresult},
            tick label style={font=\footnotesize},
            legend style={font=\footnotesize},
            ticklabel shift=-0.5mm,
            %legend pos=south west,
        ]
            \addplot [
                domain=0.95:10.5,
                samples=100,
                no markers,
                color=red,
            ] {0.07 * x^(-1) * (sqrt(ln(x+1)) + 1)};
            \addlegendentry{Est.~error rate}

            \pgfplotsset{cycle list shift=-1}
            \addplot+ [
                color=green!80!black,
                mark=*,
                mark options={fill=green!80!black, solid},
            ] table [
                header=false,
                row sep=\\,
            ] {
                1 0.1048283953960827\\
                2 0.03781574459528789\\
                3 0.03099080623228074\\
                4 0.01941504657523501\\
                5 0.01728247252447582\\
                6 0.0133213933058835\\
                7 0.01235594745029816\\
                8 0.01024708782358557\\
                9 0.009659736288876713\\
                10 0.008329448472493633\\
            };
            \addlegendentry{Product space}

            \addplot [
                draw=none,
                forget plot,
            ] table [
                header=false,
                row sep=\\,
                y={create col/linear regression={y=[index]1}},
            ] {
                7 0.01235594745029816\\
                8 0.01024708782358557\\
                9 0.009659736288876713\\
                10 0.008329448472493633\\
            };
            \xdef\productslope{\pgfplotstableregressiona} % -1.0469
            \xdef\productconst{\pgfplotstableregressionb} % -1.02898
            %\node at (axis cs:1.5,0.005) {\pgfmathprintnumber[fixed, precision=5]{\productslope}};
            %\node at (axis cs:1.5,0.002) {\pgfmathprintnumber[fixed, precision=5]{\productconst}};
            \addplot [
                color=black,
                forget plot,
                domain=7:12,
                xshift=-2mm,
                yshift=-1mm,
                dashed,
            ] {10^(-1.02898) * x^(-1.0469)}
                coordinate [pos=0.4] (productA)
                coordinate [pos=0.8] (productB)
            ;
            \draw (productA) |- (productB)
                node [
                    pos=0.25,
                    anchor=east,
                    xshift=0.5mm,
                ] {\scriptsize\pgfmathprintnumber{\productslope}};
            
            \pgfplotsset{cycle list shift=-1}
            \addplot+ [
                color=blue,
                mark=square*,
                mark options={fill=blue!80!black, solid},
            ] table [
                header=false,
                row sep=\\,
            ] {
                1 0.1048283953960827\\
                2 0.0570321127995883\\
                3 0.03653877832050813\\
                4 0.03081449032848833\\
                5 0.02267371831237261\\
                6 0.02054166226491228\\
                7 0.01740293376783863\\
                8 0.01511076781175369\\
                9 0.0140497697290185\\
                10 0.01224182763207336\\
            };
            \addlegendentry{Trunk space}

            \addplot [
                draw=none,
                forget plot,
            ] table [
                header=false,
                row sep=\\,
                y={create col/linear regression={y=[index]1}},
            ] {
                6 0.02054166226491228\\
                7 0.01740293376783863\\
                8 0.01511076781175369\\
                9 0.0140497697290185\\
                10 0.01224182763207336\\
            };
            \xdef\trunkslope{\pgfplotstableregressiona} % -0.98239
            \xdef\trunkconst{\pgfplotstableregressionb} % -0.92606
            %\node at (axis cs:1.5,0.005) {\pgfmathprintnumber[fixed, precision=5]{\trunkslope}};
            %\node at (axis cs:1.5,0.002) {\pgfmathprintnumber[fixed, precision=5]{\trunkconst}}; 
            \addplot [
                color=black,
                forget plot,
                domain=7:12,
                yshift=2.3mm,
                dashed,
            ] {10^(-0.92606) * x^(-0.98239)}
                coordinate [pos=0.4] (trunkA)
                coordinate [pos=0.8] (trunkB)
            ;
            \draw (trunkA) -| (trunkB)
                node [
                    pos=0.75,
                    anchor=west,
                    xshift=-0.5mm,
                ] {\scriptsize\pgfmathprintnumber{\trunkslope}};
        \end{loglogaxis}
        \end{tikzpicture}
        \caption{Triangle mesh; $x_0 = (1/3,1/3)$.}
        \label{fig:L2_errors_int_b}
    \end{subfigure}
    \caption{Same as Figure~\ref{fig:L2_errors_node} but when the load is in the interior of an element.}
    \label{fig:L2_errors_int}
\end{figure}

We may conclude from Figure~\ref{fig:L2_errors_node} that
when the load is located at a vertex, the $L^2$ error decays at a clearly higher rate
than what Theorem~\ref{thm:L2_convergence_of_p_version_dirac_load} predicts
for both quadrilateral and triangular meshes and for both product and trunk spaces.
For the quadrilateral elements, the product space is a better choice than the trunk space
when measured in terms of convergence rate, which is not surprising.
The internal shape functions for quadrilaterals seem to be important for improving the
quality of the approximation as can be seen when comparing, for example, the
transitions from $p=2$ to $p=3$ and from $p=3$ to $p=4$ for the trunk space.
Recall that the trunk space for quadrilaterals does not contain any internal shape functions
until $p=4$. The difference between the product and trunk spaces for triangular elements
is not as striking. The asymptotic rate of convergence for the product space
seems to be again better, however.

Based on Figure~\ref{fig:L2_errors_side}, when the Dirac delta load is on a side,
Theorem~\ref{thm:L2_convergence_of_p_version_dirac_load} only slightly overestimates the
true rate of convergence. The error is in general larger for the same values of $p$
when compared to the load being at a vertex in Figure~\ref{fig:L2_errors_node}.
The error also decreases more irregularly. The decrease in error is larger for even values
of $p$ than for odd values of $p$. The error may even remain the same for
odd values of $p$ as can be seen for the product space especially.
The load is set to be in the middle of the side for both meshes, but
a similar phenomenon can be observed for other locations on the side as well,
where the decrease in error fluctuates periodically.
The above discussion also applies to the case where the load is in the interior of an element
in Figure~\ref{fig:L2_errors_int}.
For both cases, choosing between the product space and the trunk space does not
seem to have a large effect on the asymptotic convergence rate of the error.

\begin{figure}[b]
    \centering
    \begin{subfigure}[t]{0.475\textwidth}
        \centering
        \resizebox{0.75\textwidth}{!}{
            \begin{tikzpicture}
                \fill[gray!40] (2,2) rectangle (3,3);
                \fill[gray!40] (3,2) rectangle (4,3);
                \fill[gray!40] (3,3) rectangle (4,4);
                \draw (0,0) grid (4,4);
                \node[anchor=center] at (2.5, 2.5) {$1$};
                \node[anchor=center] at (3.5, 2.5) {$2$};
                \node[anchor=center] at (3.5, 3.5) {$3$};
                \node[circle, draw=none, fill=red, inner sep=0pt, minimum size=4pt] (aa) at (2,2) {};
                \node[anchor=north east, xshift=1mm, yshift=0.5mm] at (2,2) {\footnotesize$x_0$};
            \end{tikzpicture}
        }
        \caption{The considered elements.}
        \label{fig:L2_errors_elementwise_quad_a}
    \end{subfigure}\vspace{10pt}
    
    \begin{subfigure}[t]{0.475\textwidth}
        \centering
        \begin{tikzpicture}[baseline, trim axis left, trim axis right]
        \begin{loglogaxis}[
            log basis x=10,
            log basis y=10,
            width=0.9\textwidth,
            scale only axis=true,
            xmin=1, xmax=15,
            scale mode=stretch to fill,
            axis x line=bottom,
            axis y line=left,
            enlarge x limits=true,
            enlarge y limits=true,
            clip=false,
            xtick={1,2,5,10,15},
            extra x ticks={3,4,6,7,8,9,11,12,13,14},
            extra x tick labels={},
            xticklabel=\pgfmathparse{10^\tick}\pgfmathprintnumber{\pgfmathresult},
            tick label style={font=\footnotesize},
            legend style={font=\footnotesize},
            ticklabel shift=-0.5mm,
            legend pos=south west,
        ]
            \addplot+ [
                color=red,
                mark=*,
                mark options={fill=red!80!black, solid},
            ] table [
                header=false,
                row sep=\\,
            ] {
                1 0.01094694743408355\\
                2 0.005916321271534262\\
                3 0.003427407964893827\\
                4 0.002246107616907983\\
                5 0.001585044423288995\\
                6 0.001177353021703965\\
                7 0.0009085054372213324\\
                8 0.0007219835448634648\\
                9 0.0005873570032437086\\
                10 0.0004870531979734667\\
            };
            \addlegendentry{Element 1}
            \addplot [
                draw=none,
                forget plot,
            ] table [
                header=false,
                row sep=\\,
                y={create col/linear regression={y=[index]1}},
            ] {
                7 0.0009085054372213324\\
                8 0.0007219835448634648\\
                9 0.0005873570032437086\\
                10 0.0004870531979734667\\
            };
            \xdef\eloneslope{\pgfplotstableregressiona} % -1.74796
            \xdef\eloneconst{\pgfplotstableregressionb} % -1.5638
            %\node at (axis cs:1.5,0.005) {\pgfmathprintnumber[fixed, precision=5]{\eloneslope}};
            %\node at (axis cs:1.5,0.002) {\pgfmathprintnumber[fixed, precision=5]{\eloneconst}};
            \addplot [
                color=black,
                forget plot,
                domain=7:12,
                yshift=2mm,
                dashed,
            ] {10^(-1.5638) * x^(-1.74796)}
                coordinate [pos=0.4] (eloneA)
                coordinate [pos=0.8] (eloneB)
            ;
            \draw (eloneA) -| (eloneB)
                node [
                    pos=0.75,
                    anchor=west,
                    xshift=-0.5mm,
                ] {\scriptsize\pgfmathprintnumber{\eloneslope}};

            \addplot+ [
                color=green!80!black,
                mark=square*,
                mark options={fill=green!80!black, solid},
            ] table [
                header=false,
                row sep=\\,
            ] {
                1 0.003760314170725563\\
                2 0.0008125599570475858\\
                3 0.0003763271321208731\\
                4 0.0001923552931703659\\
                5 0.000109017458510906\\
                6 6.846234873149577e-05\\
                7 4.571644748714572e-05\\
                8 3.201079415624558e-05\\
                9 2.328463622062495e-05\\
                10 1.746371347458305e-05\\
            };
            \addlegendentry{Element 2}
            \addplot [
                draw=none,
                forget plot,
            ] table [
                header=false,
                row sep=\\,
                y={create col/linear regression={y=[index]1}},
            ] {
                7 4.571644748714572e-05\\
                8 3.201079415624558e-05\\
                9 2.328463622062495e-05\\
                10 1.746371347458305e-05\\
            };
            \xdef\eltwoslope{\pgfplotstableregressiona} % -2.6964
            \xdef\eltwoconst{\pgfplotstableregressionb} % -2.0605
            %\node at (axis cs:1.5,0.005) {\pgfmathprintnumber[fixed, precision=5]{\eltwoslope}};
            %\node at (axis cs:1.5,0.002) {\pgfmathprintnumber[fixed, precision=5]{\eltwoconst}};
            \addplot [
                color=black,
                forget plot,
                domain=7:12,
                yshift=2mm,
                dashed,
            ] {10^(-2.0605) * x^(-2.6964)}
                coordinate [pos=0.4] (eltwoA)
                coordinate [pos=0.8] (eltwoB)
            ;
            \draw (eltwoA) -| (eltwoB)
                node [
                    pos=0.75,
                    anchor=west,
                    xshift=-0.5mm,
                ] {\scriptsize\pgfmathprintnumber{\eltwoslope}};

            \addplot+ [
                color=blue,
                mark=diamond*,
                mark options={fill=blue!80!black, solid},
            ] table [
                header=false,
                row sep=\\,
            ] {
                1 0.002362963039340629\\
                2 0.0001869584419078544\\
                3 5.819426665616335e-05\\
                4 2.690420457578392e-05\\
                5 1.351319550114775e-05\\
                6 7.429286408461356e-06\\
                7 4.409435715795378e-06\\
                8 2.777849359126753e-06\\
                9 1.834928999918978e-06\\
                10 1.259792902777296e-06\\
            };
            \addlegendentry{Element 3}
            \addplot [
                draw=none,
                forget plot,
            ] table [
                header=false,
                row sep=\\,
                y={create col/linear regression={y=[index]1}},
            ] {
                7 4.409435715795378e-06\\
                8 2.777849359126753e-06\\
                9 1.834928999918978e-06\\
                10 1.259792902777296e-06\\
            };
            \xdef\elthreeslope{\pgfplotstableregressiona} % -3.5103
            \xdef\elthreeconst{\pgfplotstableregressionb} % -2.388
            %\node at (axis cs:1.5,0.005) {\pgfmathprintnumber[fixed, precision=5]{\elthreeslope}};
            %\node at (axis cs:1.5,0.002) {\pgfmathprintnumber[fixed, precision=5]{\elthreeconst}};
            \addplot [
                color=black,
                forget plot,
                domain=7:12,
                yshift=2mm,
                dashed,
            ] {10^(-2.388) * x^(-3.5103)}
                coordinate [pos=0.4] (elthreeA)
                coordinate [pos=0.8] (elthreeB)
            ;
            \draw (elthreeA) -| (elthreeB)
                node [
                    pos=0.75,
                    anchor=west,
                    xshift=-0.5mm,
                ] {\scriptsize\pgfmathprintnumber{\elthreeslope}};
        \end{loglogaxis}
        \end{tikzpicture}
        \caption{Product space.}
        \label{fig:L2_errors_elementwise_quad_b}
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[t]{0.475\textwidth}  
        \centering 
        \begin{tikzpicture}[baseline, trim axis left, trim axis right]
        \begin{loglogaxis}[
            log basis x=10,
            log basis y=10,
            width=0.9\textwidth,
            scale only axis=true,
            xmin=1, xmax=15,
            scale mode=stretch to fill,
            axis x line=bottom,
            axis y line=left,
            enlarge x limits=true,
            enlarge y limits=true,
            clip=false,
            xtick={1,2,5,10,15},
            extra x ticks={3,4,6,7,8,9,11,12,13,14},
            extra x tick labels={},
            xticklabel=\pgfmathparse{10^\tick}\pgfmathprintnumber{\pgfmathresult},
            tick label style={font=\footnotesize},
            legend style={font=\footnotesize},
            ticklabel shift=-0.5mm,
            legend pos=south west,
        ]
            \addplot+ [
                color=red,
                mark=*,
                mark options={fill=red!80!black, solid},
            ] table [
                header=false,
                row sep=\\,
            ] {
                1 0.01094694743408355\\
                2 0.006645147449784917\\
                3 0.005931285128593899\\
                4 0.004723748084187016\\
                5 0.003705827841665341\\
                6 0.003001762454140037\\
                7 0.002501675371971704\\
                8 0.002132966246331382\\
                9 0.001850633301005331\\
                10 0.001626877557402803\\
            };
            \addlegendentry{Element 1}
            \addplot [
                draw=none,
                forget plot,
            ] table [
                header=false,
                row sep=\\,
                y={create col/linear regression={y=[index]1}},
            ] {
                7 0.002501675371971704\\
                8 0.002132966246331382\\
                9 0.001850633301005331\\
                10 0.001626877557402803\\
            };
            \xdef\eloneslope{\pgfplotstableregressiona} % -1.20572
            \xdef\eloneconst{\pgfplotstableregressionb} % -1.58253
            %\node at (axis cs:1.5,0.005) {\pgfmathprintnumber[fixed, precision=5]{\eloneslope}};
            %\node at (axis cs:1.5,0.002) {\pgfmathprintnumber[fixed, precision=5]{\eloneconst}};
            \addplot [
                color=black,
                forget plot,
                domain=7:12,
                yshift=2mm,
                dashed,
            ] {10^(-1.58253) * x^(-1.20572)}
                coordinate [pos=0.4] (eloneA)
                coordinate [pos=0.8] (eloneB)
            ;
            \draw (eloneA) -| (eloneB)
                node [
                    pos=0.75,
                    anchor=west,
                    xshift=-0.5mm,
                ] {\scriptsize\pgfmathprintnumber{\eloneslope}};

            \addplot+ [
                color=green!80!black,
                mark=square*,
                mark options={fill=green!80!black, solid},
            ] table [
                header=false,
                row sep=\\,
            ] {
                1 0.003760314170725563\\
                2 0.002100085225515138\\
                3 0.002105932807638351\\
                4 0.001402963899356926\\
                5 0.0007548607054688248\\
                6 0.0004959053243437548\\
                7 0.000333175240928009\\
                8 0.0002381296667484669\\
                9 0.0001752150137740697\\
                10 0.0001333873075911694\\
            };
            \addlegendentry{Element 2}
            \addplot [
                draw=none,
                forget plot,
            ] table [
                header=false,
                row sep=\\,
                y={create col/linear regression={y=[index]1}},
            ] {
                7 0.000333175240928009\\
                8 0.0002381296667484669\\
                9 0.0001752150137740697\\
                10 0.0001333873075911694\\
            };
            \xdef\eltwoslope{\pgfplotstableregressiona} % -2.568
            \xdef\eltwoconst{\pgfplotstableregressionb} % -1.30592
            %\node at (axis cs:1.5,0.005) {\pgfmathprintnumber[fixed, precision=5]{\eltwoslope}};
            %\node at (axis cs:1.5,0.002) {\pgfmathprintnumber[fixed, precision=5]{\eltwoconst}};
            \addplot [
                color=black,
                forget plot,
                domain=7:12,
                yshift=2mm,
                dashed,
            ] {10^(-1.30592) * x^(-2.568)}
                coordinate [pos=0.4] (eltwoA)
                coordinate [pos=0.8] (eltwoB)
            ;
            \draw (eltwoA) -| (eltwoB)
                node [
                    pos=0.75,
                    anchor=west,
                    xshift=-0.5mm,
                ] {\scriptsize\pgfmathprintnumber{\eltwoslope}};

            \addplot+ [
                color=blue,
                mark=diamond*,
                mark options={fill=blue!80!black, solid},
            ] table [
                header=false,
                row sep=\\,
            ] {
                1 0.002362963039340629\\
                2 0.001275353342211139\\
                3 0.0009650659775637892\\
                4 0.0004763209585489727\\
                5 0.0002911982183797607\\
                6 0.0001761906106741981\\
                7 0.0001160149624218436\\
                8 8.09934928142345e-05\\
                9 5.904835148180123e-05\\
                10 4.450218697535163e-05\\
            };
            \addlegendentry{Element 3}
            \addplot [
                draw=none,
                forget plot,
            ] table [
                header=false,
                row sep=\\,
                y={create col/linear regression={y=[index]1}},
            ] {
                7 0.0001160149624218436\\
                8 8.09934928142345e-05\\
                9 5.904835148180123e-05\\
                10 4.450218697535163e-05\\
            };
            \xdef\elthreeslope{\pgfplotstableregressiona} % -2.6855
            \xdef\elthreeconst{\pgfplotstableregressionb} % -1.66609
            %\node at (axis cs:1.5,0.005) {\pgfmathprintnumber[fixed, precision=5]{\elthreeslope}};
            %\node at (axis cs:1.5,0.002) {\pgfmathprintnumber[fixed, precision=5]{\elthreeconst}};
            \addplot [
                color=black,
                forget plot,
                domain=7:12,
                yshift=2mm,
                dashed,
            ] {10^(-1.66609) * x^(-2.6855)}
                coordinate [pos=0.4] (elthreeA)
                coordinate [pos=0.8] (elthreeB)
            ;
            \draw (elthreeA) -| (elthreeB)
                node [
                    pos=0.75,
                    anchor=west,
                    xshift=-0.5mm,
                ] {\scriptsize\pgfmathprintnumber{\elthreeslope}};
        \end{loglogaxis}
        \end{tikzpicture}
        \caption{Trunk space.}
        \label{fig:L2_errors_elementwise_quad_c}
    \end{subfigure}
    \caption{Log-log plots of $\norm{u - u_S}_{L^2(E_i)}$ vs.~$p$ over individual elements of the quadrilateral mesh. The Dirac delta load is at $x_0=(0,0)$.}
    \label{fig:L2_errors_elementwise_quad}
\end{figure}

All in all, the results above suggest that Theorem~\ref{thm:L2_convergence_of_p_version_dirac_load}
is more applicable to the cases where the Dirac delta load is located either on a side or
in the interior of an element. It seems more beneficial to construct the mesh so that
the load is at a vertex since, based on the examples above, this choice results in much
better convergence rate and more robust error behavior.
Let us now briefly consider this case further by
investigating how the $L^2$ error is distributed between the individual elements.
Figures~\ref{fig:L2_errors_elementwise_quad} and \ref{fig:L2_errors_elementwise_tri}
show the element-wise errors for the quadrilateral mesh and the triangle mesh, respectively.
Due to symmetry, it suffices to only consider the designated elements to cover
the other elements as well.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.475\textwidth}
        \centering
        \resizebox{0.75\textwidth}{!}{
            \begin{tikzpicture}
                \filldraw[draw=none, fill=gray!40] (4,0) -- (4,2) -- (2,2) -- cycle;
                \filldraw[draw=none, fill=gray!40] (2,2) -- (4,2) -- (2,4) -- cycle;
                \filldraw[draw=none, fill=gray!40] (4,2) -- (4,4) -- (2,4) -- cycle;
                \draw[step=2cm] (0,0) grid (4,4);
                \draw (2,0) -- (0,2);
                \draw (4,0) -- (0,4);
                \draw (4,2) -- (2,4);
                \node[anchor=center] at (3.33,1.33) {$1$};
                \node[anchor=center] at (2.66,2.66) {$2$};
                \node[anchor=center] at (3.33, 3.33) {$3$};
                \node[circle, draw=none, fill=red, inner sep=0pt, minimum size=4pt] (aa) at (2,2) {};
                \node[anchor=north east, xshift=1mm, yshift=0.5mm] at (2,2) {\footnotesize$x_0$};
            \end{tikzpicture}
        }
        \caption{The considered elements.}
        \label{fig:L2_errors_elementwise_tri_a}
    \end{subfigure}\vspace{10pt}
    
    \begin{subfigure}[t]{0.475\textwidth}
        \centering
        \begin{tikzpicture}[baseline, trim axis left, trim axis right]
        \begin{loglogaxis}[
            log basis x=10,
            log basis y=10,
            width=0.9\textwidth,
            scale only axis=true,
            xmin=1, xmax=15,
            scale mode=stretch to fill,
            axis x line=bottom,
            axis y line=left,
            enlarge x limits=true,
            enlarge y limits=true,
            clip=false,
            xtick={1,2,5,10,15},
            extra x ticks={3,4,6,7,8,9,11,12,13,14},
            extra x tick labels={},
            xticklabel=\pgfmathparse{10^\tick}\pgfmathprintnumber{\pgfmathresult},
            tick label style={font=\footnotesize},
            legend style={font=\footnotesize},
            ticklabel shift=-0.5mm,
            legend pos=south west,
        ]
            \addplot+ [
                color=red,
                mark=*,
                mark options={fill=red!80!black, solid},
            ] table [
                header=false,
                row sep=\\,
            ] {
                1 0.02159955207254669\\
                2 0.01516193958838212\\
                3 0.009228911407982302\\
                4 0.006535750166056712\\
                5 0.004820208831747048\\
                6 0.003738109755127364\\
                7 0.002988328612693951\\
                8 0.002456430204130792\\
                9 0.002059237977526098\\
                10 0.001756321141730147\\
            };
            \addlegendentry{Element 1}
            \addplot [
                draw=none,
                forget plot,
            ] table [
                header=false,
                row sep=\\,
                y={create col/linear regression={y=[index]1}},
            ] {
                5 0.004820208831747048\\
                6 0.003738109755127364\\
                7 0.002988328612693951\\
                8 0.002456430204130792\\
                9 0.002059237977526098\\
                10 0.001756321141730147\\
            };
            \xdef\eloneslope{\pgfplotstableregressiona} % -1.45804
            \xdef\eloneconst{\pgfplotstableregressionb} % -1.29475
            %\node at (axis cs:1.5,0.005) {\pgfmathprintnumber[fixed, precision=5]{\eloneslope}};
            %\node at (axis cs:1.5,0.002) {\pgfmathprintnumber[fixed, precision=5]{\eloneconst}};
            \addplot [
                color=black,
                forget plot,
                domain=7:12,
                yshift=2mm,
                dashed,
            ] {10^(-1.29475) * x^(-1.45804)}
                coordinate [pos=0.4] (eloneA)
                coordinate [pos=0.8] (eloneB)
            ;
            \draw (eloneA) -| (eloneB)
                node [
                    pos=0.75,
                    anchor=west,
                    xshift=-0.5mm,
                ] {\scriptsize\pgfmathprintnumber{\eloneslope}};

            \addplot+ [
                color=green!80!black,
                mark=square*,
                mark options={fill=green!80!black, solid},
            ] table [
                header=false,
                row sep=\\,
            ] {
                1 0.02551184979129133\\
                2 0.009603422485375385\\
                3 0.005949526461283435\\
                4 0.003831216645021726\\
                5 0.002788251622426721\\
                6 0.00204929888621079\\
                7 0.001609441887501835\\
                8 0.001277520632078392\\
                9 0.001051365931963892\\
                10 0.0008717107408934837\\
            };
            \addlegendentry{Element 2}
            \addplot [
                draw=none,
                forget plot,
            ] table [
                header=false,
                row sep=\\,
                y={create col/linear regression={y=[index]1}},
            ] {
                5 0.002788251622426721\\
                6 0.00204929888621079\\
                7 0.001609441887501835\\
                8 0.001277520632078392\\
                9 0.001051365931963892\\
                10 0.0008717107408934837\\
            };
            \xdef\eltwoslope{\pgfplotstableregressiona} % -1.66917
            \xdef\eltwoconst{\pgfplotstableregressionb} % -1.3871
            %\node at (axis cs:1.5,0.005) {\pgfmathprintnumber[fixed, precision=5]{\eltwoslope}};
            %\node at (axis cs:1.5,0.002) {\pgfmathprintnumber[fixed, precision=5]{\eltwoconst}};
            \addplot [
                color=black,
                forget plot,
                domain=7:12,
                yshift=1.8mm,
                dashed,
            ] {10^(-1.3871) * x^(-1.66917)}
                coordinate [pos=0.4] (eltwoA)
                coordinate [pos=0.8] (eltwoB)
            ;
            \draw (eltwoA) -| (eltwoB)
                node [
                    pos=0.75,
                    anchor=west,
                    xshift=-0.5mm,
                ] {\scriptsize\pgfmathprintnumber{\eltwoslope}};

            \addplot+ [
                color=blue,
                mark=diamond*,
                mark options={fill=blue!80!black, solid},
            ] table [
                header=false,
                row sep=\\,
            ] {
                1 0.01356890039412235\\
                2 0.002772779214210311\\
                3 0.0006702622355202311\\
                4 0.0006858128988821995\\
                5 0.0003173953724525401\\
                6 0.0002755065032962035\\
                7 0.0001323652285771819\\
                8 0.0001357628250708961\\
                9 8.081712853992563e-05\\
                10 7.933587375554804e-05\\
            };
            \addlegendentry{Element 3}
            \addplot [
                draw=none,
                forget plot,
            ] table [
                header=false,
                row sep=\\,
                y={create col/linear regression={y=[index]1}},
            ] {
                4 0.0006858128988821995\\
                6 0.0002755065032962035\\
                8 0.0001357628250708961\\
                10 7.933587375554804e-05\\
            };
            \xdef\elthreeslope{\pgfplotstableregressiona} % -2.35806
            \xdef\elthreeconst{\pgfplotstableregressionb} % -1.73734
            %\node at (axis cs:1.5,0.005) {\pgfmathprintnumber[fixed, precision=5]{\elthreeslope}};
            %\node at (axis cs:1.5,0.002) {\pgfmathprintnumber[fixed, precision=5]{\elthreeconst}};
            \addplot [
                color=black,
                forget plot,
                domain=7:12,
                yshift=2mm,
                dashed,
            ] {10^(-1.73734) * x^(-2.35806)}
                coordinate [pos=0.4] (elthreeA)
                coordinate [pos=0.8] (elthreeB)
            ;
            \draw (elthreeA) -| (elthreeB)
                node [
                    pos=0.75,
                    anchor=west,
                    xshift=-0.5mm,
                ] {\scriptsize\pgfmathprintnumber{\elthreeslope}};
        \end{loglogaxis}
        \end{tikzpicture}
        \caption{Product space.}
        \label{fig:L2_errors_elementwise_tri_b}
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[t]{0.475\textwidth}  
        \centering 
        \begin{tikzpicture}[baseline, trim axis left, trim axis right]
        \begin{loglogaxis}[
            log basis x=10,
            log basis y=10,
            width=0.9\textwidth,
            scale only axis=true,
            xmin=1, xmax=15,
            scale mode=stretch to fill,
            axis x line=bottom,
            axis y line=left,
            enlarge x limits=true,
            enlarge y limits=true,
            clip=false,
            xtick={1,2,5,10,15},
            extra x ticks={3,4,6,7,8,9,11,12,13,14},
            extra x tick labels={},
            xticklabel=\pgfmathparse{10^\tick}\pgfmathprintnumber{\pgfmathresult},
            tick label style={font=\footnotesize},
            legend style={font=\footnotesize},
            ticklabel shift=-0.5mm,
            legend pos=south west,
        ]
            \addplot+ [
                color=red,
                mark=*,
                mark options={fill=red!80!black, solid},
            ] table [
                header=false,
                row sep=\\,
            ] {
                1 0.02159955207254669\\
                2 0.01302535693517707\\
                3 0.008684356235912261\\
                4 0.006261666038354169\\
                5 0.004771572025240436\\
                6 0.003791657466795554\\
                7 0.003106579891919291\\
                8 0.002605509732276115\\
                9 0.002225965867289673\\
                10 0.001930337201465429\\
            };
            \addlegendentry{Element 1}
            \addplot [
                draw=none,
                forget plot,
            ] table [
                header=false,
                row sep=\\,
                y={create col/linear regression={y=[index]1}},
            ] {
                5 0.004771572025240436\\
                6 0.003791657466795554\\
                7 0.003106579891919291\\
                8 0.002605509732276115\\
                9 0.002225965867289673\\
                10 0.001930337201465429\\
            };
            \xdef\eloneslope{\pgfplotstableregressiona} % -1.30632
            \xdef\eloneconst{\pgfplotstableregressionb} % -1.40588
            %\node at (axis cs:1.5,0.005) {\pgfmathprintnumber[fixed, precision=5]{\eloneslope}};
            %\node at (axis cs:1.5,0.002) {\pgfmathprintnumber[fixed, precision=5]{\eloneconst}};
            \addplot [
                color=black,
                forget plot,
                domain=7:12,
                yshift=2mm,
                dashed,
            ] {10^(-1.40588) * x^(-1.30632)}
                coordinate [pos=0.3] (eloneA)
                coordinate [pos=0.8] (eloneB)
            ;
            \draw (eloneA) -| (eloneB)
                node [
                    pos=0.75,
                    anchor=west,
                    xshift=-0.5mm,
                ] {\scriptsize\pgfmathprintnumber{\eloneslope}};

            \addplot+ [
                color=green!80!black,
                mark=square*,
                mark options={fill=green!80!black, solid},
            ] table [
                header=false,
                row sep=\\,
            ] {
                1 0.02551184979129133\\
                2 0.0121148686195217\\
                3 0.007299297699954569\\
                4 0.005186155919772657\\
                5 0.003951324704316316\\
                6 0.003161248162047394\\
                7 0.002613752288963853\\
                8 0.002213367958710818\\
                9 0.001908710604638855\\
                10 0.00166979443827333\\
            };
            \addlegendentry{Element 2}
            \addplot [
                draw=none,
                forget plot,
            ] table [
                header=false,
                row sep=\\,
                y={create col/linear regression={y=[index]1}},
            ] {
                5 0.003951324704316316\\
                6 0.003161248162047394\\
                7 0.002613752288963853\\
                8 0.002213367958710818\\
                9 0.001908710604638855\\
                10 0.00166979443827333\\
            };
            \xdef\eltwoslope{\pgfplotstableregressiona} % -1.2423
            \xdef\eltwoconst{\pgfplotstableregressionb} % -1.53392
            %\node at (axis cs:1.5,0.005) {\pgfmathprintnumber[fixed, precision=5]{\eltwoslope}};
            %\node at (axis cs:1.5,0.002) {\pgfmathprintnumber[fixed, precision=5]{\eltwoconst}};
            \addplot [
                color=black,
                forget plot,
                domain=7:12,
                xshift=-2mm,
                yshift=-1.5mm,
                dashed,
            ] {10^(-1.53392) * x^(-1.2423)}
                coordinate [pos=0.3] (eltwoA)
                coordinate [pos=0.8] (eltwoB)
            ;
            \draw (eltwoA) |- (eltwoB)
                node [
                    pos=0.25,
                    anchor=east,
                    xshift=0.5mm,
                ] {\scriptsize\pgfmathprintnumber{\eltwoslope}};

            \addplot+ [
                color=blue,
                mark=diamond*,
                mark options={fill=blue!80!black, solid},
            ] table [
                header=false,
                row sep=\\,
            ] {
                1 0.01356890039412235\\
                2 0.005088799589543467\\
                3 0.001152824307845613\\
                4 0.0001596658111138588\\
                5 7.900015655229896e-05\\
                6 2.106606028980747e-05\\
                7 8.321636416643577e-06\\
                8 2.542105474778043e-06\\
                9 1.039429591485007e-06\\
                10 3.090197120362704e-07\\
                11 1.346558284335898e-07\\
            };
            \addlegendentry{Element 3}
            \addplot [
                draw=none,
                forget plot,
            ] table [
                header=false,
                row sep=\\,
                y={create col/linear regression={y=[index]1}},
            ] {
                8 2.542105474778043e-06\\
                9 1.039429591485007e-06\\
                10 3.090197120362704e-07\\
                11 1.346558284335898e-07\\
            };
            \xdef\elthreeslope{\pgfplotstableregressiona} % -9.4351
            \xdef\elthreeconst{\pgfplotstableregressionb} % 2.9564
            %\node at (axis cs:1.5,0.005) {\pgfmathprintnumber[fixed, precision=5]{\elthreeslope}};
            %\node at (axis cs:1.5,0.002) {\pgfmathprintnumber[fixed, precision=5]{\elthreeconst}};
            \addplot [
                color=black,
                forget plot,
                domain=7:12,
                xshift=0.5mm,
                yshift=2mm,
                dashed,
            ] {10^(2.9564) * x^(-9.4351)}
                coordinate [pos=0.4] (elthreeA)
                coordinate [pos=0.8] (elthreeB)
            ;
            \draw (elthreeA) -| (elthreeB)
                node [
                    pos=0.75,
                    anchor=west,
                    xshift=-0.5mm,
                ] {\scriptsize\pgfmathprintnumber{\elthreeslope}};
        \end{loglogaxis}
        \end{tikzpicture}
        \caption{Trunk space.}
        \label{fig:L2_errors_elementwise_tri_c}
    \end{subfigure}
    \caption{Log-log plots of $\norm{u - u_S}_{L^2(E_i)}$ vs.~$p$ over individual elements of the triangle mesh. The Dirac delta load is at $x_0=(0,0)$.}
    \label{fig:L2_errors_elementwise_tri}
\end{figure}

Unsurprisingly, the element-wise error is the largest and the rate of convergence is the slowest
in the elements that overlap with the Dirac delta load. The rate of convergence in those elements
also determines the rate of convergence in the whole domain as can be seen when comparing
the results to Figure~\ref{fig:L2_errors_node}.
The error converges significantly faster when the element does not overlap with the load.
Thus, it would make sense to refine the mesh around the load to improve the error over
the whole domain.
In general, the further away the element is from the load, the faster the convergence seems to be.
The convergence can even be exponential as can be seen for the trunk space for
triangular elements. Interestingly, the convergence is not exponential when the trunk space
is replaced with the product space or when triangles are replaced with quadrilaterals.
Although we have only provided the computations for the case where the load is at a vertex,
similar observations apply to the other locations of the load as well.

The computations presented above were performed using computer resources within the Aalto
University School of Science ``Science-IT'' project.

\clearpage

\section{Summary}
\label{sec:summary}

In Section~\ref{sec:intro}, we set out to study Poisson's equation with a Dirac delta load term
in the context of the $p$-version of the finite element method for which we defined two objectives.
The first objective was to consider the unique solvability of the problem with Dirichlet and
Neumann boundary values in two-dimensional bounded polygonal domains.
The second objective was to study the convergence of the $p$-version
of the finite element method when applied to the same problem,
which does not seem to be covered by the existing literature.

For the first objective, the standard theory for solving elliptic boundary value problems
in a Hilbert space is not alone applicable to assert the existence of a solution to the
Dirac delta problem simply because the problem cannot be formulated in the
usual Hilbert space setting. It is, however, a well-known fact from the classical theory of
partial differential equations that the Green's function for the Laplacian is a solution to the 
Dirac delta problem when the boundary values are set accordingly.
We used the Green's function in combination with the standard Hilbert space PDE theory
to show that the Dirac delta problem with general boundary data has a solution.
Moreover, when the domain satisfies certain convexity assumptions,
we were able to show that the solution is unique. This is in line with the existence and uniqueness
result by Casas \cite{casas1985} who considers the homogeneous Dirichlet problem.

For the second objective, we proved the $L^2$ convergence of the $p$-version of the finite element
method for the Dirac delta problem under the same convexity assumptions that were used to prove the
uniqueness of the solution. We used the same proof strategy that Casas \cite{casas1985}
used to prove $L^2$ convergence for the $h$-version. The proof relies on standard error estimates
for the $p$-version and on some additional pointwise estimates that do not seem to
be covered by the existing standard literature. The proofs for the additional estimates
are largely based on the results of Babu{\v s}ka and Suri \cite{babuskasuri1987} and
Schwab \cite{schwab1998}.

Finally, the accuracy of the obtained $L^2$ error bound was assessed by applying the
$p$- finite element method to the Neumann problem whose exact solution is given by
the Green's function mentioned before. We considered the error for several different configurations:
quadrilateral and triangular elements, different locations for the Dirac delta load and
different polynomial space types. The proven $L^2$ error bound applies to all of these
configurations. When the Dirac delta load is on a side or in the interior of an element,
the error bound overestimates the exact error rate only slightly.
When the load is at a vertex of the mesh, the predicted error convergence rate
is overly pessimistic, and the element-wise convergence rate can even be exponential
for a triangular element that does not overlap with the load.

Nothing has been said about the optimality of the obtained $L^2$ error bound,
which could be the topic for a future study. The numerical results suggest
that the bound could be improved. Moreover, the behavior of the error heavily depends
on the location of the Dirac delta load, which is not accounted for by the obtained error bound.
Two additional open questions are whether the Dirac delta problem still admits a unique solution
when the convexity assumption is dropped and, if so,
whether Theorem~\ref{thm:L2_convergence_of_p_version_dirac_load} is still true.

\clearpage
%% Bibliography/ list of references
%%
\thesisbibliography
\bibliographystyle{ieeetr}
\bibliography{refs}

\end{document}
